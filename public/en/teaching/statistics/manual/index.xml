<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics Manual | Aprende con Alf</title>
    <link>/en/teaching/statistics/manual/</link>
      <atom:link href="/en/teaching/statistics/manual/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistics Manual</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/logo_hude38443eeb2faa5fa84365aba7d86a77_3514_300x300_fit_lanczos_3.png</url>
      <title>Statistics Manual</title>
      <link>/en/teaching/statistics/manual/</link>
    </image>
    
    <item>
      <title>Introduction</title>
      <link>/en/teaching/statistics/manual/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/en/teaching/statistics/manual/introduction/</guid>
      <description>&lt;h2 id=&#34;statistics-as-a-scientific-tool&#34;&gt;Statistics as a scientific tool&lt;/h2&gt;
&lt;h3 id=&#34;what-is-statistics&#34;&gt;What is Statistics?&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Statistics&lt;/strong&gt;. &lt;em&gt;Statistics&lt;/em&gt; is a branch of Mathematics that deals with data collection, summary, analysis and interpretation.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The role of Statistics is to extract information from data in order to gain knowledge for taking decisions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduction/statistics_purpose.png&#34; alt=&#34;Statistics purpose.&#34; title=&#34;Statistics purpose&#34;&gt;&lt;/p&gt;
&lt;p&gt;Statistics is essential in any scientific or technical discipline which requires data handling, especially with large volumes of data, such as Physics, Chemistry, Medicine, Psychology, Economics or Social Sciences.&lt;/p&gt;
&lt;p&gt;But, &lt;em&gt;why is Statistics necessary?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-changing-world&#34;&gt;A changing World&lt;/h3&gt;
&lt;p&gt;Scientists try to study the World. A World with a high variability that makes difficult determining the behaviour of things.&lt;/p&gt;
&lt;p&gt;Statistics provides a bridge between the real world and the mathematical models that attempt to explain it, providing a methodology to assess the discrepancies between reality and theoretical models.&lt;/p&gt;
&lt;p&gt;This makes Statistics an indispensable tool in applied sciences that require design of experiments and data analysis.&lt;/p&gt;
&lt;h2 id=&#34;population-and-sample&#34;&gt;Population and sample&lt;/h2&gt;
&lt;h3 id=&#34;statistical-population&#34;&gt;Statistical population&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Population&lt;/strong&gt;. A &lt;em&gt;population&lt;/em&gt; is a set of elements defined by an or more features that has all the elements and only them. Every element of the population is called &lt;em&gt;individual&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition - Population size&lt;/strong&gt;. The number of individuals in a population is known as the &lt;em&gt;population size&lt;/em&gt; and is represented by $N$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Sometimes not all the individuals are accessible to study. Then we distinguish between:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Theoretical population&lt;/strong&gt;: Individuals to which we want to extrapolate the study conclusions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Studied population&lt;/strong&gt;: Individuals truly accessible in the study.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In a study about a particular disease, the theoretical population would be all the persons that suffered the disease in some moment, even if they were not born yet. While the studied population will be the set o persons that have suffered the disease and that we can really study (observe that this exclude people with the disease but that we do not have any mean to get information about them).&lt;/p&gt;
&lt;h3 id=&#34;drawbacks-in-the-population-study&#34;&gt;Drawbacks in the population study&lt;/h3&gt;
&lt;p&gt;Scientists study a phenomenon in a population to understand it, to get knowledge about it, and so to control it.&lt;/p&gt;
&lt;p&gt;But, for a complete knowledge of the population it is necessary to study all its individuals.&lt;/p&gt;
&lt;p&gt;However, this is not always possible for several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The population size is infinite or too large to study all its individuals.&lt;/li&gt;
&lt;li&gt;The operations that individuals undergo are destructive.&lt;/li&gt;
&lt;li&gt;The cost, both in money and time, that would require study all the individuals in the population is not affordable.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;statistics-sample&#34;&gt;Statistics Sample&lt;/h3&gt;
&lt;p&gt;When it is not possible or convenient to study all the individuals in a population, we study only a subset of them.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample&lt;/strong&gt;. A &lt;em&gt;sample&lt;/em&gt; is a subset of the population.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition - Sample size&lt;/strong&gt;. The number of individuals of the sample is called &lt;em&gt;sample size&lt;/em&gt; and is represented by $n$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Usually, the population study is conducted on samples drawn from it.&lt;/p&gt;
&lt;p&gt;The sample study only gives an approximate knowledge of the population. But in most cases it is &lt;em&gt;enough&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;sample-size-determination&#34;&gt;Sample size determination&lt;/h3&gt;
&lt;p&gt;One of the most interesting questions that arise:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How many individuals are required to sample to have an approximate but enough knowledge of the population?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The answer depends of several factors, as the population variability or the desired reliability for extrapolations to the population.&lt;/p&gt;
&lt;p&gt;Unfortunately we can not answer that question until the end of the course, but in general, the most individuals the sample has, the more reliable will the conclusions be on the population, but also the study will be longer and more expensive.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. To understand what a sufficient sample size means we can use a picture example. A digital photography consist of a lot of small points called pixels disposed in an big array layout with rows and columns (the more rows and columns, the more resolution the picture has). Here the picture is the population and every pixel is an individual. Every pixel has a colour and it is the variability of colours what forms the picture motif.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How many pixels must we take in a sample in order to know the motif of a picture?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The answer depends on the variability of colours in the picture. If all the pixels in the picture are of the same colour, only one pixel is required to know the motif. But, if there is a lot of variability in the colours, a large sample size will be required.&lt;/p&gt;
&lt;p&gt;The image below contains a small sample of the pixels of a picture. Could you find out the motif of the picture?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduction/sample_windmill1.jpg&#34; alt=&#34;Picture with low resolution.&#34; title=&#34;Picture with low resolution.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;With a small sample size it is difficult to find out the picture motif!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Surely you has not been able to guess the motif because the number of pixels picked in the sample is too small to understand the variability of colours in the picture.&lt;/p&gt;
&lt;p&gt;The image below contains a larger sample of pixels. Could you find out the motif of the picture now?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduction/sample_windmill2.jpg&#34; alt=&#34;Picture with medium resolution.&#34; title=&#34;Picture with medium resolution.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;With a large sample is easier to find out the picture motif!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And here is the whole population.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduction/sample_windmill3.jpg&#34; alt=&#34;Picture with full resolution.&#34; title=&#34;Picture with full resolution.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;It is not required to know all the pixels of a picture to find out its motif!&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;types-of-reasoning&#34;&gt;Types of reasoning&lt;/h3&gt;
&lt;img src=&#34;../img/introduction/types_reasoning.png&#34; alt=&#34;Types of reasoning&#34; width=&#34;400px&#34;&gt;
&lt;p&gt;&lt;strong&gt;Deduction properties&lt;/strong&gt;: If the premises are true, it guarantees the certainty of the conclusions (that is, if something is true in the population, it is also true in the sample). However,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Induction properties&lt;/strong&gt;: It does not guarantee the certainty of the conclusions (if something is true in the sample, it may not be true in the population, so be careful with the extrapolations!). But, &lt;em&gt;it is the only way to generate new knowledge!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Statistics is fundamentally based on inductive reasoning, because it uses the information obtained from samples to draw conclusions about populations.&lt;/p&gt;
&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sampling&lt;/strong&gt;. The process of selecting the elements included in a sample is known as &lt;em&gt;sampling&lt;/em&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduction/sampling.png&#34; alt=&#34;Sampling&#34;&gt;&lt;/p&gt;
&lt;p&gt;To reflect reliable information about the whole population, the sample must be representative of the population. That means that the sample should reproduce on a smaller scale the population variability.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The goal is to get a representative sample of the population.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;types-of-sampling&#34;&gt;Types of sampling&lt;/h3&gt;
&lt;p&gt;There exist a lot of sampling methods but all of them can be grouped in two categories:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random sampling&lt;/strong&gt;: The sample individuals are selected randomly. All the population individuals have the same likelihood of being selected (&lt;em&gt;equiprobability&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non random sampling&lt;/strong&gt;: The sample individuals are not selected randomly. Some population individuals have a higher likelihood of being selected than others.&lt;/p&gt;
&lt;p&gt;Only random sampling methods avoid the selection bias and guarantee the representativeness of the sample, and therefore, the validity of conclusions.&lt;/p&gt;
&lt;p&gt;Non random sampling methods are not suitable to make generalizations because they do not guarantee the representativeness of the sample. Nevertheless, usually they are less expensive and can be used in exploratory studies.&lt;/p&gt;
&lt;h3 id=&#34;simple-random-sampling&#34;&gt;Simple random sampling&lt;/h3&gt;
&lt;p&gt;The most popular random sampling method is the &lt;em&gt;simple random sampling&lt;/em&gt;, that has the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All the population individuals have the same likelihood of being selected in the sample.&lt;/li&gt;
&lt;li&gt;The individual selection is performed with replacement, that is, each selected individual is returned to the population before selecting the next one. In this way the population does not change.&lt;/li&gt;
&lt;li&gt;Each individual selection is independent of the others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The only way of doing a random sampling is to assign a unique identity number to each population individual (conducting a &lt;em&gt;census&lt;/em&gt;) and performing a random drawing.&lt;/p&gt;
&lt;h2 id=&#34;statistical-variables&#34;&gt;Statistical variables&lt;/h2&gt;
&lt;p&gt;In every statistical study we are interested in some properties or characteristics of individuals.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Statistical variable&lt;/strong&gt;. A &lt;em&gt;statistical variable&lt;/em&gt; is a property or characteristic measured in the population individuals.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;em&gt;data&lt;/em&gt; is the actual values or outcomes recorded on a statistical variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/introduction/statistical_variables.png&#34; alt=&#34;Statistical variables&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;types-of-statistical-variables&#34;&gt;Types of statistical variables&lt;/h3&gt;
&lt;p&gt;According to the nature of their values and their scale, they can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Qualitative variables&lt;/strong&gt;. They measure non-numeric qualities. They can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Nominals&lt;/strong&gt;: There is no natural order between its categories. Example: The hair colour or the gender.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ordinals&lt;/strong&gt;: There is a natural order between its categories. Example: The education level.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantitative variables&lt;/strong&gt;: They measure numeric quantities. They can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete&lt;/strong&gt;: Their values are isolated numbers (usually integers). Example: The number of children or cars in a family.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Continuous&lt;/strong&gt;: They can take any value in a real interval. Example: The height, weight or age of a person.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Qualitative and discrete variables are also called &lt;em&gt;categorical variables&lt;/em&gt; and their values &lt;em&gt;categories&lt;/em&gt;.&lt;/p&gt;
&lt;img src=&#34;../img/introduction/variable_types.svg&#34; alt=&#34;Types of statistical variables&#34; width=&#34;800px&#34;&gt;
&lt;h4 id=&#34;choosing-the-appropriate-type-of-variable&#34;&gt;Choosing the appropriate type of variable&lt;/h4&gt;
&lt;p&gt;Sometimes a characteristic could be measured in variables of different types.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Whether a person smokes or not could be measure in several ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Smokes: yes/no. (Nominal)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Smoking level: No smoking/unusual/moderate/quite/heavy. (Ordinal)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of cigarettes per day: 0,1,2,&amp;hellip;(Discrete)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In those cases quantitative variables are preferable to qualitative, continuous variables are preferable to discrete variables and ordinal variables are preferable to nominal, as they give more information.&lt;/p&gt;
&lt;img src=&#34;../img/introduction/variable_information.svg&#34; alt=&#34;Amount of information of the different types of variables&#34; width=&#34;600px&#34;&gt;
&lt;p&gt;According to their role in the study:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Independent variables&lt;/strong&gt;: Variables that do not depend on other variables in the study. Usually they are manipulate in an experiment in order to observe their effect on a dependent variable. They are also known as &lt;em&gt;predictor variables&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dependent variables&lt;/strong&gt;: Variables that depend on other variables in the study. They are not manipulated in an experiment and are also known as &lt;em&gt;outcome variables&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In a study on the performance of students in a course, the intelligence of students and the daily study time are independent variables, while the course grade is a dependent variable.&lt;/p&gt;
&lt;h3 id=&#34;types-of-statistical-studies&#34;&gt;Types of statistical studies&lt;/h3&gt;
&lt;p&gt;According to their role in the study:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Experimental&lt;/strong&gt;: When the independent variables are manipulated in order to see the effect that that change has on the dependent variables.&lt;br&gt;
&lt;strong&gt;Example&lt;/strong&gt;. In a study on the performance of students in a test, the teacher manipulates the methodology and creates two or more groups following different methodologies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-experimental&lt;/strong&gt;: When the independent variables are not manipulated. That does not mean that it is impossible to do so, but it will either be impractical or unethical to do so.&lt;br&gt;
&lt;strong&gt;Example&lt;/strong&gt;. In a study a researcher could be interested in the effect of smoking over the lung cancer. However, whilst possible, it would be unethical to ask individuals to smoke in order to study what effect this had on their lungs. In this case, the researcher could study two groups of people, one with lung cancer and other without, an observe in each group how many persons smoke or not.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Experimental studies allow to identify a cause and effect between variables while non-experimental studies only allow to identify association or relationship between variables.&lt;/p&gt;
&lt;h3 id=&#34;the-data-table&#34;&gt;The data table&lt;/h3&gt;
&lt;p&gt;The variables of a study will be measured in each individual of the sample. This will give a data set that usually is arranged in a tabular form known as &lt;em&gt;data table&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this table each column contains the information of a variable and each row contains the information of an individual.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The table below contains data about the variables Name, Age, Gender, Weight and Height of a sample of 6 persons.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Name&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Age&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Gender&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Weight(Kg)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Height(cm)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;José Luis Martínez&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;H&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rosa Díaz&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;32&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;65&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;173&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Javier García&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;24&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;H&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;71&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;181&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Carmen López&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;65&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;170&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Marisa López&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;46&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;51&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;158&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Antonio Ruiz&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;68&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;H&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;66&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;174&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;phases-of-a-statistical-study&#34;&gt;Phases of a statistical study&lt;/h2&gt;
&lt;p&gt;Usually a statistical study goes through the following phases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The study begins with a previous design in which the study goals, the population, the variables to measure and the required sample size are set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, the sample is selected from the population and the variables are measured in the individuals of the sample (getting the data table). This is accomplished by &lt;em&gt;Sampling&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The next step consists in describing and summarizing the information of the sample. This is the job of &lt;em&gt;Descriptive Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then, the information obtained is projected on a mathematical model that intend to explain what happens in population, and the model is validated. This is accomplished by &lt;em&gt;Inferential Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, the validated model is used to perform predictions and to draw conclusions on the population.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-statistical-cycle&#34;&gt;The statistical cycle&lt;/h3&gt;
&lt;img src=&#34;../img/introduction/statistical_cycle.svg&#34; alt=&#34;Statistical cycle&#34; width=&#34;600px&#34;&gt;
</description>
    </item>
    
    <item>
      <title>Descriptive Statistics</title>
      <link>/en/teaching/statistics/manual/descriptive-statistics/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/teaching/statistics/manual/descriptive-statistics/</guid>
      <description>&lt;p&gt;Descriptive Statistics is the part of Statistics in charge of representing, analysing and summarizing the information contained in the sample.&lt;/p&gt;
&lt;p&gt;After the sampling process, this is the next step in every statistical study and usually consists of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To classify, group and sort the data of the sample.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To tabulate and plot data according to their frequencies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To calculate numerical measures that summarize the information contained in the sample (&lt;em&gt;sample statistics&lt;/em&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It has no inferential power, so &lt;em&gt;do not generalize to the population from the measures computed by Descriptive Statistics!&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;frequency-distribution&#34;&gt;Frequency distribution&lt;/h2&gt;
&lt;p&gt;The study of a statistical variable starts by measuring the variable in the individuals of the sample and classifying the values.&lt;/p&gt;
&lt;p&gt;There are two ways of classifying data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-grouping&lt;/strong&gt;: Sorting values from lowest to highest value (if there is an order). Used with qualitative variables and discrete variables with few distinct values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Grouping&lt;/strong&gt;: Grouping values into intervals (classes) and sort them from lowest to highest intervals. Used with continuous variables and discrete variables with many distinct values.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sample-classification&#34;&gt;Sample classification&lt;/h3&gt;
&lt;p&gt;It consists in grouping the values that are the same and sorting them if there is an order among them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. $X=$Height&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/sample_classification.png&#34; alt=&#34;Sample classification&#34; width=400px&gt;
&lt;h3 id=&#34;frequency-count&#34;&gt;Frequency count&lt;/h3&gt;
&lt;p&gt;It consists in counting the number of times that every value appears in the sample.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. $X=$Height&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/frequency_count.png&#34; alt=&#34;Frequency count&#34; width=400px&gt;
&lt;h4 id=&#34;sample-frequencies&#34;&gt;Sample frequencies&lt;/h4&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample frequencies&lt;/strong&gt;. Given a sample of $n$ values of a variable $X$, for every value $x_i$ of the variable we define&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Absolute Frequency $n_i$&lt;/strong&gt;: The number of times that value $x_i$ appears in the sample.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relative Frequency $f_i$&lt;/strong&gt;: The proportion of times that value $x_i$ appears in the sample.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$f_i = \frac{n_i}{n}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cumulative Absolute Frequency $N_i$&lt;/strong&gt;: The number of values in the sample less than or equal to $x_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$N_i = n_1 + \cdots + n_i = N_{i-1}+n_i$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cumulative Relative Frequency $F_i$&lt;/strong&gt;: The proportion of values in the sample less than or equal to $x_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$F_i = \frac{N_i}{n}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;frequency-table&#34;&gt;Frequency table&lt;/h3&gt;
&lt;p&gt;The set of values of a variable with their respective frequencies is called &lt;strong&gt;frequency distribution&lt;/strong&gt; of the variable in the sample, and it is usually represented as a &lt;strong&gt;frequency table&lt;/strong&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$X$ values&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Absolute frequency&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Relative frequency&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Cumulative absolute frequency&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Cumulative relative frequency&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$n_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$f_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$N_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$F_1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$n_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$f_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$N_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$F_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\vdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$n_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$f_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$N_k$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$F_k$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Example - Quantitative variable and non-grouped data&lt;/strong&gt;. The number of children in 25 families are:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
1, 2, 4, 2, 2, 2, 3, 2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 2
&lt;/div&gt;
&lt;p&gt;The frequency table for the number of children in this sample is&lt;/p&gt;
&lt;p&gt;$$ \begin{array}{rrrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; N_i &amp;amp; F_i\newline
\hline
0 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 2 &amp;amp; 0.08\newline
1 &amp;amp; 6 &amp;amp; 0.24 &amp;amp; 8 &amp;amp; 0.32\newline
2 &amp;amp; 14 &amp;amp; 0.56 &amp;amp; 22 &amp;amp; 0.88\newline
3 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 24 &amp;amp; 0.96\newline
4 &amp;amp; 1 &amp;amp; 0.04 &amp;amp; 25 &amp;amp; 1 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example - Quantitative variable and grouped data&lt;/strong&gt;. The heights (in cm) of 30 students are:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
179, 173, 181, 170, 158, 174, 172, 166, 194, 185,&lt;br&gt;
162, 187, 198, 177, 178, 165, 154, 188, 166, 171,&lt;br&gt;
175, 182, 167, 169, 172, 186, 172, 176, 168, 187.
&lt;/div&gt;
&lt;p&gt;The frequency table for the height in this sample is&lt;/p&gt;
&lt;p&gt;$$ \begin{array}{crrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; N_i &amp;amp; F_i\newline
\hline
(150,160] &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 2 &amp;amp; 0.07\newline
(160,170] &amp;amp; 8 &amp;amp; 0.27 &amp;amp; 10 &amp;amp; 0.34\newline
(170,180] &amp;amp; 11 &amp;amp; 0.36 &amp;amp; 21 &amp;amp; 0.70\newline
(180,190] &amp;amp; 7 &amp;amp; 0.23 &amp;amp; 28 &amp;amp; 0.93\newline
(190,200] &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 30 &amp;amp; 1 \newline
\hline
\sum &amp;amp; 30 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h4 id=&#34;classes-construction&#34;&gt;Classes construction&lt;/h4&gt;
&lt;p&gt;Intervals are known as &lt;strong&gt;classes&lt;/strong&gt; and the center of intervals as &lt;strong&gt;class marks&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When grouping data into intervals, the following rules must be taken into account:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of intervals should not be too big nor too small.
A usual rule of thumb is to take a number of intervals approximately $\sqrt{n}$ or $\log_2(n)$.&lt;/li&gt;
&lt;li&gt;The intervals must not overlap and must cover the entire range of values.
It does not matter if intervals are left-open and right-closed or vice versa.&lt;/li&gt;
&lt;li&gt;The minimum value must fall in the first interval and the maximum value in the last.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example - Qualitative variable&lt;/strong&gt;. The blood types of 30 people are:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
A, B, B, A, AB, 0, 0, A, B, B, A, A, A, A, AB, A, A, A, B, 0, B, B, B, A, A, A, 0, A, AB, 0.
&lt;/div&gt;
&lt;p&gt;The frequency table of the blood type in this sample is&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crr}
\hline x_i &amp;amp; n_i &amp;amp; f_i \newline
\hline \mbox{0} &amp;amp; 5 &amp;amp; 0.16 \newline
\mbox{A} &amp;amp; 14 &amp;amp; 0.47 \newline
\mbox{B} &amp;amp; 8 &amp;amp; 0.27 \newline
\mbox{AB} &amp;amp; 3 &amp;amp; 0.10 \newline
\hline
\sum &amp;amp; 30 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Observe that in this case cumulative frequencies are nonsense as there is no order in the variable.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;frequency-distribution-graphs&#34;&gt;Frequency distribution graphs&lt;/h2&gt;
&lt;p&gt;Usually the frequency distribution is also displayed graphically.
Depending on the type of variable and whether data has been grouped or not, there are different types of charts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bar chart&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Histogram&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Line or polygon chart.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pie chart&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bar-chart&#34;&gt;Bar chart&lt;/h3&gt;
&lt;!--TODO: Insert bar chart animation https://tinystats.github.io/teacups-giraffes-and-statistics/02_bellCurve.html --&gt;
&lt;p&gt;A &lt;strong&gt;bar chart&lt;/strong&gt; consists of a set of bars, one for every value or category of the variable, plotted on a coordinate system.&lt;/p&gt;
&lt;p&gt;Usually the values or categories of the variable are represented on the $x$-axis, and the frequencies on the $y$-axis.
For each value or category of the variable, a bar is draw to the height of its frequency. The width of the bar is not important but bars should be clearly separated among them.&lt;/p&gt;
&lt;p&gt;Depending on the type of frequency represented in the $y$-axis we get different types of bar charts.&lt;/p&gt;
&lt;p&gt;Sometimes a polygon, known as &lt;strong&gt;frequency polygon&lt;/strong&gt;, is plotted joining the top of every bar with straight lines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The bar chart below shows the absolute frequency distribution of the number of children in the previous sample.&lt;/p&gt;


&lt;div id=&#34;chart-852743619&#34; class=&#34;chart pb-3&#34; style=&#34;max-width: 100%; margin: auto;&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;./img/absolute-barchart.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-852743619&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();
&lt;/script&gt;
&lt;p&gt;The bar chart below shows the relative frequency distribution of the number of children with the frequency polygon.&lt;/p&gt;
&lt;div id=&#34;relative-barchart&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;
&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;relative-barchart&#34;&gt;
{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,1,2,3,4],&#34;y&#34;:[0.08,0.24,0.56,0.08,0.04],&#34;name&#34;:&#34;bar&#34;, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,1,2,3,4],&#34;y&#34;:[0.08,0.24,0.56,0.08,0.04],&#34;name&#34;:&#34;polygon&#34;, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Relative frequency distribution of number of children&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Number of children&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Relative frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0.5,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;filename&#34;:&#34;Relative frequency distribution of number of children&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;p&gt;The bar chart below shows the cumulative absolute frequency distribution of the number of children.&lt;/p&gt;
&lt;div id=&#34;cumulative-absolute-barchart&#34; class=&#34;plotly&#34; style=&#34;margin: 25px auto; width:80%&#34;&gt;
&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;cumulative-absolute-barchart&#34;&gt;
{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,1,2,3,4],&#34;y&#34;:[2,8,22,24,25], &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Cumulative absolute frequency distribution of number of children&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Number of children&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Cumulative absolute frequency&#34;},&#34;autosize&#34;:false, &#34;width&#34;:600, &#34;height&#34;:400,&#34;barsgap&#34;:0.5,&#34;bargap&#34;:0.5,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;filename&#34;:&#34;Cumulative absolute frequency distribution of number of children&#34;},&#34;evals&#34;:[]}
&lt;/script&gt;
&lt;p&gt;And the bar chart below shows the cumulative relative frequency distribution of the number of children with the frequency polygon.&lt;/p&gt;
&lt;div id=&#34;cumulative-relative-barchart&#34; class=&#34;plotly&#34; style=&#34;margin: 25px auto; width:80%&#34;&gt;
&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;cumulative-relative-barchart&#34;&gt;
{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,0,1,2,3,4],&#34;y&#34;:[0,0.08,0.32,0.88,0.96,1],&#34;name&#34;:&#34;bar&#34;, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[0,0,1,2,3,4],&#34;y&#34;:[0,0.08,0.32,0.88,0.96,1],&#34;name&#34;:&#34;polygon&#34;,&#34;line&#34;:{&#34;shape&#34;:&#34;hv&#34;}, &#34;marker&#34;:{&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Cumulative relative frequency distribution of number of children&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Number of children&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Cumulative relative frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0.5,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}}, &#34;filename&#34;:&#34;Cumulative relative frequency distribution of number of children&#34;},&#34;evals&#34;:[]}
&lt;/script&gt;
&lt;h3 id=&#34;histogram&#34;&gt;Histogram&lt;/h3&gt;
&lt;p&gt;A &lt;em&gt;histogram&lt;/em&gt; is similar to a bar chart but for grouped data.&lt;/p&gt;
&lt;p&gt;Usually the classes or grouping intervals are represented on the $x$-axis, and the frequencies on the $y$-axis. For each class, a bar is draw to the height of its frequency. Contrary to bar charts, the width of bars coincides with the width of classes, and there are no space between two consecutive bars.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/histogram_creation.gif&#34; alt=&#34;Sample classification&#34; width=&#34;400&#34;&gt;
&lt;p&gt;Depending on the type of frequency represented in the $y$-axis we get different types of histograms.&lt;/p&gt;
&lt;p&gt;As with the bar chart, the &lt;em&gt;frequency polygon&lt;/em&gt; can be drawn joining the top centre of every bar with straight lines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The histogram below shows the absolute frequency distribution of heights.&lt;/p&gt;
&lt;div id=&#34;absolute-histogram&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;absolute-histogram&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;histogram&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[179,173,181,169,158,174,172,166,194,185,162,187,198,177,178,165,154,188,166,171,175,182,167,169,172,186,172,176,168,187],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;}}],&#34;layout&#34;:{&#34;title&#34;:&#34;Absolute frequency distribution of heights&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Height&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Absolute frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Absolute frequency distribution of heights&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Height&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Absolute frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400},&#34;filename&#34;:&#34;Absolute frequency distribution of heights&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;p&gt;The histogram below shows the relative frequency distribution of heights with the frequency polygon.&lt;/p&gt;
&lt;div id=&#34;relative-histogram&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;relative-histogram&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;histogram&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[179,173,181,169,158,174,172,166,194,185,162,187,198,177,178,165,154,188,166,171,175,182,167,169,172,186,172,176,168,187],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;},&#34;histnorm&#34;:&#34;probability&#34;,&#34;name&#34;:&#34;bar&#34;},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[155,165,175,185,195],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;},&#34;histnorm&#34;:&#34;probability&#34;,&#34;name&#34;:&#34;polygon&#34;,&#34;y&#34;:[0.0667,0.2667,0.3667,0.2333,0.0667]}],&#34;layout&#34;:{&#34;title&#34;:&#34;Relative frequency distribution of heights&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Height&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Relative frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Relative frequency distribution of heights&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Height&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Relative frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;showlegend&#34;:false},&#34;filename&#34;:&#34;Relative frequency distribution of heights&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;p&gt;The cumulative frequency polygon (for absolute or relative frequencies) is known as &lt;strong&gt;ogive&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The histogram and the ogive below show the cumulative relative distribution of heights.&lt;/p&gt;
&lt;div id=&#34;ogive&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;ogive&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;bar&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[155,165,175,185,195],&#34;y&#34;:[0.0666666666666667,0.333333333333333,0.7,0.933333333333333,1],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(137,211,243,1)&#34;},&#34;name&#34;:&#34;bar&#34;},{&#34;type&#34;:&#34;scatter&#34;,&#34;inherit&#34;:true,&#34;x&#34;:[150,160,170,180,190,200],&#34;y&#34;:[0,0.0666666666666667,0.333333333333333,0.7,0.933333333333333,1],&#34;marker&#34;:{&#34;line&#34;:{&#34;width&#34;:1},&#34;color&#34;:&#34;rgba(238,50,36,1)&#34;},&#34;name&#34;:&#34;ogive&#34;}],&#34;layout&#34;:{&#34;title&#34;:&#34;Cumulative relative frequency distribution of heights&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Height&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Cumulative relative frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0,&#34;showlegend&#34;:false,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Cumulative relative frequency distribution of heights&#34;,&#34;xaxis&#34;:{&#34;title&#34;:&#34;Height&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Cumulative relative frequency&#34;},&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;bargap&#34;:0,&#34;showlegend&#34;:false},&#34;filename&#34;:&#34;Cumulative relative frequency distribution of heights&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;p&gt;Observe that in the ogive we join the top right corner of bars with straight lines, instead of the top center, because we do not reach the accumulated frequency of the class until the end of the interval.&lt;/p&gt;
&lt;h3 id=&#34;pie-chart&#34;&gt;Pie chart&lt;/h3&gt;
&lt;p&gt;A &lt;em&gt;pie chart&lt;/em&gt; consists of a circle divided in slices, one for every value or category of the variable. Each slice is called a &lt;em&gt;sector&lt;/em&gt; and its angle or area is proportional to the frequency of the corresponding value or category.&lt;/p&gt;
&lt;p&gt;Pie charts can represent absolute or relative frequencies, but not cumulative frequencies, and are used with nominal qualitative variables. For ordinal qualitative or quantitative variables is better to use bar charts, because it is easier to perceive differences in one dimension (length of bars) than in two dimensions (areas of sectors).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The pie chart below shows the relative frequency distribution of blood types.&lt;/p&gt;
&lt;div id=&#34;piechart&#34; class=&#34;plotly&#34; style=&#34;margin: auto; width:80%&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;piechart&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;type&#34;:&#34;pie&#34;,&#34;inherit&#34;:true,&#34;labels&#34;:[&#34;0&#34;,&#34;A&#34;,&#34;AB&#34;,&#34;B&#34;],&#34;values&#34;:[5,14,3,8]}],&#34;layout&#34;:{&#34;title&#34;:&#34;Relative frequency distribution of blood types&#34;,&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400,&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10}},&#34;url&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;base_url&#34;:&#34;https://plot.ly&#34;,&#34;layout.1&#34;:{&#34;title&#34;:&#34;Relative frequency distribution of blood types&#34;,&#34;autosize&#34;:false,&#34;width&#34;:600,&#34;height&#34;:400},&#34;filename&#34;:&#34;Relative frequency distribution of blood types&#34;},&#34;evals&#34;:[]}&lt;/script&gt;
&lt;h3 id=&#34;the-normal-distribution&#34;&gt;The normal distribution&lt;/h3&gt;
&lt;p&gt;Distributions with different properties will show different shapes.&lt;/p&gt;
&lt;!-- TODO: Insert histograms with different shapes --&gt;
&lt;h2 id=&#34;outliers&#34;&gt;Outliers&lt;/h2&gt;
&lt;p&gt;One of the main problems in samples are &lt;strong&gt;outliers&lt;/strong&gt;, values very different from the rest of values of the sample.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The last height of the following sample of heights is an outlier.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/outlier.png&#34; alt=&#34;Outlier&#34; width=&#34;400&#34;&gt;
&lt;p&gt;It is important to find out outliers before doing any analysis, because outliers usually distort the results.&lt;/p&gt;
&lt;p&gt;They always appears in the ends of the distribution, and can be found out easily with a box and whiskers chart (as be show later).&lt;/p&gt;
&lt;h3 id=&#34;outliers-management&#34;&gt;Outliers management&lt;/h3&gt;
&lt;p&gt;With big samples outliers have less importance and can be left in the sample.&lt;/p&gt;
&lt;p&gt;With small samples we have several options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remove the outlier if it is an error.&lt;/li&gt;
&lt;li&gt;Replace the outlier by the lower or higher value in the distribution that is not an outlier if it is not an error and the outlier does not fit the theoretical distribution.&lt;/li&gt;
&lt;li&gt;Leave the outlier if it is not an error, and change the theoretical model to fit it to outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sample-statistics&#34;&gt;Sample statistics&lt;/h2&gt;
&lt;p&gt;The frequency table and charts summarize and give an overview of the distribution of values of the studied variable in the sample, but it is difficult to describe some aspects of the distribution from it, as for example, which are the most representative values of the distribution, how is the spread of data, which data could be considered outliers, or how is the symmetry of the distribution.&lt;/p&gt;
&lt;p&gt;To describe those aspects of the sample distribution more specific numerical measures, called &lt;strong&gt;sample statistics&lt;/strong&gt;, are used.&lt;/p&gt;
&lt;p&gt;According to the aspect of the distribution that they study, there are different types of statistics:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Measures of locations&lt;/strong&gt;: They measure the values where data are concentrated or that divide the distribution into equal parts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Measures of dispersion&lt;/strong&gt;: They measure the spread of data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Measures of shape&lt;/strong&gt;: They measure aspects related to the shape of the distribution , as the symmetry and the concentration of data around the mean.&lt;/p&gt;
&lt;h2 id=&#34;location-statistics&#34;&gt;Location statistics&lt;/h2&gt;
&lt;p&gt;There are two groups:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Central location measures&lt;/strong&gt;: They measure the values where data are concentrated, usually at the centre of the distribution. These values are the values that best represents the sample data. The most important are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Arithmetic mean&lt;/li&gt;
&lt;li&gt;Median&lt;/li&gt;
&lt;li&gt;Mode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Non-central location measures&lt;/strong&gt;: They divide the sample data into equals parts. The most important are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quartiles.&lt;/li&gt;
&lt;li&gt;Deciles.&lt;/li&gt;
&lt;li&gt;Percentiles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;arithmetic-mean&#34;&gt;Arithmetic mean&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample arithmetic mean $\bar{x}$&lt;/strong&gt;. The &lt;em&gt;sample arithmetic mean&lt;/em&gt; of a variable $X$ is the sum of observed values in the sample divided by the sample size&lt;/p&gt;
&lt;p&gt;$$\bar{x} = \frac{\sum x_i}{n}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It can be calculated from the frequency table with the formula&lt;/p&gt;
&lt;p&gt;$$\bar{x} = \frac{\sum x_in_i}{n} = \sum x_i f_i$$&lt;/p&gt;
&lt;p&gt;In most cases the arithmetic mean is the value that best represent the observed values in the sample.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Watch out!&lt;/strong&gt; It can not be calculated with qualitative variables.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example - Non-grouped data&lt;/strong&gt;. Using the data of the sample with the number of children of families, the arithmetic mean is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\bar{x} &amp;amp;= \frac{1+2+4+2+2+2+3+2+1+1+0+2+2}{25}+\newline\newline
&amp;amp;+\frac{0+2+2+1+2+2+3+1+2+2+1+2}{25} = \frac{44}{25} = 1.76 \mbox{ children}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;or using the frequency table&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; x_in_i &amp;amp; x_if_i\newline
\hline
0 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 0 &amp;amp; 0\newline
1 &amp;amp; 6 &amp;amp; 0.24 &amp;amp; 6 &amp;amp; 0.24\newline
2 &amp;amp; 14 &amp;amp; 0.56 &amp;amp; 28 &amp;amp; 1.12\newline
3 &amp;amp; 2  &amp;amp; 0.08 &amp;amp; 6 &amp;amp; 0.24\newline
4 &amp;amp; 1 &amp;amp; 0.04 &amp;amp; 4 &amp;amp; 0.16 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 1 &amp;amp; 44 &amp;amp; 1.76 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$
\bar{x} = \frac{\sum x_in_i}{n} = \frac{44}{25}= 1.76 \mbox{ children}\qquad \bar{x}=\sum{x_if_i} = 1.76 \mbox{ children}.
$$&lt;/p&gt;
&lt;p&gt;That means that the value that best represent the number of children in the families of the sample is 1.76 children.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example - Grouped data&lt;/strong&gt;. Using the data of the sample of student heights, the arithmetic mean is&lt;/p&gt;
&lt;p&gt;$$\bar{x} = \frac{179+173+\cdots+187}{30} = 175.07 \mbox{ cm}.$$&lt;/p&gt;
&lt;p&gt;or using the frequency table and taking the class marks as $x_i$,&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crrrrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; x_in_i &amp;amp; x_if_i\newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 310 &amp;amp; 10.33\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; 0.27 &amp;amp; 1320 &amp;amp; 44.00\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 0.36 &amp;amp; 1925 &amp;amp; 64.17\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 0.23 &amp;amp; 1295 &amp;amp; 43.17\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 0.07 &amp;amp; 390 &amp;amp; 13 \newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; 1 &amp;amp; 5240 &amp;amp; 174.67 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$
\bar{x} = \frac{\sum x_in_i}{n} = \frac{5240}{30}= 174.67 \mbox{ cm} \qquad \bar{x}=\sum{x_if_i} = 174.67 \mbox{ cm}.
$$&lt;/p&gt;
&lt;p&gt;Observe that when the mean is calculated from the table the result differs a little from the real value, because the values used in the calculations are the class marks instead of the actual values.&lt;/p&gt;
&lt;h4 id=&#34;weighted-mean&#34;&gt;Weighted mean&lt;/h4&gt;
&lt;p&gt;In some cases the values of the sample have different importance. In that case the importance or &lt;em&gt;weight&lt;/em&gt; of each value of the sample must be taken into account when calculating the mean.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample weighted mean $\bar{x}_p$&lt;/strong&gt;. Given a sample of values $x_1,\ldots,x_n$ where every value $x_i$ has a weight $w_i$, the &lt;em&gt;sample weighted mean&lt;/em&gt; of variable $X$ is the sum of the product of each value by its weight, divided by sum of weights&lt;/p&gt;
&lt;p&gt;$$\bar{x}_w = \frac{\sum x_iw_i}{\sum w_i}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;From the frequency table can be calculated with the formula&lt;/p&gt;
&lt;p&gt;$$\bar{x}_w = \frac{\sum x_iw_in_i}{\sum w_i}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Assume that a student wants to calculate a representative measure of his/her performance in a course. The grade and the credits of every subjects are&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Subject&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Credits&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Grade&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Maths&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Economics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Chemistry&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The arithmetic mean is&lt;/p&gt;
&lt;p&gt;$$\bar{x} = \frac{\sum x_i}{n} = \frac{5+3+6}{3}= 4.67 \text{ points}.$$&lt;/p&gt;
&lt;p&gt;However, this measure does not represent well the performance of the student, as not all the subjects have the same importance and require the same effort to pass. Subjects with more credits require more work and must have more weight in the calculation of the mean.&lt;/p&gt;
&lt;p&gt;In this case it is better to use the weighted mean, using the credits as the weights of grades, as a representative measure of the student effort&lt;/p&gt;
&lt;p&gt;$$
\bar{x}_w = \frac{\sum x_iw_i}{\sum w_i} = \frac{5\cdot 6+3\cdot 4+6\cdot 8}{6+4+8}= \frac{90}{18} = 5 \text{ points}.
$$&lt;/p&gt;
&lt;h3 id=&#34;median&#34;&gt;Median&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sample median $Me$&lt;/strong&gt;. The &lt;em&gt;sample median&lt;/em&gt; of a variable $X$ is the value that is in the middle of the ordered sample.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The median divides the sample distribution into two equal parts, that is, there are the same number of values above and below the median. Therefore, it has cumulative frequencies $N_{Me}= n/2$ y $F_{Me}= 0.5$.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Watch out! It can not be calculated for nominal variables.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With non-grouped data, there are two possibilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Odd sample size: The median is the value in the position $\frac{n+1}{2}$.&lt;/li&gt;
&lt;li&gt;Even sample size: The median is the average of values in positions $\frac{n}{2}$ and $\frac{n}{2}+1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptive/median.svg&#34; alt=&#34;Median calculation for non-grouped data&#34; width=&#34;700&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Using the data of the sample with the number of children of families, the sample size is 25, that is odd, and the median is the value in the
position $\frac{25+1}{2} = 13$ of the sorted sample.&lt;/p&gt;
&lt;p&gt;$$0,0,1,1,1,1,1,1,2,2,2,2,\fbox{2},2,2,2,2,2,2,2,2,2,3,3,4$$&lt;/p&gt;
&lt;p&gt;And the median is 2 children.&lt;/p&gt;
&lt;p&gt;With the frequency table, the median is the lowest value with a cumulative absolute frequency greater than or equal to $13$, or with a cumulative relative frequency greater than or equal to $0.5$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrrrr}
\hline
x_i &amp;amp; n_i &amp;amp; f_i &amp;amp; N_i &amp;amp; F_i\newline
\hline
0 &amp;amp; 2 &amp;amp; 0.08 &amp;amp; 2 &amp;amp; 0.08\newline
1 &amp;amp; 6 &amp;amp; 0.24 &amp;amp; 8 &amp;amp; 0.32\newline
\color{red}2 &amp;amp; 14 &amp;amp; 0.56 &amp;amp; 22 &amp;amp; 0.88\newline
3 &amp;amp; 2  &amp;amp; 0.08 &amp;amp; 24 &amp;amp; 0.96\newline
4 &amp;amp; 1 &amp;amp; 0.04 &amp;amp; 25 &amp;amp; 1 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h4 id=&#34;median-calculation-for-grouped-data&#34;&gt;Median calculation for grouped-data&lt;/h4&gt;
&lt;p&gt;For grouped data the median is calculated from the ogive, interpolating in the class with cumulative relative frequency 0.5.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/interpolation.svg&#34; alt=&#34;Median calculation for grouped data&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Both expressions are equal as the angle $\alpha$ is the same, and solving the equation we get that the formula for the median is&lt;/p&gt;
&lt;p&gt;$$
Me=l_i+\frac{0.5-F_{i-1}}{F_i-F_{i-1}}(l_i-l_{i-1})=l_i+\frac{0.5-F_{i-1}}{f_i}a_i
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example - Grouped data&lt;/strong&gt;. Using the data of the sample of student heights, the median falls in class (170,180].&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/interpolation_example_1.svg&#34; alt=&#34;Example of median calculation for grouped data&#34; width=&#34;600&#34;&gt;
&lt;p&gt;And interpolating in interval (170,180] we get&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/interpolation_example_2.svg&#34; alt=&#34;Example of median calculation for grouped data&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Equating both expressions and solving the equation, we get&lt;/p&gt;
&lt;p&gt;$$
Me= 170+\frac{0.5-0.34}{0.7-0.34}(180-170)=170+\frac{0.16}{0.36}10=174.54 \mbox{ cm}.
$$&lt;/p&gt;
&lt;p&gt;This means that half of the students in the sample have an height lower than or equal to 174.54 cm and the other half greater than or equal to.&lt;/p&gt;
&lt;h3 id=&#34;mode&#34;&gt;Mode&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sample Mode $Mo$&lt;/strong&gt;. The &lt;em&gt;sample mode&lt;/em&gt; of a variable $X$ is the most frequent value in the sample.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;With grouped data the &lt;em&gt;modal class&lt;/em&gt; is the class with the highest frequency.&lt;/p&gt;
&lt;p&gt;It can be calculated for all types of variables (qualitative and quantitative).&lt;/p&gt;
&lt;p&gt;Distributions can have more than one mode.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/mode.png&#34; alt=&#34;Mode calculation&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Using the data of the sample with the number of children of families, the value with the highest frequency is $2$, that is the mode $Mo = 2$
children.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rr}
\hline
x_i &amp;amp; n_i \newline
\hline
0 &amp;amp; 2 \newline
1 &amp;amp; 6 \newline
\color{red} 2 &amp;amp; 14 \newline
3 &amp;amp; 2  \newline
4 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Using the data of the sample of student heights, the class with the highest frequency is $(170,180]$ that is the modal class $Mo=(170,180]$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cr}
\hline
X &amp;amp; n_i \newline
\hline
(150,160] &amp;amp; 2 \newline
(160,170] &amp;amp; 8 \newline
\color{red}{(170,180]} &amp;amp; 11 \newline
(180,190] &amp;amp; 7 \newline
(190,200] &amp;amp; 2 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h3 id=&#34;which-central-tendency-statistic-should-i-use&#34;&gt;Which central tendency statistic should I use?&lt;/h3&gt;
&lt;p&gt;In general, when all the central tendency statistics can be calculated, is advisable to use them as representative values in the following order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The mean. Mean takes more information from the sample than the others, as it takes into account the magnitude of data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The median. Median takes less information than mean but more than mode, as it takes into account the order of data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The mode. Mode is the measure that fewer information takes from the sample, as it only takes into account the absolute frequency of values.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But, &lt;em&gt;be careful with outliers&lt;/em&gt;, as the mean can be distorted by them. In that case it is better to use the median as the value most representative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. If a sample of number of children of 7 families is&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
0, 0, 1, 1, 2, 2, 15,
&lt;/div&gt;
&lt;p&gt;then, $\bar{x}=3$ children and $Me=1$ children.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Which measure represent better the number of children in the sample?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;non-central-location-measures&#34;&gt;Non-central location measures&lt;/h3&gt;
&lt;p&gt;The non-central location measures or &lt;em&gt;quantiles&lt;/em&gt; divide the sample distribution in equal parts.&lt;/p&gt;
&lt;p&gt;The most used are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quartiles&lt;/strong&gt;: Divide the distribution into 4 equal parts. There are 3 quartiles: $C_1$ (25% accumulated) , $C_2$ (50% accumulated), $C_3$ (75% accumulated).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deciles&lt;/strong&gt;: Divide the distribution into 10 equal parts. There are 9 deciles: $D_1$ (10% accumulated) ,…, $D_9$ (90% accumulated).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Percentiles&lt;/strong&gt;: Divide the distribution into en 100 equal parts. There are 99 percentiles: $P_1$ (1% accumulated),…, $P_{99}$ (99% accumulated).&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/quantiles.svg&#34; alt=&#34;Quartiles, deciles and percentiles&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Observe that there is a correspondence between quartiles, deciles and percentiles. For example, first quartile coincides with percentile 25, and fourth decile coincides with the percentile 40.&lt;/p&gt;
&lt;p&gt;Quantiles are calculated in a similar way to the median. The only difference lies in the cumulative relative frequency that correspond to every quantile.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/quantiles_calculation.svg&#34; alt=&#34;Quartiles, deciles and percentiles calculation&#34; width=&#34;600&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Using the data of the sample with the number of children of families, the cumulative relative frequencies were&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rr}
\hline
x_i &amp;amp; F_i \newline
\hline
0 &amp;amp; 0.08\newline
1 &amp;amp; 0.32\newline
2 &amp;amp; 0.88\newline
3 &amp;amp; 0.96\newline
4 &amp;amp; 1\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
F_{Q_1}=0.25 &amp;amp;\Rightarrow Q_1 = 1 \text{ children},\newline
F_{Q_2}=0.5 &amp;amp;\Rightarrow Q_2 = 2 \text{ children},\newline
F_{Q_3}=0.75 &amp;amp;\Rightarrow Q_3 = 2 \text{ children},\newline
F_{D_4}=0.4 &amp;amp;\Rightarrow D_4 = 2 \text{ children},\newline
F_{P_{92}}=0.92 &amp;amp;\Rightarrow P_{92} = 3 \text{ children}.
\end{aligned}$$&lt;/p&gt;
&lt;h2 id=&#34;dispersion-statistics&#34;&gt;Dispersion statistics&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Dispersion&lt;/em&gt; or &lt;em&gt;spread&lt;/em&gt; refers to the variability of data. So, dispersion statistics measure how the data values are scattered in general, or with respect to a central location measure.&lt;/p&gt;
&lt;p&gt;For quantitative variables, the most important are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Range&lt;/li&gt;
&lt;li&gt;Interquartile range&lt;/li&gt;
&lt;li&gt;Variance&lt;/li&gt;
&lt;li&gt;Standard deviation&lt;/li&gt;
&lt;li&gt;Coefficient of variation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;range&#34;&gt;Range&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample range&lt;/strong&gt;. The &lt;em&gt;sample range&lt;/em&gt; of a variable $X$ is the difference between the the maximum and the minimum values in the sample.&lt;/p&gt;
&lt;p&gt;$$\text{Range} = \max_{x_i} -\min_{x_i}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/descriptive/range.svg&#34; alt=&#34;Range&#34; width=&#34;600&#34;&gt;
&lt;p&gt;The range measures the largest variation among the sample data. However, it is very sensitive to outliers, as they appear at the ends of the distribution, and for that reason is rarely used.&lt;/p&gt;
&lt;h3 id=&#34;interquartile-range&#34;&gt;Interquartile range&lt;/h3&gt;
&lt;p&gt;The following measure avoids the problem of outliers and is much more used.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample interquartile range&lt;/strong&gt;. The &lt;em&gt;sample interquartile range&lt;/em&gt; of a variable $X$ is the difference between the third and the first sample quartiles.&lt;/p&gt;
&lt;p&gt;$$\text{IQR} = Q_3-Q_1$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/descriptive/interquartile_range.svg&#34; alt=&#34;Interquartile range&#34; width=&#34;600&#34;&gt;
&lt;p&gt;The interquartile range measures the spread of the 50% central data.&lt;/p&gt;
&lt;h3 id=&#34;box-plot&#34;&gt;Box plot&lt;/h3&gt;
&lt;p&gt;The dispersion of a variable in a sample can be graphically represented with a &lt;em&gt;box plot&lt;/em&gt;, that represent five descriptive statistics (minimum, quartiles and maximum) known as the &lt;em&gt;five-numbers&lt;/em&gt;. It consist in a box, drawn from the lower to the upper quartile, that represent the interquartile range, and two segments, known as the lower and the upper &lt;em&gt;whiskers&lt;/em&gt;. Usually the box is split in two with the median.&lt;/p&gt;
&lt;p&gt;This chart is very helpful as it serves to many purposes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It serves to measure the spread of data as it represents the range and the interquartile range.&lt;/li&gt;
&lt;li&gt;It serves to detect outliers, that are the values outside the interval defined by the whiskers.&lt;/li&gt;
&lt;li&gt;It serves to measure the symmetry of distribution, comparing the length of the boxes and whiskers above and below the median.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The chart below shows a box plot of newborn weights.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/boxplot.svg&#34; alt=&#34;Box plot of newborns weights&#34; width=&#34;800&#34;&gt;
&lt;p&gt;To create a box plot follow the steps below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the quartiles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Draw a box from the lower to the upper quartile.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Split the box with the median or second quartile.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For the whiskers calculate first two values called &lt;em&gt;fences&lt;/em&gt; $f_1$ y  $f_2$. The lower fence is the lower quartile minus one and a half the interquartile range, and the upper fence is the upper quartile plus one and a half the interquartile range:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
f_1&amp;amp;=Q_1-1.5,\text{IQR}\newline
f_2&amp;amp;=Q_3+1.5,\text{IQR}
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;The fences define the interval where data are considered normal. Any value outside that interval is considered an outlier.&lt;br&gt;
For the lower whisker draw a segment from the lower quartile to the lower value in the sample grater than or equal to $f_1$, and for the upper whisker draw a segment from the upper quartile to the highest value in the sample lower than or equal to $f_2$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The whiskers are not the fences.
  &lt;/div&gt;
&lt;/div&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Finally, if there are outliers, draw a dot at every outlier.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The box plot for the sample with the number of children si shown below.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/boxplot_children.svg&#34; alt=&#34;Box plot of number of children&#34; width=&#34;800&#34;&gt;
&lt;h3 id=&#34;deviations-from-the-mean&#34;&gt;Deviations from the mean&lt;/h3&gt;
&lt;p&gt;Another way of measuring spread of data is with respect to a central tendency measure, as for example the mean.&lt;/p&gt;
&lt;p&gt;In that case, it is measured the distance from every value in the sample to the mean, that is called &lt;strong&gt;deviation from the mean&lt;/strong&gt;·&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/deviations.svg&#34; alt=&#34;Deviations from the mean&#34; width=&#34;300&#34;&gt;
&lt;p&gt;If deviations are big, the mean is less representative than when they are small.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The grades of 3 students in a course with subjects $A$, $B$ and $C$ are shown below.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cccc}
\hline
A &amp;amp; B &amp;amp; C &amp;amp; \bar x\newline
0 &amp;amp; 5 &amp;amp; 10 &amp;amp; 5\newline
4 &amp;amp; 5 &amp;amp; 6 &amp;amp; 5\newline
5 &amp;amp; 5 &amp;amp; 5 &amp;amp; 5\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;All the students have the same mean, but, in which case does the mean represent better the course performance?&lt;/p&gt;
&lt;h3 id=&#34;variance-and-standard-deviation&#34;&gt;Variance and standard deviation&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition &amp;ndash; Sample variance $s^2$&lt;/strong&gt;. The &lt;em&gt;sample variance&lt;/em&gt; of a variable $X$ is the average of the squared deviations from the mean.&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum (x_i-\bar x)^2n_i}{n} = \sum (x_i-\bar x)^2f_i$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It can also be calculated with the formula&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum x_i^2n_i}{n} -\bar x^2= \sum (x_i^2f_i)-\bar x^2$$&lt;/p&gt;
&lt;p&gt;The variance has the units of the variable squared, and to ease its interpretation it is common to calculate its square root.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample standard deviation $s$&lt;/strong&gt;. The &lt;em&gt;sample standard deviation&lt;/em&gt; of a variable $X$ is the square root of the variance.&lt;/p&gt;
&lt;p&gt;$$s = +\sqrt{s^2}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Both variance and standard deviation measure the spread of data around the mean. When the variance or the standard deviation are small, the sample data are concentrated around the mean, and the mean is a good representative measure. In contrast, when variance or the standard deviation are high, the sample data are far from the mean, and the mean does not represent so well.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Standard deviation small&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\Rightarrow$&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Mean is representative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Standard deviation big&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\Rightarrow$&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Mean is unrepresentative&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The following samples contains the grades of 2 students in 2 subjects&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/std_deviation_interpretation.svg&#34; alt=&#34;Standard deviation interpretation&#34; width=&#34;500&#34;&gt;
&lt;p&gt;&lt;em&gt;Which mean is more representative?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example - Non-grouped data&lt;/strong&gt;. Using the data of the sample with the number of children of families, with mean $\bar x= 1.76$ children, and adding a new column to the frequency table with the squared values,&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrr}
\hline
x_i &amp;amp; n_i &amp;amp; x_i^2n_i \newline
\hline
0 &amp;amp; 2 &amp;amp; 0 \newline
1 &amp;amp; 6 &amp;amp; 6 \newline
2 &amp;amp; 14 &amp;amp; 56\newline
3 &amp;amp; 2  &amp;amp; 18\newline
4 &amp;amp; 1 &amp;amp; 16 \newline
\hline
\sum &amp;amp; 25 &amp;amp; 96 \newline
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum x_i^2n_i}{n}-\bar x^2 = \frac{96}{25}-1.76^2= 0.7424 \mbox{ children}^2.$$&lt;/p&gt;
&lt;p&gt;and the standard deviation is $s=\sqrt{0.7424} = 0.8616$ children.&lt;/p&gt;
&lt;p&gt;Compared to the range, that is 4 children, the standard deviation is not very large, so we can conclude that the dispersion of the distribution is small and consequently the mean, $\bar x=1.76$ children, represents quite well the number of children of families of the sample.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example - Grouped data&lt;/strong&gt;. Using the data of the sample with the heights of students and grouping heights in classes, we got a mean $\bar x=174.67$ cm. The calculation of variance is the same than for non-grouped data but using the class marks.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; x_i^2n_i \newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; 48050\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; 217800\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 336875\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 239575\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 76050\newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; 918350 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$s^2 = \frac{\sum x_i^2n_i}{n}-\bar x^2 = \frac{918350}{30}-174.67^2= 102.06 \mbox{ cm}^2,$$&lt;/p&gt;
&lt;p&gt;and the standard deviation is $s=\sqrt{102.06} = 10.1$ cm.&lt;/p&gt;
&lt;p&gt;This value is quite small compared to the range of the variable, that goes from 150 to 200 cm, therefore the distribution of heights has little dispersion and the mean is very representative.&lt;/p&gt;
&lt;h3 id=&#34;coefficient-of-variation&#34;&gt;Coefficient of variation&lt;/h3&gt;
&lt;p&gt;Both, variance and standard deviation, have units and that makes difficult to interpret them, specially when comparing distributions of variables with different units.&lt;/p&gt;
&lt;p&gt;For that reason it is also common to use the following dispersion measure that has no units.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample coefficient of variation $cv$&lt;/strong&gt;. The &lt;em&gt;sample coefficient of variation&lt;/em&gt; of a variable $X$ is the quotient between the sample standard deviation and the absolute value of the sample mean.&lt;/p&gt;
&lt;p&gt;$$cv = \frac{s}{|\bar x|}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The coefficient of variation measures the relative dispersion of data around the sample mean.&lt;/p&gt;
&lt;p&gt;As it has no units, it is easier to interpret: The higher the coefficient of variation is, the higher the relative dispersion with respect to the mean and the less representative the mean is.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The coefficient of variation it is very helpful to compare dispersion in distributions of different variables, even if variables have different units.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the sample of the number of children, where the mean was $\bar x=1.76$ and the standard deviation was $s=0.8616$ children, the coefficient of variation is&lt;/p&gt;
&lt;p&gt;$$cv = \frac{s}{|\bar x|} = \frac{0.8616}{|1.76|} = 0.49.$$&lt;/p&gt;
&lt;p&gt;In the sample of heights, where the mean was $\bar x=174.67$ cm and the standard deviation was $s=10.1$ cm, the coefficient of variation is&lt;/p&gt;
&lt;p&gt;$$cv = \frac{s}{|\bar x|} = \frac{10.1}{|174.67|} = 0.06.$$&lt;/p&gt;
&lt;p&gt;This means that the relative dispersion in the heights distribution is lower than in the number of children distribution, and consequently the mean of height is most representative than the mean of number of children.&lt;/p&gt;
&lt;h2 id=&#34;shape-statistics&#34;&gt;Shape statistics&lt;/h2&gt;
&lt;p&gt;They are measures that describe the shape of the distribution.&lt;/p&gt;
&lt;p&gt;In particular, the most important aspects are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Symmetry&lt;/strong&gt; It measures the symmetry of the distribution with respect to the mean. The statistics most used is the &lt;em&gt;coefficient of skewness&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kurtosis&lt;/strong&gt;: It measures the concentration of data around the mean of the distribution. The statistics most used is the &lt;em&gt;coefficient of kurtosis&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;coefficient-of-skewness&#34;&gt;Coefficient of skewness&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample coefficient of skewness $g_1$&lt;/strong&gt;. The &lt;em&gt;sample coefficient of skewness&lt;/em&gt; of a variable $X$ is the average of the deviations of values from the sample mean to cube, divided by the standard deviation to cube.&lt;/p&gt;
&lt;p&gt;$$g_1 = \frac{\sum (x_i-\bar x)^3 n_i/n}{s^3} = \frac{\sum (x_i-\bar x)^3 f_i}{s^3}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The coefficient of skewness measures the symmetric or skewness of the distribution, that is, how many values in the sample are above or below the mean and how far from it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$g_1=0$ indicates that there are the same number of values in the sample above and below the mean and equally deviated from it, and the distribution is symmetrical.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptive/symmetrical_distribution.svg&#34; alt=&#34;Symmetrical distribution&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_1&amp;lt;0$ indicates that there are more values above the mean than below it, but the values below are further from it, and the distribution is left-skewed (it has longer tail to the left).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptive/left_skewed_distribution.svg&#34; alt=&#34;Left-skewed distribution&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_1&amp;gt;0$ indicates that there are more values below the mean than above it, but the values above are further from it, and the distribution is right-skewed (it has longer tail to the right).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptive/right_skewed_distribution.svg&#34; alt=&#34;Right-skewed distribution&#34; width=&#34;600&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example - Grouped data&lt;/strong&gt;. Using the frequency table of the sample with the heights of students and adding a new column with the deviations from the mean $\bar x = 174.67$ cm to cube, we get&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{crrrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; x_i-\bar x &amp;amp; (x_i-\bar x)^3 n_i \newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; -19.67 &amp;amp; -15221.00\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; -9.67 &amp;amp; -7233.85\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 0.33 &amp;amp; 0.40\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 10.33 &amp;amp; 7716.12\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 20.33 &amp;amp; 16805.14\newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; &amp;amp; 2066.81 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$g_1 = \frac{\sum (x_i-\bar x)^3n_i/n}{s^3} = \frac{2066.81/30}{10.1^3} = 0.07.$$&lt;/p&gt;
&lt;p&gt;As it is close to 0, that means that the distribution of heights is fairly symmetrical.&lt;/p&gt;
&lt;h3 id=&#34;coefficient-of-kurtosis&#34;&gt;Coefficient of kurtosis&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample coefficient of kurtosis $g_2$&lt;/strong&gt; The &lt;em&gt;sample coefficient of kurtosis&lt;/em&gt; of a variable $X$ is the average of the deviations of values from the sample mean to the fourth power, divided by the standard deviation to the fourth power and minus 3.&lt;/p&gt;
&lt;p&gt;$$g_2 = \frac{\sum (x_i-\bar x)^4 n_i/n}{s^4}-3 = \frac{\sum (x_i-\bar x)^4 f_i}{s^4}-3$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The coefficient of kurtosis measures the concentration of data around the mean and the length of tails of distribution.
The normal (Gaussian bell-shaped) distribution is taken as a reference.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$g_2=0$ indicates that the kurtosis is normal, that is, the concentration of values around the mean is the same than in a Gaussian bell-shaped distribution (&lt;em&gt;mesokurtic&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptive/mesokurtic_distribution.svg&#34; alt=&#34;Mesokurtic distribution&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_2&amp;lt;0$ indicates that the kurtosis is less than normal, that is, the concentration of values around the mean is less than in a Gaussian bell-shaped distribution (&lt;em&gt;platykurtic&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptive/platykurtic_distribution.svg&#34; alt=&#34;Platykurtic distribution&#34; width=&#34;600&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$g_2&amp;gt;0$ indicates that the kurtosis is greater than normal, that is, the concentration of values around the mean is greater than in a Gaussian bell-shaped distribution (&lt;em&gt;leptokurtic&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/descriptive/leptokurtic_distribution.svg&#34; alt=&#34;Leptokurtic distribution&#34; width=&#34;600&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example - Grouped data&lt;/strong&gt;. Using the frequency table of the sample with the heights of students and adding a new column with the deviations from the mean $\bar x = 174.67$ cm to the fourth power, we get&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rrrrr}
\hline
X &amp;amp; x_i &amp;amp; n_i &amp;amp; x_i-\bar x &amp;amp; (x_i-\bar x)^4 n_i\newline
\hline
(150,160] &amp;amp; 155 &amp;amp; 2 &amp;amp; -19.67 &amp;amp; 299396.99\newline
(160,170] &amp;amp; 165 &amp;amp; 8 &amp;amp; -9.67 &amp;amp; 69951.31\newline
(170,180] &amp;amp; 175 &amp;amp; 11 &amp;amp; 0.33 &amp;amp; 0.13\newline
(180,190] &amp;amp; 185 &amp;amp; 7 &amp;amp; 10.33 &amp;amp; 79707.53\newline
(190,200] &amp;amp; 195 &amp;amp; 2 &amp;amp; 20.33 &amp;amp; 341648.49\newline
\hline
\sum &amp;amp;  &amp;amp; 30 &amp;amp; &amp;amp; 790704.45 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$g_2 = \frac{\sum (x_i-\bar x)^4n_i/n}{s^4} - 3 = \frac{790704.45/30}{10.1^4}-3 = -0.47.$$&lt;/p&gt;
&lt;p&gt;As it is a negative value but not too far from 0, that means that the distribution of heights is a little bit platykurtic.&lt;/p&gt;
&lt;p&gt;As we will see in the chapters of inferential statistics, many of the statistical test can only be applied to normal (bell-shaped) populations.&lt;/p&gt;
&lt;p&gt;Normal distributions are symmetrical and mesokurtic, and therefore, their coefficients of symmetry and kurtosis are equal to 0. So, a way of checking if a sample comes from a normal population is looking how far are the coefficients of skewness and kurtosis from 0.&lt;/p&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    In general, the normality of population is rejected when $g_1$ or $g_2$ are outside the interval $[-2,2]$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In that case, is common to apply a transformation to the variable to correct non-normality.&lt;/p&gt;
&lt;h3 id=&#34;non-normal-distributions&#34;&gt;Non-normal distributions&lt;/h3&gt;
&lt;h4 id=&#34;non-normal-right-skewed-distribution&#34;&gt;Non-normal right-skewed distribution&lt;/h4&gt;
&lt;p&gt;An example of left-skewed distribution is the household income.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/right_skewed_distribution_example.svg&#34; alt=&#34;Household income distribution in USA&#34; width=&#34;600&#34;&gt;
&lt;h4 id=&#34;non-normal-left-skewed-distribution&#34;&gt;Non-normal left-skewed distribution&lt;/h4&gt;
&lt;p&gt;An example of left-skewed distribution is the age at death.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/left_skewed_distribution_example.svg&#34; alt=&#34;Age at death distribution of Australian males&#34; width=&#34;600&#34;&gt;
distribution
&lt;h4 id=&#34;non-normal-bimodal-distribution&#34;&gt;Non-normal bimodal distribution&lt;/h4&gt;
&lt;p&gt;An example of left-skewed distribution is the age at death.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/bimodal_distribution_example.svg&#34; alt=&#34;Arrival time of clients of a restaurant&#34; width=&#34;600&#34;&gt;
&lt;h2 id=&#34;variable-transformations&#34;&gt;Variable transformations&lt;/h2&gt;
&lt;p&gt;In many cases, the raw sample data are transformed to correct non-normality of distribution or just to get a more appropriate scale.&lt;/p&gt;
&lt;p&gt;For example, if we are working with heights in metres and a sample contains the following values:&lt;/p&gt;
&lt;p&gt;$$
1.75 \mbox{ m}, 1.65 \mbox{ m}, 1.80 \mbox{ m},
$$&lt;/p&gt;
&lt;p&gt;it is possible to avoid decimals multiplying by 100, that is, changing from metres to centimetres:&lt;/p&gt;
&lt;p&gt;$$
175 \mbox{ cm}, 165 \mbox{ cm}, 180 \mbox{ cm},
$$&lt;/p&gt;
&lt;p&gt;And it is also possible to reduce the magnitude of data subtracting the minimum value in the sample, in this case 165 cm:&lt;/p&gt;
&lt;p&gt;$$
10 \mbox{ cm}, 0 \mbox{ cm}, 15 \mbox{ cm}.
$$&lt;/p&gt;
&lt;p&gt;It is obvious that these data are easier to work with than the original ones. In essences, what it is been done is to apply the following transformation to the data:&lt;/p&gt;
&lt;p&gt;$$Y= 100X-165$$&lt;/p&gt;
&lt;h3 id=&#34;linear-transformations&#34;&gt;Linear transformations&lt;/h3&gt;
&lt;p&gt;One of the most common transformations is the &lt;em&gt;linear transformation&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;$$Y=a+bX.$$&lt;/p&gt;
&lt;p&gt;For a linear transformation, the mean and the standard deviation of the transformed variable are&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\bar y &amp;amp;= a+ b\bar x,\newline
s_{y} &amp;amp;= |b|s_{x}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Additionally, the coefficient of kurtosis does not change and the coefficient of skewness changes only the sign if $b$ is negative.&lt;/p&gt;
&lt;h3 id=&#34;standardization-and-standard-scores&#34;&gt;Standardization and standard scores&lt;/h3&gt;
&lt;p&gt;One of the most common linear transformations is the &lt;em&gt;standardization&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Standardized variable and standard scores&lt;/strong&gt;. The &lt;em&gt;standardized variable&lt;/em&gt; of a variable $X$ is the variable that results from subtracting the mean from $X$ and dividing it by the standard deviation&lt;/p&gt;
&lt;p&gt;$$Z=\frac{X-\bar x}{s_{x}}.$$&lt;/p&gt;
&lt;p&gt;For each value $x_i$ of the sample, the &lt;em&gt;standard score&lt;/em&gt; is the value that results of applying the standardization transformation&lt;/p&gt;
&lt;p&gt;$$z_i=\frac{x_i-\bar x}{s_{x}}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    The standard score is the number of standard deviations a value is above or below the mean, and it is useful to avoid the dependency of the variable from its measurement units. This helps, for instance, to compare values from different variables or samples.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The standardized variable always has mean 0 and standard deviation 1.&lt;/p&gt;
&lt;p&gt;$$\bar z = 0 \qquad s_{z} = 1$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The grades of 5 students in 2 subjects are&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rccccccccc}
\mbox{Student:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; \newline
\hline
X: &amp;amp; 2 &amp;amp; 5 &amp;amp; 4 &amp;amp; \color{red} 8 &amp;amp; 6 &amp;amp; \qquad &amp;amp; \bar x = 5 &amp;amp; \quad s_x = 2\newline
Y: &amp;amp; 1 &amp;amp; 9 &amp;amp; \color{red} 8 &amp;amp; 5 &amp;amp; 2 &amp;amp; \qquad &amp;amp; \bar y = 5 &amp;amp; \quad s_y = 3.16\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Did the fourth student get the same performance in subject $X$ than the third student in subject $Y$?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It might seem that both students had the same performance in every subject because they have the same grade, but in order to get the performance of every student relative to the group of students, the dispersion of grades in every subject must be considered. For that reason it is better to use the standard score as a measure of relative performance.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{cccccc}
\mbox{Student:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\newline
\hline
X: &amp;amp; -1.50 &amp;amp; 0.00 &amp;amp; -0.50 &amp;amp; \color{red}{1.50} &amp;amp; 0.50 \newline
Y: &amp;amp; -1.26 &amp;amp; 1.26 &amp;amp; \color{red}{0.95} &amp;amp; 0.00 &amp;amp; -0.95\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;That is, the student with an 8 in $X$ is $1.5$ times the standard deviation above the mean of $X$, while the student with an 8 in $Y$ is only $0.95$ times the standard deviation above the mean of $Y$. Therefore, the first student had a higher performance in $X$ than the second in $Y$.&lt;/p&gt;
&lt;p&gt;Following with this example and considering both subjects, &lt;em&gt;which is the best student?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we only consider the sum of grades&lt;/p&gt;
&lt;p&gt;$$\begin{array}{rccccc}
\mbox{Student:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\newline
\hline
X: &amp;amp; 2 &amp;amp; 5 &amp;amp; 4 &amp;amp; 8 &amp;amp; 6 \newline
Y: &amp;amp; 1 &amp;amp; 9 &amp;amp; 8 &amp;amp; 5 &amp;amp; 2 \newline
\hline
\sum &amp;amp; 3 &amp;amp; \color{red}{14} &amp;amp; 12 &amp;amp; 13 &amp;amp; 8
\end{array}
$$&lt;/p&gt;
&lt;p&gt;the best student is the second one.&lt;/p&gt;
&lt;p&gt;But if the relative performance is considered, taking the standard scores&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rccccc}
\mbox{Student:} &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\newline
\hline
X: &amp;amp; -1.50 &amp;amp; 0.00 &amp;amp; -0.50 &amp;amp; 1.50 &amp;amp; 0.50 \newline
Y: &amp;amp; -1.26 &amp;amp; 1.26 &amp;amp; 0.95 &amp;amp; 0.00 &amp;amp; -0.95\newline
\hline
\sum &amp;amp; -2.76 &amp;amp; 1.26 &amp;amp; 0.45 &amp;amp; \color{red}{1.5} &amp;amp; -0.45
\end{array}
$$&lt;/p&gt;
&lt;p&gt;the best student is the fourth one.&lt;/p&gt;
&lt;h3 id=&#34;non-linear-transformations&#34;&gt;Non-linear transformations&lt;/h3&gt;
&lt;p&gt;Non-linear transformations are also common to correct non-normality of distributions.&lt;/p&gt;
&lt;p&gt;The square transformation $Y=X^2$ compresses small values and expand large values. So, it is used to correct left-skewed distributions.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/square_transformation.png&#34; alt=&#34;Square transformation&#34; width=&#34;600&#34;&gt;
&lt;p&gt;The square root transformation $Y=\sqrt x$, the logarithmic transformation $Y= \log X$ and the inverse transformation $Y=1/X$ compress large values and expand small values. So, they are used to correct right-skewed distributions.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/log_transformation.png&#34; alt=&#34;Logarithmic transformation&#34; width=&#34;600&#34;&gt;
&lt;h2 id=&#34;factors&#34;&gt;Factors&lt;/h2&gt;
&lt;p&gt;Sometimes it is interesting to describe the frequency distribution of the main variable for different subsamples corresponding to the categories of another variable known as &lt;strong&gt;classificatory variable&lt;/strong&gt; or &lt;strong&gt;factor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Dividing the sample of heights by gender we get two subsamples&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lll}
\hline
\mbox{Females} &amp;amp; &amp;amp; 173, 158, 174, 166, 162, 177, 165, 154, 166, 182, 169, 172, 170, 168. \newline
\mbox{Males} &amp;amp; &amp;amp; 179, 181, 172, 194, 185, 187, 198, 178, 188, 171, 175, 167, 186, 172, 176, 187. \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h3 id=&#34;comparing-distributions-for-the-levels-of-a-factor&#34;&gt;Comparing distributions for the levels of a factor&lt;/h3&gt;
&lt;p&gt;Usually factors allow to compare the distribution of the main variable for every category of the factor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The following charts allow to compare the distribution of heights according to the gender.&lt;/p&gt;
&lt;img src=&#34;../img/descriptive/factor_histogram.svg&#34; alt=&#34;Histogram of heights by genders&#34; width=&#34;500&#34;&gt;
&lt;img src=&#34;../img/descriptive/factor_box_plot.svg&#34; alt=&#34;Box plot of heights by gender&#34; width=&#34;500&#34;&gt;
</description>
    </item>
    
    <item>
      <title>Regression</title>
      <link>/en/teaching/statistics/manual/regression/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/teaching/statistics/manual/regression/</guid>
      <description>&lt;p&gt;In the last chapter we saw how to describe the distribution of a single variable in a sample. However, in most cases, studies require to describe several variables that are often related. For instance, a nutritional study should consider all the variables that could be related to the weight, as height, age, gender, smoking, diet, physic exercise, etc.&lt;/p&gt;
&lt;p&gt;To understand a phenomenon that involve several variables is not enough to study every variable by its own. We have to study all the variables together to describe how they interact and the type of relation among them.&lt;/p&gt;
&lt;p&gt;Usually in a &lt;em&gt;dependency study&lt;/em&gt; there is a &lt;strong&gt;dependent variable&lt;/strong&gt; $Y$ that it is supposed to be influenced by a set of variables $X_1,\ldots,X_n$ known as &lt;strong&gt;independent variables&lt;/strong&gt;. The simpler case is a &lt;em&gt;simple dependency study&lt;/em&gt; when there is only one independent variable, that is the case covered in this chapter.&lt;/p&gt;
&lt;h2 id=&#34;joint-distribution&#34;&gt;Joint distribution&lt;/h2&gt;
&lt;h3 id=&#34;joint-frequencies&#34;&gt;Joint frequencies&lt;/h3&gt;
&lt;p&gt;To study the relation between two variables $X$ and $Y$, we have to study the joint distribution of the &lt;strong&gt;two-dimensional variable&lt;/strong&gt; $(X,Y)$, whose values are pairs $(x_i,y_j)$ where the first element is a value of $X$ and the second a value of $Y$.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Joint sample frequencies&lt;/strong&gt;. Given a sample of $n$ values and a two-dimensional variable $(X,Y)$, for every value of the variable $(x_i,y_j)$ is defined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Absolute frequency&lt;/strong&gt; $n_{ij}$: Is the number of times that the pair $(x_i,y_j)$ appears in the sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relative frequency&lt;/strong&gt; $f_{ij}$: Is the proportion of times that the pair $(x_i,y_j)$ appears in the sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$f_{ij}=\frac{n_{ij}}{n}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    For two-dimensional variables it make no sense cumulative frequencies.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;joint-frequency-distribution&#34;&gt;Joint frequency distribution&lt;/h3&gt;
&lt;p&gt;The values of the two-dimensional variable with their frequencies is known as &lt;strong&gt;joint frequency distribution&lt;/strong&gt;, and is represented in a &lt;strong&gt;joint frequency table&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c|ccccc|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q \newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \newline
x_i &amp;amp; n_{i1} &amp;amp; \cdots &amp;amp; n_{ij} &amp;amp; \cdots &amp;amp; n_{iq} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq} \newline
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example (grouped data)&lt;/strong&gt;. The height (in cm) and weight (in kg) of a sample of 30 students is:&lt;/p&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
(179,85), (173,65), (181,71), (170,65), (158,51), (174,66),&lt;br/&gt;
(172,62), (166,60), (194,90), (185,75), (162,55), (187,78),&lt;br/&gt;
(198,109), (177,61), (178,70), (165,58), (154,50), (183,93),&lt;br/&gt;
(166,51), (171,65), (175,70), (182,60), (167,59), (169,62),&lt;br/&gt;
(172,70), (186,71), (172,54), (176,68),(168,67), (187,80).
&lt;/div&gt;
&lt;p&gt;The joint frequency table is&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c||c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) \ \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \ \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \ \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \ \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \ \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \ \newline
\hline
\end{array}$$&lt;/p&gt;
&lt;h3 id=&#34;scatter-plot&#34;&gt;Scatter plot&lt;/h3&gt;
&lt;p&gt;The joint frequency distribution can be represented graphically with a &lt;strong&gt;scatter plot&lt;/strong&gt;, where data is displayed as a collections of points on a $XY$ coordinate system.&lt;/p&gt;
&lt;p&gt;Usually the independent variable is represented in the $X$ axis and the dependent variable in the $Y$ axis. For every data pair $(x_i,y_j)$ in the sample a dot is drawn on the plane with those coordinates.&lt;/p&gt;
&lt;img src=&#34;../img/regression/scatterplot.svg&#34; alt=&#34;Scatter plot&#34; width=&#34;300&#34;&gt;
&lt;p&gt;The result is a set of points that usually is known as a &lt;em&gt;point cloud&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The scatter plot below represent the distribution of heights and weights of the previous sample.&lt;/p&gt;
&lt;img src=&#34;../img/regression/height_weight_scatterplot.svg&#34; alt=&#34;Scatter plot of heights and weights&#34; width=&#34;600&#34;&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    The shape of the point cloud in a scatter plot gives information about the type of relation between the variables.&lt;/p&gt;
&lt;img src=&#34;../img/regression/scatterplot_different_relations.svg&#34; alt=&#34;Scatter plot of different types of relations&#34; width=&#34;700&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;marginal-frequency-distributions&#34;&gt;Marginal frequency distributions&lt;/h3&gt;
&lt;p&gt;The frequency distributions of each variable of the two-dimensional variable are known as &lt;strong&gt;marginal frequency distributions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can get the marginal frequency distributions from the joint frequency table by adding frequencies by rows and columns.&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c|ccccc|c|}
\hline
X\backslash Y &amp;amp; y_1 &amp;amp; \cdots &amp;amp; y_j &amp;amp; \cdots &amp;amp; y_q &amp;amp; \color{red}{n_x} \newline
\hline
x_1 &amp;amp; n_{11} &amp;amp; \cdots &amp;amp; n_{1j} &amp;amp; \cdots &amp;amp; n_{1q} &amp;amp; \color{red}{n_{x_1}} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow + &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots}  \newline
x_i &amp;amp; n_{i1} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{ij} &amp;amp; \stackrel{+}{\rightarrow} &amp;amp; n_{iq} &amp;amp; \color{red}{n_{x_i}} \newline
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \downarrow +  &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \color{red}{\vdots} \newline
x_p &amp;amp; n_{p1} &amp;amp; \cdots &amp;amp; n_{pj} &amp;amp; \cdots &amp;amp; n_{pq} &amp;amp; \color{red}{n_{x_p}}  \newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{n_{y_1}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_j}} &amp;amp; \color{red}{\cdots} &amp;amp; \color{red}{n_{y_q}} &amp;amp; n \newline
\hline
\end{array}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The marginal frequency distributions for the previous sample of heights and weights are&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; \color{red}{n_x}\ \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{2}\ \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{8}\ \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \color{red}{11} \ \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; \color{red}{7} \ \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; \color{red}{2}\ \newline
\hline
\color{red}{n_y} &amp;amp; \color{red}{7} &amp;amp; \color{red}{11} &amp;amp; \color{red}{7} &amp;amp; \color{red}{2} &amp;amp; \color{red}{2} &amp;amp; \color{red}{1} &amp;amp; 30\ \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;and the corresponding statistics are&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm} \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg}
\end{array}
$$&lt;/p&gt;
&lt;h2 id=&#34;covariance&#34;&gt;Covariance&lt;/h2&gt;
&lt;p&gt;To study the relation between two variables, we have to analyze the joint variation of them.&lt;/p&gt;
&lt;img src=&#34;../img/regression/deviations_to_means.svg&#34; alt=&#34;Devitations to means in an scatterplot&#34; width=&#34;600&#34;&gt;
&lt;p&gt;Dividing the point cloud of the scatter plot in 4 quadrants centered in the mean point $(\bar x, \bar y)$, the sign of deviations from the mean is:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Quadrant&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$(x_i-\bar x)$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$(y_j-\bar y)$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$(x_i-\bar x)(y_j-\bar y)$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$+$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$-$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;img src=&#34;../img/regression/scatterplot_quadrants.svg&#34; alt=&#34;Quadrants of a scatter plot&#34; width=&#34;400&#34;&gt;
&lt;p&gt;If there is an &lt;em&gt;increasing linear&lt;/em&gt; relationship between the variables, most of the points will fall in quadrants 1 and 3, and the sum of the products of deviations from the mean will be positive.&lt;/p&gt;
&lt;p&gt;$$\sum(x_i-\bar x)(y_j-\bar y) &amp;gt; 0$$&lt;/p&gt;
&lt;img src=&#34;../img/regression/increasing_linear_scatterplot.svg&#34; alt=&#34;Increasing linear scatter plot&#34; width=&#34;500&#34;&gt;
&lt;p&gt;If there is an &lt;em&gt;decreasing linear&lt;/em&gt; relationship between the variables, most of the points will fall in quadrants 2 and 4, and the sum of the products of deviations from the mean will be negative.&lt;/p&gt;
&lt;p&gt;$$\sum(x_i-\bar x)(y_j-\bar y) &amp;lt; 0$$&lt;/p&gt;
&lt;img src=&#34;../img/regression/decreasing_linear_scatterplot.svg&#34; alt=&#34;Decreasing linear scatter plot&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Using the products of deviations from the means we get the following statistic.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sample covariance&lt;/strong&gt;. The &lt;em&gt;sample covariance&lt;/em&gt; of a two-dimensional variable $(X,Y)$ is the average of the products of deviations from the respective means.$$s_{xy}=\frac{\sum (x_i-\bar x)(y_j-\bar y)n_{ij}}{n}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It can also be calculated using the formula&lt;/p&gt;
&lt;p&gt;$$s_{xy}=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y.$$&lt;/p&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The covariance measures the linear relation between two variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $s_{xy}&amp;gt;0$ there exists an increasing linear relation.&lt;/li&gt;
&lt;li&gt;If $s_{xy}&amp;lt;0$ there exists a decreasing linear relation.&lt;/li&gt;
&lt;li&gt;If $s_{xy}=0$ there is no linear relation.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Using the joint frequency table of the sample of heights and weights&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
X/Y &amp;amp; [50,60) &amp;amp; [60,70) &amp;amp; [70,80) &amp;amp; [80,90) &amp;amp; [90,100) &amp;amp; [100,110) &amp;amp; n_x\ \newline
\hline\hline
(150,160] &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 2\ \newline
\hline
(160,170] &amp;amp; 4 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 8\ \newline
\hline
(170,180] &amp;amp; 1 &amp;amp; 6 &amp;amp; 3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 11 \ \newline
\hline
(180,190] &amp;amp; 0 &amp;amp; 1 &amp;amp; 4 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 7 \ \newline
\hline
(190,200] &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2\ \newline
\hline
n_y &amp;amp; 7 &amp;amp; 11 &amp;amp; 7 &amp;amp; 2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 30\ \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;$$\bar x = 174.67 \mbox{ cm} \qquad \bar y = 69.67 \mbox{ Kg}$$&lt;/p&gt;
&lt;p&gt;we get that the covariance is equal to&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
s_{xy} &amp;amp;=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y =  \frac{155\cdot 55\cdot 2 + 165\cdot 55\cdot 4 + \cdots + 195\cdot 105\cdot 1}{30}-174.67\cdot 69.67 = \newline
&amp;amp; = \frac{368200}{30}-12169.26 = 104.07 \mbox{ cm$\cdot$ Kg}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;This means that there is a increasing linear relation between the weight and the height.&lt;/p&gt;
&lt;h2 id=&#34;regression&#34;&gt;Regression&lt;/h2&gt;
&lt;p&gt;In most cases the goal of a dependency study is not only to detect a relation between two variables, but also to express that relation with a mathematical function, $$y=f(x)$$ in order to predict the dependent variable for every value of the independent one.
The part of Statistics in charge of constructing such a function is called  &lt;strong&gt;regression&lt;/strong&gt;, and the function is known as &lt;strong&gt;regression function&lt;/strong&gt; or &lt;strong&gt;regression model&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;simple-regression-models&#34;&gt;Simple regression models&lt;/h3&gt;
&lt;p&gt;There are a lot of types of regression models. The most common models are shown in the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Equation&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Linear&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+bx$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Quadratic&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+bx+cx^2$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cubic&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+bx+cx^2+dx^3$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Potential&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a\cdot x^b$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Exponential&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=e^{a+bx}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Logarithmic&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+b\log x$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Inverse&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=a+\frac{b}{x}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sigmoidal&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y=e^{a+\frac{b}{x}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model choice depends on the shape of the points cloud in the scatterplot.&lt;/p&gt;
&lt;h3 id=&#34;residuals-or-predictive-errors&#34;&gt;Residuals or predictive errors&lt;/h3&gt;
&lt;p&gt;Once chosen the type of regression model, we have to determine which function of that family explains better the relation between the dependent and the independent variables, that is, the function that predicts better the dependent variable.&lt;/p&gt;
&lt;p&gt;That function is the function that minimizes the distances from the observed values for $Y$ in the sample to the predicted values of the regression function. These distances are known as &lt;em&gt;residuals&lt;/em&gt; or &lt;em&gt;predictive errors&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Residuals or predictive errors&lt;/strong&gt;. Given a regression model $y=f(x)$ for a two-dimensional variable $(X,Y)$, the &lt;em&gt;residual&lt;/em&gt; or &lt;em&gt;predictive error&lt;/em&gt; for every pair $(x_i,y_j)$ of the sample is the difference between the observed value of the dependent variable $y_j$ and the predicted value of the regression function for $x_i$,$$e_{ij} = y_j-f(x_i).$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/regression/y_residuals.svg&#34; alt=&#34;Regression rediduals on Y&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;least-squares-fitting&#34;&gt;Least squares fitting&lt;/h3&gt;
&lt;p&gt;A way to get the regression function is the &lt;em&gt;least squares method&lt;/em&gt;, that determines the function that minimizes the squared residuals.&lt;/p&gt;
&lt;p&gt;$$\sum e_{ij}^2.$$&lt;/p&gt;
&lt;p&gt;For a linear model $f(x) = a + bx$, the sum depends on two parameters,the intercept $a$, and the slope $b$ of the straight line,&lt;/p&gt;
&lt;p&gt;$$\theta(a,b) = \sum e_{ij}^2 =\sum (y_j - f(x_i))^2 =\sum (y_j-a-bx_i)^2.$$&lt;/p&gt;
&lt;p&gt;This reduces the problem to determine the values of $a$ and $b$ that minimize this sum.&lt;/p&gt;
&lt;p&gt;To solve the minimization problem, we have to set to zero the partial derivatives with respect to $a$ and $b$.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\partial \theta(a,b)}{\partial a} &amp;amp;=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial a} =0 \newline
\frac{\partial \theta(a,b)}{\partial b} &amp;amp;=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial b} =0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;And solving the equation system, we get&lt;/p&gt;
&lt;p&gt;$$a= \bar y - \frac{s_{xy}}{s_x^2}\bar x \qquad b=\frac{s_{xy}}{s_x^2}$$&lt;/p&gt;
&lt;p&gt;This values minimize the residuals on $Y$ and give us the optimal linear model.&lt;/p&gt;
&lt;h2 id=&#34;regression-line&#34;&gt;Regression line&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Regression line&lt;/strong&gt;. Given a sample of a two-dimensional variable $(X,Y)$, the &lt;em&gt;regression line&lt;/em&gt; of $Y$ on $X$ is$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x).$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    The regression line of $Y$ on $X$ is the straight line that minimizes the predictive errors on $Y$, therefore it is the linear regression model that gives better predictions of $Y$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Using the previous sample of heights ($X$) and weights ($Y$) with the following statistics&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 &amp;amp; \quad &amp;amp; s_x = 10.1 \mbox{ cm} \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 &amp;amp; &amp;amp; s_y = 12.82 \mbox{ Kg} \newline
&amp;amp; &amp;amp; s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg} &amp;amp; &amp;amp;
\end{array}
$$&lt;/p&gt;
&lt;p&gt;the regression line of weight on height is&lt;/p&gt;
&lt;p&gt;$$y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x) = 69.67+\frac{104.07}{102.06}(x-174.67) = -108.49 +1.02 x$$&lt;/p&gt;
&lt;p&gt;And the regression line of height on weight is&lt;/p&gt;
&lt;p&gt;$$x = \bar x +\frac{s_{xy}}{s_y^2}(y-\bar y) = 174.67+\frac{104.07}{164.42}(y-69.67) = 130.78 + 0.63 y$$&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Observe that the regression lines are different!
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/regression/regression_lines.svg&#34; alt=&#34;Regression lines of heights and wieghts&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;relative-position-of-the-regression-lines&#34;&gt;Relative position of the regression lines&lt;/h3&gt;
&lt;p&gt;Usually, the regression line of $Y$ on $X$ and the regression line of $X$ on $Y$ are not the same, but they always intersect in the mean point $(\bar x,\bar y)$.&lt;/p&gt;
&lt;p&gt;If there is a perfect linear relation between the variables, then both regression lines are the same, as that line makes both $X$-residuals and $Y$-residuals zero.&lt;/p&gt;
&lt;img src=&#34;../img/regression/perfect_linear_regression.svg&#34; alt=&#34;Perfect linear regression&#34; width=&#34;500&#34;&gt;
&lt;p&gt;If there is no linear relation between the variables, then both regression lines are constant and equals to the respective means,&lt;/p&gt;
&lt;p&gt;$$y = \bar y,\quad x = \bar x.$$&lt;/p&gt;
&lt;p&gt;So, they intersect perpendicularly.&lt;/p&gt;
&lt;img src=&#34;../img/regression/non_linear_regression.svg&#34; alt=&#34;Non linear regression&#34; width=&#34;500&#34;&gt;
&lt;h3 id=&#34;regression-coefficient&#34;&gt;Regression coefficient&lt;/h3&gt;
&lt;p&gt;The most important parameter of a regression line is the slope.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Regression coefficient&lt;/strong&gt; $b_{yx}$. Given a sample of a two-dimensional variable $(X,Y)$, the &lt;em&gt;regression coefficient&lt;/em&gt; of the regression line of $Y$ on $X$ is its slope,$$b_{yx} = \frac{s_{xy}}{s_x^2}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The regression coefficient has always the same sign as the covariance.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    It measures how the dependent variable changes in relation to the independent one according to the regression line. In particular, it gives the number of units that the dependent variable increases or decreases for every unit that the independent variable increases.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the sample of heights and weights, the regression line of weight on height was&lt;/p&gt;
&lt;p&gt;$$y=-108.49 +1.02 x.$$&lt;/p&gt;
&lt;p&gt;Thus, the regression coefficient of weight on height is&lt;/p&gt;
&lt;p&gt;$$b_{yx}= 1.02 \mbox{Kg/cm.}$$&lt;/p&gt;
&lt;p&gt;That means that, according to the regression line of weight on height, the weight will increase $1.02$ Kg for every cm that the height increases.&lt;/p&gt;
&lt;h3 id=&#34;regression-predictions&#34;&gt;Regression predictions&lt;/h3&gt;
&lt;p&gt;Usually the regression models are used to predict the dependent variable for some values of the independent variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the sample of heights and weights, to predict the weight of a person with a height of 180 cm, we have to use the regression line of weight on height,&lt;/p&gt;
&lt;p&gt;$$y = -108.49 + 1.02 \cdot 180  = 75.11 \mbox{ Kg}.$$&lt;/p&gt;
&lt;p&gt;But to predict the height of a person with a weight of 79 Kg, we have to use the regression line of height on weight,&lt;/p&gt;
&lt;p&gt;$$x = 130.78 + 0.63\cdot 79 = 180.55 \mbox{ cm}.$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;However, how reliable are these predictions?&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;correlation&#34;&gt;Correlation&lt;/h2&gt;
&lt;p&gt;Once we have a regression model, in order to see if it is a good predictive model we have to assess the goodness of fit of the model and the strength of the of relation set by it. The part of Statistics in charge of this is &lt;strong&gt;correlation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The correlation study the residuals of a regression model: the smaller the residuals, the greater the goodness of fit, and the stronger the relation set by the model.&lt;/p&gt;
&lt;h3 id=&#34;residual-variance&#34;&gt;Residual variance&lt;/h3&gt;
&lt;p&gt;To measure the goodness of fit of a regression model is common to use the &lt;em&gt;residual variance&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Sample residual variance&lt;/strong&gt; $s_{ry}^2$. Given a regression model $y=f(x)$ of a two-dimensional variable $(X,Y)$, its &lt;em&gt;sample residual variance&lt;/em&gt; is the average of the squared residuals,&lt;/p&gt;
&lt;p&gt;$$s_{ry}^2 = \frac{\sum e_{ij}^2n_{ij}}{n} = \frac{\sum (y_j - f(x_i))^2n_{ij}}{n}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The greater the residuals, the greater the residual variance and the smaller the goodness of fit.&lt;/p&gt;
&lt;p&gt;When the linear relation is perfect, the residuals are zero and the residual variance is zero. Conversely, when there are no relation, the residuals coincide with deviations from the mean, and the residual variance is equal to the variance of the dependent variable.&lt;/p&gt;
&lt;p&gt;$$0\leq s_{ry}^2\leq s_y^2$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;explained-and-non-explained-variation&#34;&gt;Explained and non-explained variation&lt;/h3&gt;
&lt;img src=&#34;../img/regression/variation_decomposition.gif&#34; alt=&#34;Variation decomposition by regression model&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;coefficient-of-determination&#34;&gt;Coefficient of determination&lt;/h3&gt;
&lt;p&gt;From the residual variance is possible to define another correlation statistic easier to interpret.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sample coefficient of determination $r^2$&lt;/strong&gt;. Given a regression model $y=f(x)$ of a two-dimensional variable $(X,Y)$, its &lt;em&gt;coefficient of determination&lt;/em&gt; is$$r^2 = 1- \frac{s_{ry}^2}{s_y^2}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;As the residual variance ranges from 0 to $s_y^2$, we have&lt;/p&gt;
&lt;p&gt;$$0\leq r^2\leq 1$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The greater $r^2$ is, the greater the goodness of fit of the regression model, and the more reliable will its predictions be. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $r^2 =0$ then there is no relation as set by the regression model.&lt;/li&gt;
&lt;li&gt;If $r^2=1$ then the relation set by the model is perfect.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;When the regression model is linear, the coefficient of determination can be computed with this formula&lt;/p&gt;
&lt;p&gt;$$ r^2 =  \frac{s_{xy}^2}{s_x^2s_y^2}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-18&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-18&#34;&gt;
      Proof
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-18&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      &lt;p&gt;When the fitted model is the regression line, the the residual variance is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
s_{ry}^2 &amp;amp; = \sum e_{ij}^2f_{ij} = \sum (y_j - f(x_i))^2f_{ij} = \sum \left(y_j - \bar y -\frac{s_{xy}}{s_x^2}(x_i-\bar x) \right)^2f_{ij}= \newline
&amp;amp; = \sum \left((y_j - \bar y)^2 +\frac{s_{xy}^2}{s_x^4}(x_i-\bar x)^2 - 2\frac{s_{xy}}{s_x^2}(x_i-\bar x)(y_j -\bar y)\right)f_{ij} = \newline
&amp;amp; = \sum (y_j - \bar y)^2f_{ij} +\frac{s_{xy}^2}{s_x^4}\sum (x_i-\bar x)^2f_{ij}- 2\frac{s_{xy}}{s_x^2}\sum (x_i-\bar x)(y_j -\bar y)f_{ij}= \newline
&amp;amp; = s_y^2 + \frac{s_{xy}^2}{s_x^4}s_x^2 - 2 \frac{s_{xy}}{s_x^2}s_{xy} = s_y^2 - \frac{s_{xy}^2}{s_x^2}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;and the coefficient of determination is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
r^2 &amp;amp;= 1- \frac{s_{ry}^2}{s_y^2} = 1- \frac{s_y^2 - \frac{s_{xy}^2}{s_x^2}}{s_y^2} = 1 - 1 + \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{s_{xy}^2}{s_x^2s_y^2}.
\end{aligned}
$$&lt;/p&gt;

    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the sample of heights and weights, we had&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 \newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Thus, the linear coefficient of determination is&lt;/p&gt;
&lt;p&gt;$$r^2 = \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{(104.07 \mbox{ cm\cdot Kg})^2}{102.06 \mbox{ cm}^2 \cdot 164.42 \mbox{ Kg}^2} = 0.65.$$&lt;/p&gt;
&lt;p&gt;This means that the linear model of weight on height explains the 65% of the variation of weight, and the linear model of height on weight also explains 65% of the variation of height.&lt;/p&gt;
&lt;h3 id=&#34;correlation-coefficient&#34;&gt;Correlation coefficient&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sample correlation coefficient $r$&lt;/strong&gt;. Given a sample of a two-dimensional variable $(X,Y)$, the &lt;em&gt;sample correlation coefficient&lt;/em&gt; is the square root of the linear coefficient of determination, with the sign of the covariance,$$r = \dfrac{s_{xy}}{s_xs_y}.$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;As $r^2$ ranges from 0 to 1, $r$ ranges from -1 to 1,&lt;/p&gt;
&lt;p&gt;$$-1\leq r\leq 1.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The correlation coefficient measures not only the strength of the linear association but also its direction (increasing or decreasing):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $r=0$ then there is no linear relation.&lt;/li&gt;
&lt;li&gt;Si $r=1$ then there is a perfect increasing linear relation.&lt;/li&gt;
&lt;li&gt;Si $r=-1$ then there is a perfect decreasing linear relation.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the sample of heights and weights, we had&lt;/p&gt;
&lt;p&gt;$$\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} &amp;amp; \quad &amp;amp; s^2_x = 102.06 \mbox{ cm}^2 \newline
\bar y = 69.67 \mbox{ Kg} &amp;amp; &amp;amp; s^2_y = 164.42 \mbox{ Kg}^2 \newline
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Thus, the correlation coefficient is&lt;/p&gt;
&lt;p&gt;$$r = \frac{s_{xy}}{s_xs_y} = \frac{104.07 \mbox{ cm\cdot Kg}}{10.1 \mbox{ cm} \cdot 12.82 \mbox{ Kg}} = +0.8.$$&lt;/p&gt;
&lt;p&gt;This means that there is a rather strong linear, increasing, relation between height and weight.&lt;/p&gt;
&lt;h3 id=&#34;different-linear-correlations&#34;&gt;Different linear correlations&lt;/h3&gt;
&lt;p&gt;The scatter plots below show linear regression models with differents correlations.&lt;/p&gt;
&lt;img src=&#34;../img/regression/different_correlations.svg&#34; alt=&#34;Linear regression models with different correlations&#34; width=&#34;700&#34;&gt;
&lt;h3 id=&#34;reliability-of-regression-predictions&#34;&gt;Reliability of regression predictions&lt;/h3&gt;
&lt;p&gt;The coefficient of determination explains the goodness of fit of a regression model, but there are other factors that influence the reliability of regression predictions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The coefficient of determination: The greater $r^2$, the greater the goodness of fit and the more reliable the predictions are.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The variability of the population distribution: The greater the variation, the more difficult to predict and the less reliable the predictions are.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The sample size: The greater the sample size, the more information we have and the more reliable the predictions are.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    In addition, we have to take into account that a regression model is only valid for the range of values observed in the sample. That means that, as we don’t have any information outside that range, we must not do predictions for values far from that range.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;non-linear-regression&#34;&gt;Non-linear regression&lt;/h2&gt;
&lt;p&gt;The fit of a non-linear regression can be also done by the least square fitting method.&lt;/p&gt;
&lt;p&gt;However, in some cases the fitting of a non-linear model can be reduced to the fitting of a linear model applying a simple transformation to the variables of the model.&lt;/p&gt;
&lt;h3 id=&#34;transformations-of-non-linear-regression-models&#34;&gt;Transformations of non-linear regression models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logarithmic&lt;/strong&gt;: A logarithmic model $y = a+b \log x$ can be transformed in a linear model with the change $t=\log x$:&lt;/p&gt;
&lt;p&gt;$$y=a+b\log x = a+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exponential&lt;/strong&gt;: An exponential model $y = e^{a+bx}$ can be transformed in a linear model with the change $z = \log y$:&lt;/p&gt;
&lt;p&gt;$$z = \log y = \log(e^{a+bx}) =  a+bx.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Potential&lt;/strong&gt;: A potential model $y = ax^b$ can be transformed in a linear model with the changes $t=\log x$ and $z=\log y$:&lt;/p&gt;
&lt;p&gt;$$z = \log y = \log(ax^b) = \log a + b \log x = a^\prime+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inverse&lt;/strong&gt;: An inverse model $y = a+b/x$ can be transformed in a linear model with the change $t=1/x$:&lt;/p&gt;
&lt;p&gt;$$y = a + b(1/x) = a+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sigmoidal&lt;/strong&gt;: A sigmoidal model $y = e^{a+b/x}$ can be transformed in a linear model with the changes $t=1/x$ and $z=\log y$:&lt;/p&gt;
&lt;p&gt;$$z = \log y = \log (e^{a+b/x}) = a+b(1/x) = a+bt.$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-relation&#34;&gt;Exponential relation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The number of bacteria in a culture evolves with time according to the table below.&lt;/p&gt;
&lt;p&gt;$$\begin{array}{c|c}
\mbox{Hours} &amp;amp; \mbox{Bacteria} \newline
\hline
0 &amp;amp;  25  \newline
1 &amp;amp; 28  \newline
2 &amp;amp;  47 \newline
3 &amp;amp; 65  \newline
4 &amp;amp; 86 \newline
5 &amp;amp; 121 \newline
6 &amp;amp; 190 \newline
7 &amp;amp; 290 \newline
8 &amp;amp; 362
\end{array}
$$&lt;/p&gt;
&lt;p&gt;The scatter plot of the sample is showed below.&lt;/p&gt;
&lt;img src=&#34;../img/regression/bacteria_evolution.svg&#34; alt=&#34;Scatter plot of the evolution of a bacteria culture&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Fitting a linear model we get&lt;/p&gt;
&lt;p&gt;$$\mbox{Bacteria} = -30.18+41,27,\mbox{Hours, with } r^2=0.85.$$&lt;/p&gt;
&lt;img src=&#34;../img/regression/linear_regression_bacteria.svg&#34; alt=&#34;Linear regression of the bacteria evolution&#34; width=&#34;500&#34;&gt;
&lt;p&gt;&lt;em&gt;Is a good model?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Although the linear model is not bad, according to the shape of the point cloud of the scatter plot, an exponential model looks more suitable.&lt;/p&gt;
&lt;p&gt;To construct an exponential model $y = e^{a+bx}$ we can apply the transformation $z=\log y$, that is, applying a logarithmic transformation to the dependent variable.&lt;/p&gt;
&lt;p&gt;$$\begin{array}{c|c|c}
\mbox{Hours} &amp;amp; \mbox{Bacteria} &amp;amp; \mbox{$\log$(Bacteria)} \newline
\hline
0 &amp;amp;  25 &amp;amp; 3.22 \newline
1 &amp;amp; 28 &amp;amp; 3.33 \newline
2 &amp;amp;  47 &amp;amp; 3.85 \newline
3 &amp;amp; 65  &amp;amp; 4.17 \newline
4 &amp;amp; 86 &amp;amp; 4.45 \newline
5 &amp;amp; 121 &amp;amp; 4.80 \newline
6 &amp;amp; 190 &amp;amp; 5.25 \newline
7 &amp;amp; 290 &amp;amp; 5.67 \newline
8 &amp;amp; 362 &amp;amp; 5.89
\end{array}
$$&lt;/p&gt;
&lt;img src=&#34;../img/regression/log_bacteria_evolution.svg&#34; alt=&#34;Scatter plot of the evolution of the logarithm of bacteria culture&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Now it only remains to compute the regression line of the logarithm of bacteria on hours,&lt;/p&gt;
&lt;p&gt;$$\mbox{$\log$(Bacteria)} = 3.107 + 0.352, \mbox{Horas},$$&lt;/p&gt;
&lt;p&gt;and, undoing the change of variable,&lt;/p&gt;
&lt;p&gt;$$\mbox{Bacteria} = e^{3.107+0.352,\mbox{Hours}}, \mbox{ with } r^2=0.99.$$&lt;/p&gt;
&lt;img src=&#34;../img/regression/exponential_regression_bacteria.svg&#34; alt=&#34;Exponential regression of the bacteria evolution&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Thus, the exponential model fits much better than the linear model.&lt;/p&gt;
&lt;h2 id=&#34;regression-risks&#34;&gt;Regression risks&lt;/h2&gt;
&lt;h3 id=&#34;lack-of-fit-does-not-mean-independence&#34;&gt;Lack of fit does not mean independence&lt;/h3&gt;
&lt;p&gt;It is important to note that every regression model has its own coefficient of determination.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Thus, a coefficient of determination near zero means that there is no relation as set by the model, but &lt;em&gt;that does not mean that the variables are independent&lt;/em&gt;, because there could be a different type of relation.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;img src=&#34;../img/regression/linear_regression_parabolic_relation.svg&#34; alt=&#34;Linear regression on a cuadratic relation&#34; width=&#34;500&#34;&gt;
&lt;img src=&#34;../img/regression/parabolic_regression.svg&#34; alt=&#34;Cuadratic regression on a cuadratic relation&#34; width=&#34;500&#34;&gt;
&lt;/div&gt;
&lt;h3 id=&#34;outliers-influence-in-regression&#34;&gt;Outliers influence in regression&lt;/h3&gt;
&lt;p&gt;Outliers in regression studies are points that clearly do not follow the tendency of the rest of points, even if the values of the pair are not outliers for every variable separately.&lt;/p&gt;
&lt;img src=&#34;../img/regression/scatterplot_with_outliers.svg&#34; alt=&#34;Scatter plot with an outlier&#34; width=&#34;500&#34;&gt;
&lt;p&gt;Outliers in regression studies can provoke drastic changes in the regression models.&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;img src=&#34;../img/regression/linear_regression_with_outliers.svg&#34; alt=&#34;Linear regression with an outlier&#34; width=&#34;500&#34;&gt; &lt;img src=&#34;../img/regression/linear_regression_without_outliers.svg&#34; alt=&#34;Linear regression without outliers&#34; width=&#34;500&#34;&gt;
&lt;/div&gt;
&lt;h3 id=&#34;the-simpsons-paradox&#34;&gt;The Simpson&amp;rsquo;s paradox&lt;/h3&gt;
&lt;p&gt;Sometimes a trend can disappears or even reverses when we split the sample into groups according to a qualitative variable that is related to the dependent variable.
This is known as the &lt;em&gt;Simpson&amp;rsquo;s paradox&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The scatterplot below shows an inverse relation between the study hours and the score in an exam.&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;img src=&#34;../img/regression/simpson_paradox_1.svg&#34; alt=&#34;Simpon&#39;s paradox. Inverse relation between study hours and the score in an exam.&#34; width=&#34;500&#34;&gt;
&lt;/div&gt;
&lt;p&gt;But if we split the sample in two groups (good and bad students) we get different trends and now the relation is direct, which makes more sense.&lt;/p&gt;
&lt;div class=&#34;center&#34;&gt;
&lt;img src=&#34;../img/regression/simpson_paradox_2.svg&#34; alt=&#34;Simpon&#39;s paradox. Direct relation between study hours and the score in an exam.&#34;&#34; width=&#34;500&#34;&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability</title>
      <link>/en/teaching/statistics/manual/probability/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/teaching/statistics/manual/probability/</guid>
      <description>&lt;p&gt;Descriptive Statistics provides methods to describe the variables measured in the sample and their relations, but it does not allow to draw any conclusion about the population.&lt;/p&gt;
&lt;p&gt;Now it is time to take the leap from the sample to the population and the bridge for that is &lt;strong&gt;Probability Theory&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Remember that the sample has a limited information about the population, and in order to draw valid conclusions for the population the sample must be representative of it. For that reason, to guarantee the representativeness of the sample, this must be drawn &lt;em&gt;randomly&lt;/em&gt;. This means that the choice of individuals in the sample is by &lt;em&gt;chance&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Probability Theory will provide us the tools to control the random in the sampling and to determine the level of reliability of the conclusions drawn from the sample.&lt;/p&gt;
&lt;h2 id=&#34;random-experiments-and-events&#34;&gt;Random experiments and events&lt;/h2&gt;
&lt;h3 id=&#34;random-experiments&#34;&gt;Random experiments&lt;/h3&gt;
&lt;p&gt;The study of a characteristic of the population is conducted through random experiments.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Random experiment&lt;/strong&gt;. A &lt;em&gt;random experiment&lt;/em&gt; is an experiment that meets two conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The set of possible outcomes is known.&lt;/li&gt;
&lt;li&gt;It is impossible to predict the outcome with absolute certainty.&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Gambling are typical examples of random experiments. The roll of a dice, for example, is a random experiment because&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It is known the set of possible outcomes: $\{1,2,3,4,5,6\}$.&lt;/li&gt;
&lt;li&gt;Before rolling the dice, it is impossible to predict with absolute certainty the outcome.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another non-gambling example is the random choice of an individual of a human population and the determination of its blood type.&lt;/p&gt;
&lt;p&gt;Generally, the draw of a sample by a random method is an random experiment.&lt;/p&gt;
&lt;h3 id=&#34;sample-space&#34;&gt;Sample space&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sample space&lt;/strong&gt;. The set $\Omega$ of the possible outcomes of a random experiment is known as the &lt;em&gt;sample space&lt;/em&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Some examples of sample spaces are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the toss of a coin $\Omega=\{\mbox{heads},\mbox{tails}\}$.&lt;/li&gt;
&lt;li&gt;For the roll of a dice $\Omega=\{1,2,3,4,5,6\}$.&lt;/li&gt;
&lt;li&gt;For the blood type of an individual drawn by chance $\Omega=\{\mbox{A},\mbox{B},\mbox{AB},\mbox{0}\}$.&lt;/li&gt;
&lt;li&gt;For the height of an individual drawn by chance $\Omega=\mathbb{R}^+$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tree-diagrams&#34;&gt;Tree diagrams&lt;/h3&gt;
&lt;p&gt;In experiments where more than one variable is measured, the determination of the sample space can be difficult. In such a cases, it is advisable to use a &lt;strong&gt;tree diagram&lt;/strong&gt; to construct the sample space.&lt;/p&gt;
&lt;p&gt;In a tree diagram every variable is represented in a level of the tree and every possible outcome of the variable as a branch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The tree diagram below represents the sample space of a random experiment where the gender and the blood type is measured in a random individual.&lt;/p&gt;
&lt;img src=&#34;../img/probability/sample_space.svg&#34; alt=&#34;Tree diagram of gender and blood type&#34; width=&#34;500&#34;&gt;
&lt;h3 id=&#34;random-events&#34;&gt;Random events&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Random event&lt;/strong&gt;. A &lt;em&gt;random event&lt;/em&gt; is any subset of the sample space $\Omega$ of a random experiment.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There are different types of events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Impossible event:&lt;/strong&gt; Is the event with no elements $\emptyset$. It has no chance of occurring.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elemental events:&lt;/strong&gt; Are events with only one element, that is, a singleton.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composed events:&lt;/strong&gt; Are events with two or more elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sure event:&lt;/strong&gt; Is the event that contains the whole sample space $\Omega$. It always happens.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;set-theory&#34;&gt;Set theory&lt;/h2&gt;
&lt;h3 id=&#34;event-space&#34;&gt;Event space&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Event space&lt;/strong&gt;. Given a sample space $\Omega$ of a random experiment, the &lt;em&gt;event space&lt;/em&gt; of $\Omega$ is the set of all possible events of $\Omega$, and is noted $\mathcal{P}(\Omega).$
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Given the sample space $\Omega=\{a,b,c\}$, its even space is&lt;/p&gt;
&lt;p&gt;$$\mathcal{P}(\Omega)=\{\emptyset, {a},{b},{c},{a,b},{a,c},{b,c},{a,b,c}\}$$&lt;/p&gt;
&lt;p&gt;As events are subsets of the sample space, using the set theory we have the following operations on events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Union&lt;/li&gt;
&lt;li&gt;Intersection&lt;/li&gt;
&lt;li&gt;Complement&lt;/li&gt;
&lt;li&gt;Difference&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;union-of-events&#34;&gt;Union of events&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Union event&lt;/strong&gt;. Given two events $A,B\subseteq \Omega$, the &lt;em&gt;union&lt;/em&gt; of $A$ and $B$, denoted by $A\cup B$, is the event of all elements that are members of $A$ or $B$ or both.&lt;/p&gt;
&lt;p&gt;$$A\cup B = \{x\,|\, x\in A\textrm{ or }x\in B\}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probability/union.svg&#34; alt=&#34;Union of two events&#34; width=&#34;300&#34;&gt;
&lt;p&gt;The union event $A\cup B$ happens when $A$ &lt;span style=&#34;color:red;&#34;&gt;or&lt;/span&gt; $B$ happen.&lt;/p&gt;
&lt;h3 id=&#34;intersection-of-events&#34;&gt;Intersection of events&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Intersection event&lt;/strong&gt;. Given two events $A,B\subseteq \Omega$, the &lt;em&gt;intersection&lt;/em&gt; of $A$ and $B$, denoted by $A\cap B$, is the event of all elements that are members of both $A$ and $B$.&lt;/p&gt;
&lt;p&gt;$$A\cap B = \{x\,|\, x\in A\textrm{ and }x\in B\}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probability/intersection.svg&#34; alt=&#34;Intersection of two events&#34; width=&#34;300&#34;&gt;
&lt;p&gt;The intersection event $A\cap B$ happens when $A$ &lt;span style=&#34;color:red;&#34;&gt;and&lt;/span&gt; $B$ happen.&lt;/p&gt;
&lt;p&gt;Two events are &lt;strong&gt;incompatible&lt;/strong&gt; if their intersection is empty.&lt;/p&gt;
&lt;h3 id=&#34;complement-of-an-event&#34;&gt;Complement of an event&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Complementary event&lt;/strong&gt;. Given an event $A\subseteq \Omega$, the &lt;em&gt;complementary or contrary event&lt;/em&gt; of $A$, denoted by $\bar A$, is the event of all elements of $\Omega$ except the elements that are members of $A$.&lt;/p&gt;
&lt;p&gt;$$\bar A = \{x\,|\, x\not\in A\}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probability/complement.svg&#34; alt=&#34;Complement of an event&#34; width=&#34;300&#34;&gt;
&lt;p&gt;The complementary event $\bar A$ happens when $A$ does &lt;span style=&#34;color:red;&#34;&gt;not&lt;/span&gt; happen.&lt;/p&gt;
&lt;h3 id=&#34;difference-of-events&#34;&gt;Difference of events&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Difference event&lt;/strong&gt;. Given two events $A,B\subseteq \Omega$, the &lt;em&gt;difference&lt;/em&gt; of $A$ and $B$, denoted by $A-B$, is the event of all elements that are members of $A$ but not are members of $B$.&lt;/p&gt;
&lt;p&gt;$$A-B = \{x\,|\, x\in A\textrm{ and }x\not\in B\} = A \cap \bar B.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probability/difference.svg&#34; alt=&#34;Diference of two events&#34; width=&#34;300&#34;&gt;
&lt;p&gt;The difference event $A-B$ happens when $A$ happens but $B$ does not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Given the sample space of rolling a dice $\Omega=\{1,2,3,4,5,6\}$ and the events $A=\{2,4,6\}$ and $B=\{1,2,3,4\}$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The union of $A$ and $B$ is $A\cup B=\{1,2,3,4,6\}$.&lt;/li&gt;
&lt;li&gt;The intersection of $A$ and $B$ is $A\cap B=\{2,4\}$.&lt;/li&gt;
&lt;li&gt;The complement of $A$ is $\bar A=\{1,3,5\}$.&lt;/li&gt;
&lt;li&gt;The events $A$ and $\bar A$ are incompatible.&lt;/li&gt;
&lt;li&gt;The difference of $A$ and $B$ is $A-B=\{6\}$, and the difference of $B$ and $A$ is $B-A=\{1,3\}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algebra-of-events&#34;&gt;Algebra of events&lt;/h3&gt;
&lt;p&gt;Given the events $A,B,C\subseteq \Omega$, the following properties are meet.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A\cup A=A$, $\quad A\cap A=A$ (idempotency).&lt;/li&gt;
&lt;li&gt;$A\cup B=B\cup A$, $\quad A\cap B = B\cap A$ (commutative).&lt;/li&gt;
&lt;li&gt;$(A\cup B)\cup C = A\cup (B\cup C)$, $\quad (A\cap B)\cap C = A\cap (B\cap C)$ (associative).&lt;/li&gt;
&lt;li&gt;$(A\cup B)\cap C = (A\cap C)\cup (B\cap C)$, $\quad (A\cap B)\cup C = (A\cup C)\cap (B\cup C)$ (distributive).&lt;/li&gt;
&lt;li&gt;$A\cup \emptyset=A$, $\quad A\cap \Omega=A$ (neutral element).&lt;/li&gt;
&lt;li&gt;$A\cup \Omega=\Omega$, $\quad A\cap \emptyset=\emptyset$ (absorbing element).&lt;/li&gt;
&lt;li&gt;$A\cup \overline A = \Omega$, $\quad A\cap \overline A= \emptyset$ (complementary symmetric element).&lt;/li&gt;
&lt;li&gt;$\overline{\overline A} = A$ (double contrary).&lt;/li&gt;
&lt;li&gt;$\overline{A\cup B} = \overline A\cap \overline B$, $\quad \overline{A\cap B} = \overline A\cup \overline B$ (Morgan’s laws).&lt;/li&gt;
&lt;li&gt;$A\cap B\subseteq A\cup B$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;probability-definition&#34;&gt;Probability definition&lt;/h2&gt;
&lt;h3 id=&#34;classical-definition-of-probability&#34;&gt;Classical definition of probability&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Probability (Laplace)&lt;/strong&gt;. Given a sample space $\Omega$ of a random experiment where all elements of $\Omega$ are equally likely, the &lt;em&gt;probability&lt;/em&gt; of an event $A\subseteq \Omega$ is the quotient between the number of elements of $A$ and the number of elements of $\Omega$&lt;/p&gt;
&lt;p&gt;$$P(A) = \frac{|A|}{|\Omega|} = \frac{\mbox{number of favorable outcomes}}{\mbox{number of possible outcomes}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This definition is well known, but it has important restrictions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is required that all the elements of the sample space are equally likely (&lt;em&gt;equiprobability&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;It can not be used with infinite sample spaces.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Given the sample space of rolling a dice $\Omega=\{1,2,3,4,5,6\}$ and the event $A=\{2,4,6\}$, the probability of $A$ is&lt;/p&gt;
&lt;p&gt;$$P(A) = \frac{|A|}{|\Omega|} = \frac{3}{6} = 0.5.$$&lt;/p&gt;
&lt;p&gt;However, given the sample space of the blood type of a random individual $\Omega=\{O,A,B,AB\}$, it is not possible to use the classical definition to compute the probability of having group $A$,&lt;/p&gt;
&lt;p&gt;$$P(A) \neq \frac{|A|}{|\Omega|} = \frac{1}{4} = 0.25,$$&lt;/p&gt;
&lt;p&gt;because the blood types are not equally likely in human populations.&lt;/p&gt;
&lt;h3 id=&#34;frequency-definition-of-probability&#34;&gt;Frequency definition of probability&lt;/h3&gt;
&lt;div class=&#34;alert alert-theo&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Theorem - Law of large numbers&lt;/strong&gt;. When a random experiment is repeated a large number of times, the relative frequency of an event tends to the probability of the event.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The following definition of probability uses this theorem.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Frequency probability&lt;/strong&gt;. Given a sample space $\Omega$ of a replicable random experiment, the &lt;em&gt;probability&lt;/em&gt; of an event $A\subseteq \Omega$ is the relative frequency of the event $A$ in an infinite number of repetitions of the experiment&lt;/p&gt;
&lt;p&gt;$$P(A) = lim_{n\rightarrow \infty}\frac{n_A}{n}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Although frequency probability avoid the restrictions of classical definition, it also have some drawbacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It computes an estimation of the real probability (more accurate the higher the sample size).&lt;/li&gt;
&lt;li&gt;The repetition of the experiment must be in identical conditions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Given the sample space of tossing a coin $\Omega=\{H,T\}$, if after tossing the coin 100 times we got 54 heads, then the probability of $H$ is&lt;/p&gt;
&lt;p&gt;$$P(H) = \frac{n_H}{n} = \frac{54}{100} = 0.54.$$&lt;/p&gt;
&lt;p&gt;Given the sample space of the blood type of a random individual $\Omega=\{O,A,B,AB\}$, if after drawing a random sample of 1000 persons we got 412 with blood type $A$, then the probability of $A$ is&lt;/p&gt;
&lt;p&gt;$$P(A) = \frac{n_A}{n} = \frac{412}{1000} = 0.412.$$&lt;/p&gt;
&lt;h3 id=&#34;axiomatic-definition-of-probability&#34;&gt;Axiomatic definition of probability&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Probability (Kolmogórov)&lt;/strong&gt;. Given a sample space $\Omega$ of a random experiment, a &lt;em&gt;probability&lt;/em&gt; function is a function that maps every event $A\subseteq \Omega$ a real number $P(A)$, known as the probability of $A$, that meets the following axioms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The probability of any event is nonnegative,&lt;/p&gt;
&lt;p&gt;$$P(A)\geq 0.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability of the sure event is 1,&lt;/p&gt;
&lt;p&gt;$$P(\Omega)=1$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability of the union of two incompatible events ($A\cap B=\emptyset$) is the sum of their probabilities&lt;/p&gt;
&lt;p&gt;$$P(A\cup B) = P(A)+P(B).$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;From the previous axioms is possible to deduce some important properties of a probability function.&lt;/p&gt;
&lt;p&gt;Given a sample space $\Omega$ of a random experiment and the events $A,B\subseteq \Omega$, the following properties are meet:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$P(\bar A) = 1-P(A)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(\emptyset)= 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $A\subseteq B$ then $P(A)\leq P(B)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(A) \leq 1$. This means that $P(A)\in [0,1]$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(A-B)=P(A)-P(A\cap B)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(A\cup B)= P(A) + P(B) - P(A\cap B)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $A=\{e_1,\ldots,e_n\}$, where $e_i$ $i=1,\ldots,n$ are elemental events, then&lt;/p&gt;
&lt;p&gt;$$P(A)=\sum_{i=1}^n P(e_i).$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-12&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-12&#34;&gt;
      Proof
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-12&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      &lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\bar A = \Omega \Rightarrow P(A\cup \bar A) = P(\Omega) \Rightarrow P(A)+P(\bar A) = 1 \Rightarrow P(\bar A)=1-P(A)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\emptyset = \bar \Omega \Rightarrow P(\emptyset) = P(\bar \Omega) = 1-P(\Omega) = 1-1 = 0.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$B = A\cup (B-A)$. As $A$ and $B-A$ are incompatible, $P(B) = P(A\cup (B-A)) = P(A)+P(B-A) \geq P(A).$&lt;/p&gt;
&lt;p&gt;If we think of probabilities as areas, it is easy to see graphically,&lt;/p&gt;
 &lt;img src=&#34;../img/probability/inclusion_probability.svg&#34; alt=&#34;Probability of a event included in other&#34; width=&#34;300&#34;&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A\subseteq \Omega \Rightarrow P(A)\leq P(\Omega)=1.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A=(A-B)\cup (A\cap B)$. As $A-B$ and $A\cap B$ are incompatible, $P(A)=P(A-B)+P(A\cap B) \Rightarrow P(A-B)=P(A)-P(A\cap B)$.&lt;/p&gt;
&lt;p&gt;If we think of probabilities as areas, it is easy to see graphically,&lt;/p&gt;
 &lt;img src=&#34;../img/probability/difference_probability.svg&#34; alt=&#34;Probability of the difference event&#34; width=&#34;300&#34;&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A\cup B= (A-B) \cup (B-A) \cup (A\cap B)$. As $A-B$, $B-A$ and $A\cap B$ are incompatible, $P(A\cup B)=P(A-B)+P(B-A)+P(A\cap B) =P(A)-P(A\cap B)+P(B)-P(A\cap B)+P(A\cap B)$ $=P(A)+P(B)-P(A\cup B)$.&lt;/p&gt;
&lt;p&gt;If we think again of probabilities as areas, it is easy to see graphically because the area of $A\cap B$ is added twice (one for $A$ and other for $), so it must be subtracted once.&lt;/p&gt;
 &lt;img src=&#34;../img/probability/union_probability.svg&#34; alt=&#34;Probability of the union event&#34; width=&#34;300&#34;&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A=\{e_1,\cdots,e_n\} = \{e_1\}\cup \cdots \cup \{e_n\} \Rightarrow P(A)=P(\{e_1\}\cup \cdots \cup \{e_n\}) =
P(\{e_1\})+ \cdots P(\{e_n\}).$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;probability-interpretation&#34;&gt;Probability interpretation&lt;/h3&gt;
&lt;p&gt;As set by the previous axioms, the probability of an event $A$, is a real number $P(A)$ that always ranges from 0 to 1.&lt;/p&gt;
&lt;p&gt;In a certain way, this number expresses the plausibility of the event, that is, the chances that the event $A$ occurs in the experiment. Therefore, it also gives a measure of the uncertainty about the event.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The maximum uncertainty correspond to probability $P(A)=0.5$ ($A$ and $\bar A$ have the same chances of happening).&lt;/li&gt;
&lt;li&gt;The minimum uncertainty correspond to probability $P(A)=1$ ($A$ will happen with absolute certainty) and $P(A)=0$ ($A$ won’t happen with absolute certainty)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When $P(A)$ is closer to 0 than to 1, the chances of not happening $A$ are greater than the chances of happening $A$. On the contrary, when $P(A)$ is closer to 1 than to 0, the chances of happening $A$ are greater than the chances of not happening $A$.&lt;/p&gt;
&lt;h2 id=&#34;conditional-probability&#34;&gt;Conditional probability&lt;/h2&gt;
&lt;h3 id=&#34;conditional-experiments&#34;&gt;Conditional experiments&lt;/h3&gt;
&lt;p&gt;Occasionally, we can get some information about the experiment before its realization. Usually that information is given as an event $B$ of the same sample space that we know that is true before we conduct the experiment.&lt;/p&gt;
&lt;p&gt;In such a case, we will say that $B$ is a &lt;em&gt;conditioning&lt;/em&gt; event and the probability of another event $A$ is known as a &lt;em&gt;conditional probability&lt;/em&gt; and expressed $P(A\vert B)$. This must be read as &lt;em&gt;probability of $A$ given $B$&lt;/em&gt; or &lt;em&gt;probability of $A$ under the condition $B$&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Usually, conditioning events change the sample space and therefore the probabilities of events.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Assume that we have a sample of 100 women and 100 men with the following frequencies&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c|c|c|}
\hline
&amp;amp; \mbox{Non-smokers} &amp;amp; \mbox{Smokers} \newline
\hline
\mbox{Females} &amp;amp; 80 &amp;amp; 20 \newline
\hline
\mbox{Males} &amp;amp; 60 &amp;amp; 40 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Then, using the frequency definition of probability, the&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Smoker})= \frac{60}{200}=0.3.$$&lt;/p&gt;
&lt;p&gt;However, if we know that the person is a woman, then the sample is reduced to the first row, and the probability of being smoker is&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Smoker}\mid\mbox{Female})=\frac{20}{100}=0.2.$$&lt;/p&gt;
&lt;h3 id=&#34;conditional-probability-1&#34;&gt;Conditional probability&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Conditional probability&lt;/strong&gt; Given a sample space $\Omega$ of a random experiment, and two events $A,B\subseteq \Omega$, the probability of $A$ conditional on $B$ occurring is&lt;/p&gt;
&lt;p&gt;$$P(A|B) = \frac{P(A\cap B)}{P(B)},$$as long as, $P(B)\neq 0$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This definition allows to calculate conditional probabilities without changing the original sample space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the previous example&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Smoker}\mid\mbox{Female})= \frac{P(\mbox{Smoker}\cap \mbox{Female})}{P(\mbox{Female})} = \frac{20/200}{100/200}=\frac{80}{100}=0.8.$$&lt;/p&gt;
&lt;h3 id=&#34;probability-of-the-intersection-event&#34;&gt;Probability of the intersection event&lt;/h3&gt;
&lt;p&gt;From the definition of conditional probability it is possible to derive the formula for the probability of the intersection of two events.&lt;/p&gt;
&lt;p&gt;$$P(A\cap B) = P(A)P(B|A) = P(B)P(A|B).$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In a population there are a 30% of smokers and we know that there are a 40% of smokers with breast cancer. The probability of a random person being smoker and having breast cancer is&lt;/p&gt;
&lt;p&gt;$$P(\mbox{Smoker}\cap \mbox{Cancer})= P(\mbox{Smoker})P(\mbox{Cancer}\mid\mbox{Smoker}) = 0.3\times 0.4 = 0.12.$$&lt;/p&gt;
&lt;h3 id=&#34;independence-of-events&#34;&gt;Independence of events&lt;/h3&gt;
&lt;p&gt;Sometimes, the probability of the conditioning event does not change the original probability of the main event.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Independent events&lt;/strong&gt;. Given a sample space $\Omega$ of a random experiment, two events $A,B\subseteq \Omega$ are &lt;em&gt;independents&lt;/em&gt; if the probability of $A$ does not change when conditioning on $B$, and vice-versa, that is,&lt;/p&gt;
&lt;p&gt;$$P(A|B) = P(A) \quad \mbox{and} \quad P(B|A)=P(B),$$&lt;/p&gt;
&lt;p&gt;if $P(A)\neq 0$ and $P(B)\neq 0$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This means that the occurrence of one event does not give relevant information to change the uncertainty of the other.&lt;/p&gt;
&lt;p&gt;When two events are independent, the probability of the intersection of them is equal to the product of their probabilities,&lt;/p&gt;
&lt;p&gt;$$P(A\cap B) = P(A)P(B).$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The sample space of tossing twice a coin is $\Omega=\{(H,H),(H,T),(T,H),(T,T)\}$ and all the elements are equiprobable if the coin is fair. Thus, applying the classical definition of probability we have&lt;/p&gt;
&lt;p&gt;$$P((H,H)) = \frac{1}{4} = 0.25.$$&lt;/p&gt;
&lt;p&gt;If we name $H_1={(H,H),(H,T)}$, that is, having heads in the first toss, and $H_2=\{(H,H),(T,H)\}$, that is, having heads in the second toss, we can get the same result assuming that these events are independent,&lt;/p&gt;
&lt;p&gt;$$P(H,H)= P(H_1\cap H_2) = P(H_1)P(H_2) = \frac{2}{4}\frac{2}{4}=\frac{1}{4}=0.25.$$&lt;/p&gt;
&lt;h2 id=&#34;probability-space&#34;&gt;Probability Space&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Probability space&lt;/strong&gt;. A &lt;em&gt;probability space&lt;/em&gt; of a random experiment is a triplet $(\Omega,\mathcal{F},P)$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$ is the sample space of the experiment.&lt;/li&gt;
&lt;li&gt;$\mathcal{F}$ is a set of events of the experiment.&lt;/li&gt;
&lt;li&gt;$P$ is a probability function.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;If we know the probabilities of all the elements of $\Omega$, then we can calculate the probability of every event in $\mathcal{F}$ and we can construct easily the probability space.&lt;/p&gt;
&lt;h3 id=&#34;probability-space-construction&#34;&gt;Probability space construction&lt;/h3&gt;
&lt;p&gt;In order to determine the probability of every elemental event we can use a tree diagram, using the following rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For every node of the tree, label the incoming edge with the probability of the variable in that level having the value of the node, conditioned by events corresponding to its ancestor nodes in the tree.&lt;/li&gt;
&lt;li&gt;The probability of every elemental event in the leaves is the product of the probabilities on edges that go form the root to the leave.&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;../img/probability/probability_space.svg&#34; alt=&#34;Diagram tree of a probability space&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;probability-tree-with-dependent-variables&#34;&gt;Probability tree with dependent variables&lt;/h3&gt;
&lt;p&gt;In a probability tree with dependent variables, the probababilities of every level of the tree are different depending on the outcome of the previous leves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In a population there are a 30% of smokers and we know that there are a 40% of smokers with breast cancer, while only 10% of non-smokers have breast cancer. The probability tree of the probability space of the random experiment consisting of picking a random person and measuring the variables smoking and breast cancer is shown below.&lt;/p&gt;
&lt;img src=&#34;../img/probability/smoking_cancer_probability_space.svg&#34; alt=&#34;Tree diagram of the probability space of smoking and having breast cancer&#34; width=&#34;550&#34;&gt;
&lt;h3 id=&#34;probability-tree-with-independent-variables&#34;&gt;Probability tree with independent variables&lt;/h3&gt;
&lt;p&gt;In a probability tree with independent variables, the probabilities of every level of the tree are the same no matter the outcome of the previous leves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The probability tree of the random experiment of tossing two coins is shown below.&lt;/p&gt;
&lt;img src=&#34;../img/probability/coins_probability_space.svg&#34; alt=&#34;Tree diagram of the probability space of tossing two coins&#34; width=&#34;550&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In a population there are 40% of males and 60% of females, the probability tree of drawing a random sample of three persons is shown below.&lt;/p&gt;
&lt;img src=&#34;../img/probability/sample_probability_space.svg&#34; alt=&#34;Diagram tree of the probability space of the gender of three random individuals&#34; width=&#34;600&#34;&gt;
&lt;h2 id=&#34;total-probability-theorem&#34;&gt;Total probability theorem&lt;/h2&gt;
&lt;h3 id=&#34;partition-of-the-sample-space&#34;&gt;Partition of the sample space&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Partition of the sample space&lt;/strong&gt;. A collection of events $A_1,A_2,\ldots,A_n$ of the same sample space $\Omega$ is a &lt;em&gt;partition&lt;/em&gt; of the sample space if it satisfies the following conditions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The union of the events is the sample space, that is, $A_1\cup \cdots\cup A_n =\Omega$.&lt;/li&gt;
&lt;li&gt;All the events are mutually incompatible, that is, $A_i\cap A_j = \emptyset$ $\forall i\neq j$.&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;img src=&#34;../img/probability/sample_space_partition.svg&#34; alt=&#34;Partition of a sample space&#34; width=&#34;300&#34;&gt;
&lt;p&gt;Usually it is easy to get a partition of the sample space splitting a population according to some categorical variable, like for example gender, blood type, etc.&lt;/p&gt;
&lt;h3 id=&#34;total-probability-theorem-1&#34;&gt;Total probability theorem&lt;/h3&gt;
&lt;p&gt;If we have a partition of a sample space, we can use it to calculate the probabilities of other events in the same sample space.&lt;/p&gt;
&lt;div class=&#34;alert alert-theo&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Theorem - Total probability&lt;/strong&gt;. Given a partition $A_1,\ldots,A_n$ of a sample space $\Omega$, the probability of any other event $B$ of the same sample space can be calculated with the formula&lt;/p&gt;
&lt;p&gt;$$P(B) = \sum_{i=1}^n P(A_i\cap B) = \sum_{i=1}^n P(A_i)P(B|A_i).$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-18&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-18&#34;&gt;
      Proof
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-18&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      &lt;p&gt;The proof of the theorem is quite simple. As $A_1,\ldots,A_n$ is a partition of $\Omega$, we have&lt;/p&gt;
&lt;p&gt;$$B = B\cap \Omega = B\cap (A_1\cup \cdots \cup A_n) = (B\cap A_1)\cup \cdots \cup (B\cap A_n).$$&lt;/p&gt;
&lt;p&gt;And all the events of this union are mutually incompatible as $A_1,\ldots,A_n$ are, thus&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
P(B) &amp;amp;= P((B\cap A_1)\cup \cdots \cup (B\cap A_n)) = P(B\cap A_1)+\cdots + P(B\cap A_n) =\newline
&amp;amp;= P(A_1)P(B|A_1)+\cdots + P(A_n)P(B|A_n) = \sum_{i=1}^n P(A_i)P(B|A_i).
\end{aligned}
$$&lt;/p&gt;
&lt;img src=&#34;../img/probability/total_probability.svg&#34; alt=&#34;Graphical proof of the total probability theorem&#34; width=&#34;300&#34;&gt;

    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. A symptom $S$ can be caused by a disease $D$, but it can also be present in persons without the disease. In a population, the rate of people with the disease is $0.2$. We know also that $90%$ of persons with the disease have the symptom, while only $40%$ of persons without the disease have it.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What is the probability that a random person of the population has the symptom?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To answer the question we can apply the total probability theorem using the partition $\{A,\bar A\}$:&lt;/p&gt;
&lt;p&gt;$$P(S) = P(D)P(S|D)+P(\bar D)P(S|\bar D) = 0.2\cdot 0.9 + 0.8\cdot 0.4 = 0.5.$$&lt;/p&gt;
&lt;p&gt;That is, half of the population has the symptom.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Indeed, it is a weighted mean of probabilities!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The answer to the previous question is even clearer with the tree diagram of the probability space.&lt;/p&gt;
&lt;img src=&#34;../img/probability/total_probability_space.svg&#34; alt=&#34;Diagram tree of the probability space of a diagnosis experiment&#34; width=&#34;550&#34;&gt;
&lt;p&gt;$$
\begin{aligned}
P(S) &amp;amp;= P(D,S) + P(\bar D,S) = P(D)P(S|D)+P(\bar D)P(S|\bar D)\newline
&amp;amp; = 0.2\cdot 0.9+ 0.8\cdot 0.4 = 0.18 + 0.32 = 0.5.
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;bayes-theorem&#34;&gt;Bayes theorem&lt;/h2&gt;
&lt;p&gt;A partition of a sample space $A_1,\cdots,A_n$ may also be interpreted as a set of feasible hypothesis for a fact $B$.&lt;/p&gt;
&lt;p&gt;In such cases it may be helpful to calculate the posterior probability $P(A_i\vert B)$ of every hypothesis.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Bayes&lt;/strong&gt;. Given a partition $A_1,\ldots,A_n$ of a sample space $\Omega$ and another event $B$ of the same sample space, the conditional probability of every even $A_i$ $i=1,\ldots,n$ on $B$ can be calculated with the following formula&lt;/p&gt;
&lt;p&gt;$$P(A_i|B) = \frac{P(A_i\cap B)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum_{i=1}^n P(A_i)P(B|A_i)}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the previous example, a more interesting question is about the diagnosis for a person with the symptom.&lt;/p&gt;
&lt;p&gt;In this case we can interpret $D$ and $\overline{D}$ as the two feasible hypothesis for the symptom $S$. The prior probabilities for them are $P(D)=0.2$ and $P(\overline{D})=0.8$. That means that if we do not have information about the symptom, the diagnosis would be that the person does not have the disease.&lt;/p&gt;
&lt;p&gt;However, if after examining the person we observe the symptom, that information changes the uncertainty about the hypothesis, and we need calculate the posterior probabilities to diagnose, that is, $P(D\vert S)$ and $P(\overline{D}\vert S)$.&lt;/p&gt;
&lt;p&gt;To calculate the posterior probabilities we can use the Bayes theorem.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
P(D|S) &amp;amp;= \frac{P(D)P(S|D)}{P(D)P(S|D)+P(\overline{D})P(S|\overline{D})} = \frac{0.2\cdot 0.9}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.18}{0.5}=0.36,\newline
P(\overline{D}|S) &amp;amp;= \frac{P(\overline{D})P(S|\overline{D})}{P(D)P(S|D)+P(\overline{D})P(S|\overline{D})} = \frac{0.8\cdot 0.4}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.32}{0.5}=0.64.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As we can see the probability of having the disease has increased. Nevertheless, the probability of not having the disease is still greater than the probability of having it, and for that reason, the diagnosis is not having the disease.&lt;/p&gt;
&lt;p&gt;In this case it is said the the symptom $S$ is &lt;em&gt;not decisive&lt;/em&gt; in order to diagnose the disease.&lt;/p&gt;
&lt;h2 id=&#34;epidemiology&#34;&gt;Epidemiology&lt;/h2&gt;
&lt;p&gt;One of the branches of Medicine that makes an intensive use of probability is , that study the distribution and causes of diseases in populations identifying risk factors for disease and targets for preventive healthcare.&lt;/p&gt;
&lt;p&gt;In Epidemiology we are interested in how often appears an event or &lt;em&gt;medical event&lt;/em&gt; $D$ (typically a disease like flu, a risk factor like smoking or a protection factor like a vaccine) that is measured as a nominal variable with two categories (occurrence or not of the event).&lt;/p&gt;
&lt;p&gt;There are different measures related to the frequency of a medical event. The most important are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prevalence&lt;/li&gt;
&lt;li&gt;Incidence&lt;/li&gt;
&lt;li&gt;Relative risk&lt;/li&gt;
&lt;li&gt;Odds ratio&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prevalence&#34;&gt;Prevalence&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Prevalence&lt;/strong&gt;. The &lt;em&gt;prevalence&lt;/em&gt; of a medical event $D$ is the proportion of a particular population that is affected by a medical event.&lt;/p&gt;
&lt;p&gt;$$\mbox{Prevalence}(D) = \frac{\mbox{Num people affected by $D$}}{\mbox{Population size}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Often, the prevalence is estimated from a sample as the relative frequency of people affected by the event in the sample.
It is also common to express that frequency as a percentage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. To estimate the prevalence of flu a sample of 1000 persons has been studied and 150 of them had flu. Thus, the prevalence of flu is approximately 150/1000=0.15, that is, a 15%.&lt;/p&gt;
&lt;h3 id=&#34;incidence&#34;&gt;Incidence&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Incidence&lt;/em&gt; measures the probability of occurrence of a medical event in a population within a given period of time. Incidence can be measured as a cumulative proportion or as a rate.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Cumulative incidence&lt;/strong&gt;. The &lt;em&gt;cumulative incidence&lt;/em&gt; of a medical event $D$ is the proportion of people that experience the event in a period of time, that is, the number of new cases with the event in the period of time divided by the size of the population at risk.&lt;/p&gt;
&lt;p&gt;$$R(D)=\frac{\mbox{Num of new cases with $D$}}{\mbox{Population at risk size}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. A population initially contains 1000 persons without flu and after two years of observation 160 of them got the flu.
The incidence proportion of flu is 160 cases per 1000 persons per two years, i.e. 16% per two years.&lt;/p&gt;
&lt;h3 id=&#34;incidence-rate-or-absolute-risk&#34;&gt;Incidence rate or Absolute risk&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Incidence rate&lt;/strong&gt;. The &lt;em&gt;incidence rate&lt;/em&gt; or &lt;em&gt;absolute risk&lt;/em&gt; of a medical event $D$ is the number of new cases with the event divided by the size of the population at risk and by the number of units of time in a given period.&lt;/p&gt;
&lt;p&gt;$$R(D)=\frac{\mbox{Num of new cases with $D$}}{\mbox{Population at risk size}\times \mbox{Num of unit time intervals}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. A population initially contains $1000$ persons without flu and after two years of observation 160 of them got the flu.
If we consider the year as the unit of time, the incidence rate of flu is 160 cases per $1000$ persons divided by two years, i.e. 80 cases per 1000 persons-year or 8% persons per year.&lt;/p&gt;
&lt;h3 id=&#34;prevalence-vs-incidence&#34;&gt;Prevalence vs Incidence&lt;/h3&gt;
&lt;p&gt;Prevalence must not be confused with incidence.
Prevalence indicates how widespread the medical event is, and is more a measure of the burden of the event on society with no regard to time at risk or when  subjects may have been exposed to a possible risk factor, whereas incidence conveys information about the risk of being affected by the event.&lt;/p&gt;
&lt;p&gt;Prevalence can be measured in cross-sectional studies at a particular time, while in order to measure incidence we need a longitudinal study observing the individuals during a period of time.&lt;/p&gt;
&lt;p&gt;Incidence is usually more useful than prevalence in understanding the event etiology: for example, if the incidence of a disease in a population increases, then there is a risk factor that promotes it.&lt;/p&gt;
&lt;p&gt;When the incidence is approximately constant for the duration of the event, prevalence is approximately the product of event incidence and average event duration, so&lt;/p&gt;
&lt;p&gt;$$\mbox{prevalence} = \mbox{incidence} \times \mbox{duration}$$&lt;/p&gt;
&lt;h3 id=&#34;comparing-risks&#34;&gt;Comparing risks&lt;/h3&gt;
&lt;p&gt;In order to determine if a factor or characteristic is associated with the medical event we need to compare the risk of the medical event in two populations, one exposed to the factor and the other not exposed.
The group of people exposed to the factor is known as the &lt;em&gt;treatment group&lt;/em&gt; or &lt;em&gt;experimental group&lt;/em&gt; and the group of people unexposed as the &lt;em&gt;control group&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Usually the cases observed for each group are represented in a 2$\times$2 table like the one below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Event $D$&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;No event $\overline D$&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Treatment group (exposed)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$a$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$b$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Control group(unexposed)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$c$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;$d$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;attributable-risk-or-risk-difference-rd&#34;&gt;Attributable risk or Risk difference $RD$&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Attributable risk&lt;/strong&gt;. The &lt;em&gt;attributable risk&lt;/em&gt; or &lt;em&gt;risk difference&lt;/em&gt; of a medical event $D$ for people exposed to a factor is the difference between the absolute risks of the treatment group and the control group.&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}RD(D) &amp;amp;= \mbox{Risk in treatment group}-\mbox{Risk in control group}=\newline
&amp;amp;= R_T(D)-R_C(D)=\frac{a}{a+b}-\frac{c}{c+d}.
\end{aligned}
$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The attributable risk is the risk of an event that is specifically due to the factor of interest.&lt;/p&gt;
&lt;p&gt;Observe that the attributable risk can be positive, when the risk of the treatment group is greater than the risk of the control group, and negative, on the contrary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. To determine the effectiveness of a vaccine against the flu, a sample of 1000 person without flu was selected at the beginning of the year.
Half of them were vaccinated (treatment group) and the other received a placebo (control group).
The table below summarize the results at the end of the year.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;Flu $D$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;No flu $\overline D$&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Treatment group(vaccinated)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;20&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Control group(Unvaccinated)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;420&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The attributable risk of getting the flu for people vaccinated is&lt;/p&gt;
&lt;p&gt;$$AR(D) = \frac{20}{20+480}-\frac{80}{80+420} = -0.12.$$&lt;/p&gt;
&lt;p&gt;This means that the risk of getting flu in vaccinated people is a 12% less than in unvaccinated.&lt;/p&gt;
&lt;h3 id=&#34;relative-risk-rr&#34;&gt;Relative risk $RR$&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Relative risk&lt;/strong&gt;. The &lt;em&gt;relative risk&lt;/em&gt; of a medical event $D$ for people exposed to a factor is the quotient between the proportions of people that acquired the event in a period of time in the treatment and control groups. That is, the quotient between the incidences of the treatment and the control groups.&lt;/p&gt;
&lt;p&gt;$$RR(D)=\frac{\mbox{Risk in treatment group}}{\mbox{Risk in control group}}=\frac{R_1(D)}{R_0(D)}=\frac{a/(a+b)}{c/(c+d)}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Relative risk compares the risk of a medical event between the treatment and the control groups.&lt;/p&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;$RR=1$ $\Rightarrow$ There is no association between the event and the exposure to the factor.&lt;/li&gt;
&lt;li&gt;$RR&amp;lt;1$ $\Rightarrow$ Exposure to the factor decreases the risk of the event.&lt;/li&gt;
&lt;li&gt;$RR&amp;gt;1$ $\Rightarrow$ Exposure to the factor increases the risk of the event.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The further from 1, the stronger the association.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. To determine the effectiveness of a vaccine against the flu, a sample of 1000 person without flu was selected at the beginning of the year.
Half of them were vaccinated (treatment group) and the other received a placebo (control group).
The table below summarize the results at the end of the year.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;Flu $D$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;No flu $\overline D$&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Treatment group(vaccinated)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;20&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Control group(Unvaccinated)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;420&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The relative risk of getting the flu for people vaccinated is&lt;/p&gt;
&lt;p&gt;$$RR(D) = \frac{20/(20+480)}{80/(80+420)} = 0.25.$$&lt;/p&gt;
&lt;p&gt;This means that vaccinated people were only one-fourth as likely to develop flu as were unvaccinated people, i.e. the vaccine reduce the risk of flu by 75%.&lt;/p&gt;
&lt;h3 id=&#34;odds&#34;&gt;Odds&lt;/h3&gt;
&lt;p&gt;An alternative way of measuring the risk of a medical event is the &lt;em&gt;odds&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Odds&lt;/strong&gt;. The &lt;em&gt;odds&lt;/em&gt; of a medical event $D$ in a population is the quotient between the people that acquired the event and people that not in a period of time.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Unlike incidence or absolute risk, that is a proportion less than 1, the odds can be greater than 1. However, it is possible to convert an odd into a probability with the formula&lt;/p&gt;
&lt;p&gt;$$P(D) = \frac{\mbox{ODDS}(D)}{\mbox{ODDS}(D)+1}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. A population initially contains $1000$ persons without flu and after a year 160 of them got the flu. The odds of flu is 160/840.&lt;/p&gt;
&lt;p&gt;Observe that the incidence is 160/1000.&lt;/p&gt;
&lt;h3 id=&#34;odds-ratio-or&#34;&gt;Odds ratio $OR$&lt;/h3&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Odds ratio&lt;/strong&gt;. The &lt;em&gt;odds ratio&lt;/em&gt; of a medical event $D$ for people exposed to a factor is the quotient between the odds of people that acquired the event in a period of time in the treatment and control groups.&lt;/p&gt;
&lt;p&gt;$$OR(D)=\frac{\mbox{Odds in treatment group}}{\mbox{Odds in control group}}=\frac{a/b}{c/d}=\frac{ad}{bc}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Odds ratio compares the odds of a medical event between the treatment and the control groups.
The interpretation is similar to the relative risk.&lt;/p&gt;
&lt;div class=&#34;alert alert-int&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;$OR=1$ $\Rightarrow$ There is no association between the event and the exposure to the factor.&lt;/li&gt;
&lt;li&gt;$OR&amp;lt;1$ $\Rightarrow$ Exposure to the factor decreases the risk of the event.&lt;/li&gt;
&lt;li&gt;$OR&amp;gt;1$ $\Rightarrow$ Exposure to the factor increases the risk of the event.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The further from 1, the stronger the association.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. To determine the effectiveness of a vaccine against the flu, a sample of 1000 person without flu was selected at the beginning of the year.
Half of them were vaccinated (treatment group) and the other received a placebo (control group).
The table below summarize the results at the end of the year.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;Flu $D$&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;No flu $\overline D$&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Treatment group(vaccinated)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;20&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Control group(Unvaccinated)&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;420&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The odds ratio of getting the flu for people vaccinated is&lt;/p&gt;
&lt;p&gt;$$OR(D) = \frac{20/480}{80/420} = 0.21875.$$&lt;/p&gt;
&lt;p&gt;This means that the odds of getting the flu versus not getting the flu in vaccinated individuals is almost one fifth of that in unvaccinated, i.e. approximately for every 22 persons vaccinated with flu there will be 100 persons unvaccinated with flu.&lt;/p&gt;
&lt;h3 id=&#34;relative-risk-vs-odds-ratio&#34;&gt;Relative risk vs Odds ratio&lt;/h3&gt;
&lt;p&gt;Relative risk and odds ratio are two measures of association but their interpretation is slightly different. While the relative risk expresses a comparison of risks between the treatment and control groups, the odds ratio expresses a comparison of odds, that is not the same than the risk.
Thus, an odds ratio of 2 &lt;em&gt;does not&lt;/em&gt; mean that the treatment group has the double of risk of acquire the medical event.&lt;/p&gt;
&lt;p&gt;The interpretation of the odds ratio is trickier because is counterfactual, and give us how many times is more frequent the event in the treatment group in comparison with the control group, assuming that in the control group the event is as frequent as the non-event.&lt;/p&gt;
&lt;p&gt;The advantage of the odds ratio is that it does not depend on the prevalence or the incidence of the event, and must be used necessarily when the number of people with the medical event is selected arbitrarily in both groups, like in the case-control studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In order to determine the association between lung cancer and smoking two samples were selected (the second one with the double of non-cancer individuals) getting the following results:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sample 1&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;Cancer&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;No cancer&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Smokers&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;60&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Non-smokers&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;40&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;320&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;$$
\begin{aligned}
RR(D) &amp;amp;= \frac{60/(60+80)}{40/(40+320)} = 3.86.\newline
OR(D) &amp;amp;= \frac{60/80}{40/320} = 6.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sample 2&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;Cancer&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;No cancer&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Smokers&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;60&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;160&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Non-smokers&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;40&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;640&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;$$
\begin{aligned}
RR(D) &amp;amp;= \frac{60/(60+160)}{40/(40+640)} = 4.64.\newline
OR(D) &amp;amp;= \frac{60/160}{40/640} = 6.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Thus, when we change the incidence or the prevalence of the event (lung cancer) the relative risk changes, while the odds ratio not.&lt;/p&gt;
&lt;p&gt;The relation between the relative risk and the odds ratio is given by the following formula&lt;/p&gt;
&lt;p&gt;$$RR = \frac{OR}{1-R_0+R_0OR} = OR\frac{1-R_1}{1-R_0},$$&lt;/p&gt;
&lt;p&gt;where $R_0$ and $R_1$ are the prevalence or the incidence in control and treatment groups respectively.&lt;/p&gt;
&lt;p&gt;The odds ratio always overestimate the relative risk when it is greater than 1 and underestimate it when it is less than 1.
However, with rare medical events (with very small prevalence or incidence) the relative risk and the odds ratio are almost the same.&lt;/p&gt;
&lt;img src=&#34;../img/probability/odds_ratio_vs_relative_risk.svg&#34; alt=&#34;Chart of the relation between the odds ratio and the relative risk for different incidences&#34; width=&#34;500&#34;&gt;
&lt;h2 id=&#34;diagnostic-tests&#34;&gt;Diagnostic tests&lt;/h2&gt;
&lt;p&gt;In Epidemiology it is common to use diagnostic test to diagnose diseases.&lt;/p&gt;
&lt;p&gt;In general, diagnostic tests are not fully reliable and have some risk of misdiagnosis as it is represented in the table below.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|l|c|c|}
\hline
&amp;amp; \mbox{Presence of disease }D &amp;amp; \mbox{Absence of disease }\bar D\newline
\hline
\mbox{Test outcome positive } + &amp;amp; \color{green}{ \mbox{True Positive } TP} &amp;amp; \color{red}{\mbox{False
Positive } FP}\newline
\hline
\mbox{Test outcome negative } - &amp;amp; \color{red}{\mbox{False Negative } FN} &amp;amp; \color{green}{\mbox{True Negative } TN}\newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;h3 id=&#34;sensitivity-and-specificity-of-a-diagnostic-test&#34;&gt;Sensitivity and specificity of a diagnostic test&lt;/h3&gt;
&lt;p&gt;The performance of a diagnostic test depends on the following two probabilities.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Sensitivity&lt;/strong&gt;. The &lt;em&gt;sensitivity&lt;/em&gt; of a diagnostic test is the proportion of positive outcomes in persons with the disease$$P(+|D)=\frac{TP}{TP+FN}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Specificity&lt;/strong&gt;. The &lt;em&gt;specificity&lt;/em&gt; of a diagnostic test is the proportion of negative outcomes in persons without the disease$$P(-|\overline{D})=\frac{TN}{TN+FP}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;sensitivity-and-specificity-interpretation&#34;&gt;Sensitivity and specificity interpretation&lt;/h3&gt;
&lt;p&gt;Usually, there is a trade-off between sensitivity and specificity.&lt;/p&gt;
&lt;p&gt;A test with high sensitivity will detect the disease in most sick persons, but it will produce also more false positives than a less sensitive test. This way, a positive outcome in a test with high sensitivity is not useful for confirming the disease, but a negative outcome is useful for ruling out the disease, since it rarely misdiagnoses those who have the disease.&lt;/p&gt;
&lt;p&gt;On the other hand, a test with a high specificity will rule out the disease in most healthy persons, but it will produce also more false negatives than a less specific test. Thus, a negative outcome in a test with high specificity is not useful for ruling out the disease, but a positive is useful to confirm the disease, since it rarely give positive outcomes in healthy people.&lt;/p&gt;
&lt;p&gt;Deciding on a test with greater sensitivity or a test with greater specificity depends on the type of disease and the goal of the test. In general, we will use a sensitive test when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The disease is serious and it is important to dectect it.&lt;/li&gt;
&lt;li&gt;The disease is curable.&lt;/li&gt;
&lt;li&gt;The false positives do not provoke serious traumas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An we will use a specific test when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The disease is important but difficult or impossible to cure.&lt;/li&gt;
&lt;li&gt;The false positives provoke serious traumas.&lt;/li&gt;
&lt;li&gt;The treatment of false positives can have dangerous consequences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;predictive-values-of-a-diagnostic-test&#34;&gt;Predictive values of a diagnostic test&lt;/h3&gt;
&lt;p&gt;But the most important aspect of a diagnostic test is its predictive power, that is measured with the following two posterior probabilities.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Positive predictive value $PPV$&lt;/strong&gt;. The &lt;em&gt;positive predictive value&lt;/em&gt; of a diagnostic test is the proportion of persons with the disease to persons with a positive outcome$$P(D|+) = \frac{TP}{TP+FP}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Negative predictive value $NPV$&lt;/strong&gt;. The &lt;em&gt;negative predictive value&lt;/em&gt; of a diagnostic test is the proportion of persons without the disease to persons with a negative outcome$$P(\overline{D}|-) = \frac{TN}{TN+FN}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Positive and negative predictive values allow to confirm or to rule out the disease, respectively, if they reach at least a threshold of $0.5$.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{rcl}
PPV&amp;gt;0.5 &amp;amp; \Rightarrow &amp;amp; \mbox{Disease diagnostic}\newline
NPV&amp;gt;0.5 &amp;amp; \Rightarrow &amp;amp; \mbox{Not disease diagnostic}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;However, these probabilities depends on the proportion of persons with the disease in the population $P(D)$ that is known as of the disease. They can be calculated from the sensitivity and the specificity of the diagnostic test using the Bayes theorem.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
PPV=P(D|+) &amp;amp;= \frac{P(D)P(+|D)}{P(D)P(+|D)+P(\overline{D})P(+|\overline{D})}\newline
NPV=P(\overline{D}|-) &amp;amp;= \frac{P(\overline{D})P(-|\overline{D})}{P(D)P(-|D)+P(\overline{D})P(-|\overline{D})}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Thus, with frequent diseases, the positive predictive value increases, and with rare diseases, the negative predictive value increases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. A diagnostic test for the flu has been tried in a random sample of 1000 persons. The results are summarized in the table below.&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|l|c|c|}
\hline
&amp;amp; \mbox{Presence of flu } D &amp;amp; \mbox{Absence of flu } \bar D\newline
\hline
\mbox{Test outcome } + &amp;amp; 95 &amp;amp; 90 \newline
\hline
\mbox{Test outcome }- &amp;amp; 5 &amp;amp; 810 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;According to this sample, the prevalence of the flu can be estimated as&lt;/p&gt;
&lt;p&gt;$$P(D) = \frac{95+5}{1000} = 0.1.$$&lt;/p&gt;
&lt;p&gt;The sensitivity of this diagnostic test is&lt;/p&gt;
&lt;p&gt;$$P(+|D) = \frac{95}{95+5}= 0.95.$$&lt;/p&gt;
&lt;p&gt;And the specificity is&lt;/p&gt;
&lt;p&gt;$$P(-|\overline{D}) = \frac{810}{90+810}=0.9.$$&lt;/p&gt;
&lt;p&gt;The predictive positive value of the diagnostic test is&lt;/p&gt;
&lt;p&gt;$$PPV = P(D|+) = \frac{95}{95+90} = 0.5135.$$&lt;/p&gt;
&lt;p&gt;As this value is over $0.5$, this means that we will diagnose the flu if the outcome of the test is positive. However, the confidence in the diagnostic will be low, as this value is pretty close to $0.5$.&lt;/p&gt;
&lt;p&gt;On the other hand, the predictive negative value is&lt;/p&gt;
&lt;p&gt;$$NPV = P(\overline{D}|-) = \frac{810}{5+810} = 0.9939.$$&lt;/p&gt;
&lt;p&gt;As this value is almost 1, that means that is almost sure that a person does not have the flu if he or she gets a negative outcome in the test.&lt;/p&gt;
&lt;p&gt;Thus, this test is a powerful test to rule out the flu, but not so powerful to confirm it.&lt;/p&gt;
&lt;h3 id=&#34;likelihood-ratios-of-a-diagnostic-test&#34;&gt;Likelihood ratios of a diagnostic test&lt;/h3&gt;
&lt;p&gt;The following measures are usually derived from sensitivity and specificity.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Positive likelihood ratio $LR+$&lt;/strong&gt;. The &lt;em&gt;positive likelihood ratio&lt;/em&gt; of a diagnostic test is the ratio between the probability of positive outcomes in persons with the disease and healthy persons respectively,&lt;/p&gt;
&lt;p&gt;$$LR+=\frac{P(+|D)}{P(+|\overline{D})} = \frac{\mbox{Sensitivity}}{1-\mbox{Specificity}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Negative likelihood ratio $LR-$&lt;/strong&gt;. The &lt;em&gt;negative likelihood ratio&lt;/em&gt; of a diagnostic test is the ratio between the probability of negative outcomes in persons with the disease and healthy persons respectively,
$$LR-=\frac{P(-|D)}{P(-|\overline{D})} = \frac{1-\mbox{Sensitivity}}{\mbox{Specificity}}$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Positive likelihood ratio can be interpreted as the number of times that a positive outcome is more probable in people with the disease than in people without it.&lt;/p&gt;
&lt;p&gt;On the other hand, negative likelihood ratio can be interpreted as the number of times that a negative outcome is more probable in people with the disease than in people without it.&lt;/p&gt;
&lt;p&gt;Post-test probabilities can be calculated from pre-test probabilities through likelihood ratios.&lt;/p&gt;
&lt;p&gt;$$P(D|+) = \frac{P(D)P(+|D)}{P(D)P(+|D)+P(\overline{D})P(+|\overline{D})} = \frac{P(D)LR+}{1-P(D)+P(D)LR+}$$&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A likelihood ratio greater than 1 increases the probability of disease.&lt;/li&gt;
&lt;li&gt;A likelihood ratio less than 1 decreases the probability of disease.&lt;/li&gt;
&lt;li&gt;A likelihood ratio 1 does not change the pre-test probability.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/probability/likelihood_ratios.svg&#34; alt=&#34;Chart of the relation between the likelihood ratio and the pre-test and post-test probabilities&#34; width=&#34;600&#34;&gt;
</description>
    </item>
    
    <item>
      <title>Discrete Random Variables</title>
      <link>/en/teaching/statistics/manual/discrete-random-variables/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/teaching/statistics/manual/discrete-random-variables/</guid>
      <description>&lt;h2 id=&#34;random-variables&#34;&gt;Random variables&lt;/h2&gt;
&lt;p&gt;The process of drawing a sample randomly is a random experiment and any variable measured in the sample is a &lt;em&gt;random variable&lt;/em&gt; because the values taken by the variable in the individuals of the sample are a matter of chance.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Random variable&lt;/strong&gt;. A &lt;em&gt;random variable&lt;/em&gt; $X$ is a function that maps every element of the sample space of a random experiment to a real number.&lt;/p&gt;
&lt;p&gt;$$X:\Omega \rightarrow \mathbb{R}$$&lt;/p&gt;
&lt;p&gt;The set of values that the variable can assume is called the &lt;em&gt;range&lt;/em&gt; and is represented by $\mbox{Ran}(X)$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In essence, a random variable is a variable whose values come from a random experiment, and every value has a probability of occurrence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The variable $X$ that measures the outcome of rolling a dice is a random variable and its range is $\mbox{Ran}(X)={1,2,3,4,5,6}$.&lt;/p&gt;
&lt;h3 id=&#34;types-of-random-variables&#34;&gt;Types of random variables&lt;/h3&gt;
&lt;p&gt;There are two types of random variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete&lt;/strong&gt;. They take isolated values, and their range is numerable.
Example. Number of children of a family, number of smoked cigarettes, number of subjects passed, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Continuous&lt;/strong&gt;. They can take any value in a real interval, and their range is non-numerable.
Example. Weight, height, age, cholesterol level, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The way of modelling each type of variable is different. In this chapter we are going to study how to model discrete variables.&lt;/p&gt;
&lt;h2 id=&#34;probability-distribution-of-a-discrete-random-variable&#34;&gt;Probability distribution of a discrete random variable&lt;/h2&gt;
&lt;p&gt;As values of a discrete random variable are linked to the elementary events of a random experiment, every value has a probability.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Probability function&lt;/strong&gt;. The &lt;em&gt;probability function&lt;/em&gt; of a discrete random variable $X$ is the function $f(x)$ that maps every value $x_i$ of the variable to its probability$$f(x_i) = P(X=x_i).$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can also accumulate probabilities the same way that we accumulated sample frequencies.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Definition - Distribution function&lt;/strong&gt;. The &lt;em&gt;distribution function&lt;/em&gt; of a discrete random variable $X$ is the function $F(x)$ that maps every value $x_i$ of the variable to the probability of having a value less than or equal to $x_i$$$F(x_i) = P(X\leq x_i) = f(x_1)+\cdots +f(x_i).$$
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The range of a discrete random variable and its probability function is known as &lt;strong&gt;probability distribution&lt;/strong&gt; of the variable, and it is usually presented in a table&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c|cccc|c|}
\hline
X &amp;amp; x_1 &amp;amp; x_2 &amp;amp; \cdots &amp;amp; x_n &amp;amp; \sum\newline \hline
f(x) &amp;amp; f(x_1) &amp;amp; f(x_2) &amp;amp; \cdots &amp;amp; f(x_n) &amp;amp; 1\newline
\hline
F(x) &amp;amp; F(x_1) &amp;amp; F(x_2) &amp;amp; \cdots &amp;amp; F(x_n) =1 &amp;amp; \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;The same way that the sample frequency table shows the distribution of values of a variable in the sample, the probability distribution of a discrete random variable shows the distribution of values in the whole population.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Let $X$ be the discrete random variable that measures the number of heads after tossing two coins. The probability tree of the random experiment is&lt;/p&gt;
&lt;img src=&#34;../img/drv/two_coins_probability_space.svg&#34; alt=&#34;Probability space of tossing two coins&#34; width=&#34;550&#34;&gt;
&lt;p&gt;According to this, the probability distribution of $X$ is&lt;/p&gt;
&lt;p&gt;$$\begin{array}{|c|ccc|}
\hline
X &amp;amp; 0 &amp;amp; 1 &amp;amp; 2\newline
\hline
f(x) &amp;amp; 0.25 &amp;amp; 0.5 &amp;amp; 0.25\newline
\hline
F(x) &amp;amp; 0.25 &amp;amp; 0.75 &amp;amp; 1 \newline
\hline
\end{array}
\qquad
F(x) =
\begin{cases}
0 &amp;amp; \mbox{si $x&amp;lt;0$}\newline
0.25 &amp;amp; \mbox{si $0\leq x&amp;lt; 1$}\newline
0.75 &amp;amp; \mbox{si $1\leq x&amp;lt; 2$}\newline
1 &amp;amp; \mbox{si $x\geq 2$}
\end{cases}
$$&lt;/p&gt;
&lt;img src=&#34;../img/drv/two_coins_probability_function.svg&#34; alt=&#34;Probability function of tossing two coins&#34; width=&#34;400&#34;&gt;
&lt;h3 id=&#34;population-statistics&#34;&gt;Population statistics&lt;/h3&gt;
&lt;p&gt;The same way we use sample statistics to describe the sample frequency distribution of a variable, we use population statistics to describe the probability distribution of a random variable in the whole population.&lt;/p&gt;
&lt;p&gt;The population statistics definition is analogous to the sample statistics definition, but using probabilities instead of relative frequencies.&lt;/p&gt;
&lt;p&gt;The most important are &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Discrete random variable mean&lt;/strong&gt; The &lt;em&gt;mean&lt;/em&gt; or the &lt;em&gt;expectec value&lt;/em&gt; of a discrete random variable $X$ is the sum of the products of its values and its probabilities:&lt;/p&gt;
&lt;p&gt;$$\mu = E(X) = \sum_{i=1}^n x_i f(x_i)$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Discrete random variable variance and standard deviation&lt;/strong&gt; The &lt;em&gt;variance&lt;/em&gt; of a discrete random variable $X$ is the sum of the products of its squared values and its probabilities, minus the squared mean:&lt;/p&gt;
&lt;p&gt;$$\sigma^2 = Var(X) = \sum_{i=1}^n x_i^2 f(x_i) -\mu^2$$&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;standard deviation&lt;/em&gt; of a random variable $X$ is the square root of the variance:&lt;/p&gt;
&lt;p&gt;$$\sigma = +\sqrt{\sigma^2}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In the random experiment of tossing two coins the probability distribution is&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{|c|ccc|}
\hline
X &amp;amp; 0 &amp;amp; 1 &amp;amp; 2\newline \hline
f(x) &amp;amp; 0.25 &amp;amp; 0.5 &amp;amp; 0.25\newline
\hline
F(x) &amp;amp; 0.25 &amp;amp; 0.75 &amp;amp; 1 \newline
\hline
\end{array}
$$&lt;/p&gt;
&lt;p&gt;The main population statistics are&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mu &amp;amp;= \sum_{i=1}^n x_i f(x_i) = 0\cdot 0.25 + 1\cdot 0.5 + 2\cdot 0.25 = 1 \mbox{ heads},\newline
\sigma^2 &amp;amp;= \sum_{i=1}^n x_i^2 f(x_i) -\mu^2 = (0^0\cdot 0.25 + 1^2\cdot 0.5 + 2^2\cdot 0.25) - 1^2 = 0.5 \mbox{ heads}^2,\newline
\sigma &amp;amp;= +\sqrt{0.5} = 0.71 \mbox{ heads}.
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;discrete-probability-distribution-models&#34;&gt;Discrete probability distribution models&lt;/h2&gt;
&lt;p&gt;According to the type of experiment where the random variable is measured, there are different probability distributions models. The most common are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discrete uniform&lt;/li&gt;
&lt;li&gt;Binomial&lt;/li&gt;
&lt;li&gt;Poisson&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;discrete-uniform-distribution-uab&#34;&gt;Discrete uniform distribution $U(a,b)$&lt;/h2&gt;
&lt;p&gt;When all the values of a random variable $X$ have equal probability, the probability distribution of $X$ is uniform.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Discrete uniform distribution $U(a,b)$&lt;/strong&gt;. A discrete random variable $X$ follows a &lt;em&gt;discrete uniform distribution model&lt;/em&gt; with parameters $a$ and $b$, noted $X\sim U(a,b)$, if its range is $\mbox{Ran}(X) = {a, a+1, \ldots,b}$ and its probability function is&lt;/p&gt;
&lt;p&gt;$$f(x)=\frac{1}{b-a+1}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Observe that $a$ and $b$ are the minimum and the maximum of the range respectively.&lt;/p&gt;
&lt;p&gt;The mean and the variance are&lt;/p&gt;
&lt;p&gt;$$\mu = \sum_{i=0}^{b-a}\frac{a+i}{b-a+1}=\frac{a+b}{2} \qquad \sigma^2 =\sum_{i=0}^{b-a}\frac{(a+i-\mu)^2}{b-a+1}=\frac{(b-a+1)^2-1}{12}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The variable that measures the outcome of rolling a dice follows a discrete uniform distribution model $U(1,6)$.&lt;/p&gt;
&lt;img src=&#34;../img/drv/discrete_uniform_probability_function.svg&#34; alt=&#34;Probability distribution of rolling a dice&#34; width=&#34;500&#34;&gt;
&lt;h2 id=&#34;binomial-distribution-bnp&#34;&gt;Binomial distribution $B(n,p)$&lt;/h2&gt;
&lt;p&gt;Usually the binomial distribution corresponds to a variable measured in a random experiment with the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The experiment consist in a sequence of $n$ repetitions of the same trial.&lt;/li&gt;
&lt;li&gt;Each trial is repeated in identical conditions and produces two possible outcomes known as &lt;em&gt;Success&lt;/em&gt; or &lt;em&gt;Failure&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The trials are independent.&lt;/li&gt;
&lt;li&gt;The probability of Success is the same in all the trials and is $P(\mbox{Success})=p$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under these conditions, the discrete random variable $X$ that measures the number of successes in the $n$ trials follows a &lt;em&gt;binomial distribution model&lt;/em&gt; with parameters $n$ and $p$.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Binomial distribution $(B(n,p)$&lt;/strong&gt;. A discrete random variable $X$ follows a &lt;em&gt;binomial distribution model&lt;/em&gt; with parameters $n$ and $p$, noted $X\sim B(n,p)$, if its range is $\mbox{Ran}(X) = {0,1,\ldots,n}$ and its probability function is&lt;/p&gt;
&lt;p&gt;$$f(x) = \binom{n}{x}p^x(1-p)^{n-x} = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Observe that $n$ is known as the number of repetitions of a trial and $p$ is known as the probability of Success in every repetition.&lt;/p&gt;
&lt;p&gt;The mean and the variance are&lt;/p&gt;
&lt;p&gt;$$\mu = n\cdot p \qquad \sigma^2 = n\cdot p\cdot (1-p).$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The variable that measures the number of heads after tossing 10 coins follows a binomial distribution model $B(10,0.5)$.&lt;/p&gt;
&lt;img src=&#34;../img/drv/binomial_probability_function.svg&#34; alt=&#34;Probability distribution of tossing 10 coins&#34; width=&#34;500&#34;&gt;
&lt;p&gt;According to this,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability of getting 4 heads is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$f(4) = \binom{10}{4}0.5^4 (1-0.5)^{10-4} = \frac{10!}{4!6!}0.5^40.5^6 = 210\cdot 0.5^{10} = 0.2051.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability of getting 2 or less heads is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\begin{aligned}
F(2) &amp;amp;= f(0) +f(1) + f(2) =\newline
&amp;amp;= \binom{10}{0}0.5^0 (1-0.5)^{10-0} + \binom{10}{1}0.5^1 (1-0.5)^{10-1} + \binom{10}{2}0.5^2 (1-0.5)^{10-2} =\newline
&amp;amp;= 0.0547.\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;And the expected number of heads is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\mu = 10\cdot 0.5 = 5 \mbox{ heads}.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; In a population there are a 40% of smokers. The variable $X$ that measures the number of smokers in a random sample with replacement of 3 persons follows a binomial distribution model $X\sim B(3,,0.4)$.&lt;/p&gt;
&lt;img src=&#34;../img/drv/binomial_probability_space.svg&#34; alt=&#34;Probability space of a binomial experiment&#34; width=&#34;600&#34;&gt;
&lt;p&gt;$$
\begin{align*}
f(0)&amp;amp;=\displaystyle\binom{3}{0}0.4^0(1-0.4)^{3-0}= 0.6^3,\newline
f(1)&amp;amp;=\displaystyle\binom{3}{1}0.4^1(1-0.4)^{3-1}= 3\cdot 0.4\cdot 0.6^2,\newline
f(2)&amp;amp;=\displaystyle\binom{3}{2}0.4^2(1-0.4)^{3-2}= 3\cdot 0.4^2\cdot 0.6,\newline
f(3)&amp;amp;=\displaystyle\binom{3}{3}0.4^3(1-0.4)^{3-3}= 0.4^3.
\end{align*}
$$&lt;/p&gt;
&lt;h2 id=&#34;poisson-distribution-plambda&#34;&gt;Poisson distribution $P(\lambda)$&lt;/h2&gt;
&lt;p&gt;Usually the Poisson distribution correspond to a variable measured in a random experiment with the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The experiment consists of observing the number of events occurring in a fixed interval of time or space. For instance, number of births in a month, number of emails in one hour, number of red blood cells in a volume of blood, etc.&lt;/li&gt;
&lt;li&gt;The events occur independently.&lt;/li&gt;
&lt;li&gt;The experiment produces the same average rate of events $\lambda$ for every interval unit.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under these conditions, the discrete random variable $X$ that measures the number of events in an interval unit follows a &lt;em&gt;Poisson distribution model&lt;/em&gt; with parameter $\lambda$.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Poisson distribution $P(\lambda)$&lt;/strong&gt;. A discrete random variable $X$ follows a &lt;em&gt;Poisson distribution model&lt;/em&gt; with parameter $\lambda$, noted $X\sim P(\lambda)$, if its range is $\mbox{Ran}(X) = {0,1,\ldots,\infty}$ and its probability function is&lt;/p&gt;
&lt;p&gt;$$f(x) = e^{-\lambda}\frac{\lambda^x}{x!}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Observe that $\lambda$ is the average rate of event for an interval unit, and it will change if the interval changes.&lt;/p&gt;
&lt;p&gt;The mean and the variance are&lt;/p&gt;
&lt;p&gt;$$\mu = \lambda \qquad \sigma^2 = \lambda.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. In a city there are an average of 4 births every day. The random variable $X$ that measures the number of births in a day in the city follows a Poisson distribution model $X\sim P(4)$.&lt;/p&gt;
&lt;img src=&#34;../img/drv/poisson_probability_function.svg&#34; alt=&#34;Probability distribution of a Poisson variable&#34; width=&#34;500&#34;&gt;
&lt;p&gt;According to this,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability that there are 5 births in a day is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$f(5) = e^{-4}\frac{4^5}{5!} = 0.1563.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability that there are less than 2 births in a day is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$F(1) = f(0)+f(1) = e^{-4}\frac{4^0}{0!} + e^{-4}\frac{4^1}{1!} = 5e^{-4} = 0.0916.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The probability that there are more than 1 birth a day is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$P(X&amp;gt;1) = 1-P(X\leq 1) = 1-F(1) = 1-0.0916 = 0.9084.$$&lt;/p&gt;
&lt;h3 id=&#34;approximation-of-binomial-by-poisson-distribution&#34;&gt;Approximation of Binomial by Poisson distribution&lt;/h3&gt;
&lt;p&gt;The Poisson distribution can be obtained from the Binomial distribution when the number of trials repetition tends to infinite and the probability of Success tends to zero.&lt;/p&gt;
&lt;div class=&#34;alert alert-theo&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Law or rare events&lt;/strong&gt;. The Binomial distribution $X\sim B(n,p)$ tends to the Poisson distribution $P(\lambda)$, with $\lambda=n\cdot p$, when $n$ tends to infinite and $p$ tends to zero, that is,&lt;/p&gt;
&lt;p&gt;$$\lim_{n\rightarrow \infty, p\rightarrow 0}\binom{n}{x}p^x(1-p)^{n-x} = e^{-\lambda}\frac{\lambda^x}{x!}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In practice, this approximation can be used for $n\geq 30$ and $p\leq 0.1$.&lt;/p&gt;
&lt;img src=&#34;../img/drv/law_rare_events.svg&#34; alt=&#34;Law of rare events&#34; width=&#34;500&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://aprendeconalf.shinyapps.io/interactive_charts/#law-rare-events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;/images/r_interactive_app.png&#34; alt=&#34;Interactive application&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. A vaccine produce an adverse reaction in 4% of cases. If a sample of 50 persons are vaccinated, what is the probability of having more than 2 persons with an adverse reaction?&lt;/p&gt;
&lt;p&gt;The variable that measures the number of persons with an adverse reaction in the sample follows a Binomial distribution model $X\sim B(50,0.04)$, but as $n=50&amp;gt;30$ and $p=0.04&amp;lt;0.1$, we can apply the law of rare events and use the Poisson distribution model $P(50\cdot 0.04)=P(2)$ to do the calculations.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
P(X&amp;gt;2) &amp;amp;= 1-P(X\leq 2) = 1-f(0)-f(1)-f(2) =\newline
&amp;amp;= 1-e^{-2}\frac{2^0}{0!}-e^{-2}\frac{2^1}{1!}-e^{-2}\frac{2^2}{2!} =\newline
&amp;amp;= 1-5e^{-2} = 0.3233.\end{aligned}
$$&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;To distinguish population statistics from sample statistics we use Greek letters.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Continuous Random Variables</title>
      <link>/en/teaching/statistics/manual/continuous-random-variables/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/teaching/statistics/manual/continuous-random-variables/</guid>
      <description>&lt;h2 id=&#34;probability-distribution-of-a-continuous-random-variable&#34;&gt;Probability distribution of a continuous random variable&lt;/h2&gt;
&lt;p&gt;Continuous random variables, unlike discrete random variables, can take any value in a real interval. Thus the range of a continuous random variables is infinite and uncountable.&lt;/p&gt;
&lt;p&gt;Such a density of values makes impossible to compute the probability for each one of them, and therefore, it’s not possible to define a probabilistic model trough a probability function like with discrete random variables.&lt;/p&gt;
&lt;p&gt;Besides, usually the measurement of continuous random variable is limited by the precision of the measuring instrument. For instance, when somebody says that is 1.68 meters tall, his or her true height is no exactly 1.68 meters, because the precision of the measuring instrument is only cm (two decimal places). This means that the true height of that person is between 1.675 y 1.685 meters.&lt;/p&gt;
&lt;p&gt;Hence, for continuous variables, &lt;em&gt;it makes no sense to calculate the probability of an isolated value, and we will calculate probabilities for intervals&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;probability-density-function&#34;&gt;Probability density function&lt;/h3&gt;
&lt;p&gt;To model the probability distribution of a continuous random variable we use a probability density function.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Probability density function&lt;/strong&gt;. The &lt;em&gt;probability density function&lt;/em&gt; of a continuous random variable $X$ is a function $f(x)$ that meets the following conditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is non-negative: $f(x)\geq 0$ $\forall x\in \mathbb{R}$,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The area bounded by the curve of the density function and the x-axis is equal to 1, that is,&lt;/p&gt;
&lt;p&gt;$$\int_{-\infty}^{\infty} f(x)\; dx = 1.$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability that $X$ assumes a value between $a$ and $b$ is equal to the area bounded by the density function and the x-axis from $a$ to $b$, that is,&lt;/p&gt;
&lt;p&gt;$$P(a\leq X\leq b) = \int_a^b f(x)\; dx$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The probability density function measures the relative likelihood of every value, but $f(x)$ &lt;em&gt;is
not the probability of $x$&lt;/em&gt;, cause $P(X=x)=0$ for every $x$ value by definition.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;distribution-function&#34;&gt;Distribution function&lt;/h3&gt;
&lt;p&gt;The same way that for discrete random variables, for continuous random variables it makes sense to calculate cumulative probabilities.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Distribution function&lt;/strong&gt;. The &lt;em&gt;distribution function&lt;/em&gt; of a continuous random variable $X$ is a function $F(x)$ that maps every value $a$ to the probability that $X$ takes on a value less than or equal to $a$, that is,&lt;/p&gt;
&lt;p&gt;$$F(a) = P(X\leq a) = \int_{-\infty}^{a} f(x)\; dx.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;probabilities-as-areas&#34;&gt;Probabilities as areas&lt;/h3&gt;
&lt;p&gt;To calculate probabilities with a continuous random variable we measure the area bounded by the probability density function and the x-axis in an interval.&lt;/p&gt;
&lt;img src=&#34;../img/crv/density_function.svg&#34; alt=&#34;Probability as areas of a probability density function&#34; width=&#34;550&#34;&gt;
&lt;p&gt;This area can be calculated integrating the density function or subtracting the distribution function that is easier,&lt;/p&gt;
&lt;p&gt;$$P(a\leq X\leq b) = \int_a^b f(x), dx = F(b)-F(a)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Given the following function&lt;/p&gt;
&lt;p&gt;$$
f(x) =
\begin{cases}
0 &amp;amp; \mbox{if $x&amp;lt;0$} \newline
e^{-x} &amp;amp; \mbox{if $x\geq 0$},
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;let’s check that is a density function.&lt;/p&gt;
&lt;p&gt;As this function is clearly non-negative, we have to check that total area bounded by the curve and the x-axis is 1.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\int_{-\infty}^\infty f(x)\;dx &amp;amp;= \int_{-\infty}^0 f(x)\;dx +\int_0^\infty f(x)\;dx = \int_{-\infty}^0 0\;dx +\int_0^\infty e^{-x}\;dx =\newline
&amp;amp;= \left[-e^{-x}\right]^{\infty}_0 = -e^{-\infty}+e^0 = 1.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Now, let’s calculate the probability of $X$ having a value between 0 and 2.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
P(0\leq X\leq 2) &amp;amp;= \int_0^2 f(x)\;dx = \int_0^2 e^{-x}\;dx = \left[-e^{-x}\right]^2_0 = -e^{-2}+e^0 = 0.8646.
\end{align*}
$$&lt;/p&gt;
&lt;h3 id=&#34;population-statistics&#34;&gt;Population statistics&lt;/h3&gt;
&lt;p&gt;The calculation of the population statistics is similar to the case of discrete variables, but using the density function instead of the probability function, and extending the discrete sum to the integral.&lt;/p&gt;
&lt;p&gt;The most important are:&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Continuous random variable mean&lt;/strong&gt; The &lt;em&gt;mean&lt;/em&gt; or the &lt;em&gt;expectec value&lt;/em&gt; of a continuous random variable $X$ is the integral of the products of its values and its probabilities:&lt;/p&gt;
&lt;p&gt;$$\mu = E(X) = \int_{-\infty}^\infty x f(x)\; dx$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Continuous random variable variance and standard deviation&lt;/strong&gt; The &lt;em&gt;variance&lt;/em&gt; of a continuous random variable $X$ is the integral of the products of its squared values and its probabilities, minus the squared mean:&lt;/p&gt;
&lt;p&gt;$$\sigma^2 = Var(X) = \int_{-\infty}^\infty x^2f(x)\; dx -\mu^2$$&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;standard deviation&lt;/em&gt; of a random variable $X$ is the square root of the variance:&lt;/p&gt;
&lt;p&gt;$$\sigma = +\sqrt{\sigma^2}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Let $X$ be a variable with the following probability density function&lt;/p&gt;
&lt;p&gt;$$
f(x) =
\begin{cases}
0 &amp;amp; \mbox{si $x&amp;lt;0$}\newline
e^{-x} &amp;amp; \mbox{si $x\geq 0$}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;The mean is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mu &amp;amp;= \int_{-\infty}^\infty xf(x)\;dx = \int_{-\infty}^0 xf(x)\;dx +\int_0^\infty xf(x)\;dx = \int_{-\infty}^0 0\;dx +\int_0^\infty xe^{-x}\;dx =\newline
&amp;amp;= \left[-e^{-x}(1+x)\right]_0^{\infty} = 1.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;and the variance is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\sigma^2 &amp;amp;= \int_{-\infty}^\infty x^2f(x)\;dx -\mu^2 = \int_{-\infty}^0 x^2f(x)\;dx +\int_0^\infty x^2f(x)\;dx -\mu^2 = \newline
&amp;amp;= \int_{-\infty}^0 0\;dx +\int_0^\infty x^2e^{-x}\;dx -\mu^2= \left[-e^{-x}(x^2+2x+2)\right]^{\infty}_0 - 1^2 = \newline
&amp;amp;= 2e^0-1 = 1.
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;continuous-probability-distribution-models&#34;&gt;Continuous probability distribution models&lt;/h2&gt;
&lt;p&gt;According to the type of experiment where the random variable is measured, there are different probability distributions models. The most common are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Continuous uniform.&lt;/li&gt;
&lt;li&gt;Normal.&lt;/li&gt;
&lt;li&gt;Student’s T.&lt;/li&gt;
&lt;li&gt;Chi-square.&lt;/li&gt;
&lt;li&gt;Fisher-Snedecor’s F.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;continuous-uniform-distribution&#34;&gt;Continuous uniform distribution&lt;/h2&gt;
&lt;p&gt;When all the values of a random variable $X$ have equal probability, the probability distribution of $X$ is uniform.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition &amp;ndash; Continuous uniform distribution $U(a,b)$&lt;/strong&gt;. A continuous random variable $X$ follows a probability distribution model &lt;em&gt;uniform&lt;/em&gt; of parameters $a$ and $b$, noted $X\sim U(a,b)$, if its range is $\mbox{Ran}(X) = [a,b]$ and its density function is&lt;/p&gt;
&lt;p&gt;$$f(x)= \frac{1}{b-a}\quad \forall x\in [a,b]$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Observe that $a$ and $b$ are the minimum and the maximum of the range respectively, and that the density function is constant.&lt;/p&gt;
&lt;p&gt;The mean and the variance are $$\mu = \frac{a+b}{2}$$ and $$\sigma^2=\frac{(b-a)^2}{12}.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. The generation of a random number between 0 and 1 is follows a continuous uniform distribution $U(0,1)$.&lt;/p&gt;
&lt;img src=&#34;../img/crv/uniform_density_function.svg&#34; alt=&#34;Uniform probability density function&#34; width=&#34;450&#34;&gt;
&lt;p&gt;As the density function is constant, the distribution function has a linear growth.&lt;/p&gt;
&lt;img src=&#34;../img/crv/uniform_distribution_function.svg&#34; alt=&#34;Uniform distribution function&#34; width=&#34;450&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. A bus has a frequency of 15 minutes. Assuming that a person can arrive to the bus station in any time, &lt;em&gt;what is the probability of waiting for the bus between 5 and 10 minutes?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this case, the variable $X$ that measures the waiting time follows a continuous uniform distribution $U(0,15)$ as any waiting time between 0 and 15 is equally likely.&lt;/p&gt;
&lt;p&gt;Then, the probability of waiting between 5 and 10 minutes is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
P(5\leq X\leq 10) &amp;amp;= \int_{5}^{10} \frac{1}{15}\;dx = \left[\frac{x}{15}\right]^{10}_5 = \newline
&amp;amp;= \frac{10}{15}-\frac{5}{15} =\frac{1}{3}.
\end{aligned}
$$&lt;/p&gt;
&lt;img src=&#34;../img/crv/uniform_probability_calculation.svg&#34; alt=&#34;Uniform probability calculation&#34; width=&#34;450&#34;&gt;
&lt;p&gt;And the expected waiting (the mean) time is $\mu=\frac{0+15}{2}=7.5$ minutes.&lt;/p&gt;
&lt;h2 id=&#34;normal-distribution&#34;&gt;Normal distribution&lt;/h2&gt;
&lt;p&gt;The normal distribution model is, without a doubt, the most important continuous distribution model as it is the most common in Nature.&lt;/p&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Normal distribution $N(\mu,\sigma)$&lt;/strong&gt;. A continuous random variable $X$ follows a probability distribution model &lt;em&gt;normal&lt;/em&gt; of parameters $\mu$ and $\sigma$, noted $X\sim N(\mu,\sigma)$, if its range is $\mbox{Ran}(X) = (-\infty,\infty)$ and its density function is&lt;/p&gt;
&lt;p&gt;$$f(x)= \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The two parameters $\mu$ and $\sigma$ are the mean and the standard deviation of the population respectively.&lt;/p&gt;
&lt;p&gt;The plot of the probability density function of a normal distribution $N(\mu,\sigma)$ is bell shaped and it is known as a &lt;em&gt;Gauss bell&lt;/em&gt;.&lt;/p&gt;
&lt;img src=&#34;../img/crv/normal_density_function.svg&#34; alt=&#34;Normal probability density function&#34; width=&#34;450&#34;&gt;
&lt;p&gt;The bell shape depends on the mean $\mu$ and the standard deviation $\sigma$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean $\mu$ sets the center of the bell.&lt;/li&gt;
&lt;li&gt;The standard deviation sets $\sigma$ the width of the bell.&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;img src=&#34;../img/crv/normal_density_function_different_means.svg&#34; alt=&#34;Normal distributions with different means&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;../img/crv/normal_density_function_different_variances.svg&#34; alt=&#34;Normal distributions with different variances&#34; width=&#34;400&#34;&gt;
&lt;/div&gt;
&lt;p&gt;The plot of the distribution function of a normal distribution is S shaped.&lt;/p&gt;
&lt;img src=&#34;../img/crv/normal_distribution_function.svg&#34; alt=&#34;Normal distribution function&#34; width=&#34;450&#34;&gt;
&lt;h3 id=&#34;normal-distribution-properties&#34;&gt;Normal distribution properties&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It is symmetric with respect to the mean, and therefore, the coefficient of skewness is zero, $g_1=0$.&lt;/li&gt;
&lt;li&gt;It is mesokurtic, as the density function is bell shaped, and so, the coefficient of kurtosis is zero, $g_2=0$.&lt;/li&gt;
&lt;li&gt;The mean, median and mode are the same $$\mu = Me = Mo.$$&lt;/li&gt;
&lt;li&gt;It asymptotically approaches 0 when $x$ tends to $\pm \infty$.&lt;/li&gt;
&lt;li&gt;$P(\mu-\sigma \leq X \leq \mu+\sigma) = 0.68$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/crv/normal_interval_68.svg&#34; alt=&#34;68% normal reference interval&#34; width=&#34;450&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$P(\mu-2\sigma \leq X \leq \mu+2\sigma) = 0.95$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/crv/normal_interval_95.svg&#34; alt=&#34;95% normal reference interval&#34; width=&#34;450&#34;&gt;
&lt;ul&gt;
&lt;li&gt;$P(\mu-3\sigma \leq X \leq \mu+3\sigma) = 0.99$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/crv/normal_interval_99.svg&#34; alt=&#34;99% normal reference interval&#34; width=&#34;450&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. It is known that the cholesterol level in females of age between 40 and 50 follows a normal distribution with mean 210 mg/dl and standard deviation 20 mg/dl.&lt;/p&gt;
&lt;p&gt;According to the Gauss bell properties, this means that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 68% of females have a cholesterol level between $210\pm 20$ mg/dl, i.e., between 190 and 230 mg/dl.&lt;/li&gt;
&lt;li&gt;The 95% of females have a cholesterol level between $210\pm 2\cdot 20$ mg/dl, i.e., between 170 and 250 mg/dl.&lt;/li&gt;
&lt;li&gt;The 99% of females have a cholesterol level between $210\pm 3\cdot 20$ mg/dl, i.e., between 150 and 270 mg/dl.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example of blood analysis&lt;/strong&gt;. In blood analysis it is common to use the interval $\mu\pm 2\sigma$ to detect possible pathologies. In the case of cholesterol, this interval is $[170\text{ mg/dl}, 250\text{ mg/dl}]$.&lt;/p&gt;
&lt;p&gt;Thus, when a women between 40 and 50 years of age has a cholesterol level out of this interval, it’s common to think about some pathology. However this person could be healthy, although the likelihood of that happening is only 5%.&lt;/p&gt;
&lt;img src=&#34;../img/crv/blood_analysis.jpg&#34; alt=&#34;Blood analysis&#34; width=&#34;600&#34;&gt;
&lt;h3 id=&#34;the-central-limit-theorem&#34;&gt;The central limit theorem&lt;/h3&gt;
&lt;p&gt;This behavior is common in many physical and biological variables in Nature.&lt;/p&gt;
&lt;p&gt;If you think about the distribution of the height, for instance, you can check that most people in the population have a height around the mean, but as the heights move away from the mean, both below and above the mean, there are few and few people with such a heights.&lt;/p&gt;
&lt;p&gt;The explanation for this behavior is the , that we will see in the next chapter; it states that a continuous random variable whose values depends on a huge number of independent factors adding their effects, always follows a normal distribution.&lt;/p&gt;
&lt;h3 id=&#34;the-standard-normal-distribution-n01&#34;&gt;The standard normal distribution $N(0,1)$&lt;/h3&gt;
&lt;p&gt;The most important normal distribution has mean zero, $\mu=0$, and standard deviation one, $\sigma=1$. It is known as &lt;strong&gt;Standard normal distribution&lt;/strong&gt; and usually represented as $Z\sim N(0,1)$.&lt;/p&gt;
&lt;img src=&#34;../img/crv/standard_normal_density_function.svg&#34; alt=&#34;Standard normal probability density function&#34; width=&#34;450&#34;&gt;
&lt;h3 id=&#34;calculation-of-probabilities-with-the-normal-distribution&#34;&gt;Calculation of probabilities with the normal distribution&lt;/h3&gt;
&lt;p&gt;To avoid integrating the normal density function to compute probabilities it’s common to use the distribution function, that is given in a tabular format like the one below. For instance, to calculate $P(Z\leq 0.52)$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;0.00&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;0.01&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;&lt;span style=&#34;color:#FF3333&#34;&gt;0.02&lt;/span&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;&amp;hellip;&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;0.0&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5000&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5040&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5080&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;0.1&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5398&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5438&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5478&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;0.2&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5793&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5832&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5871&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;0.3&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6179&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6217&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6255&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;0.4&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6554&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6591&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6628&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;&lt;span style=&#34;color:#FF3333&#34;&gt;0.5&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6915&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6950&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;span style=&#34;color:#FF3333&#34;&gt;0.6985&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;⋮&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;⋮&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;⋮&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;⋮&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;⋮&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;$$0.52 \rightarrow \mbox{row }0.5 + \mbox{column }0.02$$&lt;/p&gt;
&lt;img src=&#34;../img/crv/normal_probability_calculation_left_tail.svg&#34; alt=&#34;Normal probability calculation (left tail)&#34; width=&#34;450&#34;&gt;
&lt;p&gt;To compute cumulative probabilities to the right of a value, we can apply the rule for the complement event. For instance,&lt;/p&gt;
&lt;p&gt;$$P(Z&amp;gt;0.52) =1-P(Z\leq 0.52) = 1-F(0.52) = 1 - 0.6985 = 0.3015.$$&lt;/p&gt;
&lt;img src=&#34;../img/crv/normal_probability_calculation_right_tail.svg&#34; alt=&#34;Normal probability calculation (right tail)&#34; width=&#34;450&#34;&gt;
&lt;h3 id=&#34;standardization&#34;&gt;Standardization&lt;/h3&gt;
&lt;p&gt;We have seen how to use the table of the standard normal distribution function to compute probabilities, but, &lt;em&gt;what to do when the normal distribution is not the standard one?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In that case we can use standardization to transform any normal distribution in the standard normal distribution.&lt;/p&gt;
&lt;div class=&#34;alert alert-theo&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Theorem - Standardization&lt;/strong&gt;. If $X$ is a continuous random variables that follow a Normal probability distribution model with mean $\mu$ and standard deviation $\sigma$, $X\sim N(\mu,\sigma)$, then the variable that result of subtracting $\mu$ to $X$ and dividing by $\sigma$, follows a Standard Normal probability distribution,&lt;/p&gt;
&lt;p&gt;$$X\sim N(\mu,\sigma) \Rightarrow Z=\frac{X-\mu}{\sigma}\sim N(0,1).$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Thus, to compute probabilities with a non-standard normal distribution first we have to standardize the variable before using the table of the standard normal distribution function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Assume that the grade of an exam $X$ follows a normal probability distribution model $N(\mu=6,\sigma=1.5)$. What percentage of students didn’t pass the exam?&lt;/p&gt;
&lt;p&gt;As $X$ follows a non-standard normal distribution model, we have to apply standardization first, $Z=\displaystyle \frac{X-\mu}{\sigma} = \frac{X-6}{1.5}$,&lt;/p&gt;
&lt;p&gt;$$
P(X&amp;lt;5) = P\left(\frac{X-6}{1.5}&amp;lt;\frac{5-6}{1.5}\right) = P(Z&amp;lt;-0.67).
$$&lt;/p&gt;
&lt;p&gt;Then we can use the table of the standard normal distribution function,&lt;/p&gt;
&lt;p&gt;$$P(Z&amp;lt;-0.67) = F(-0.67) = 0.2514.$$&lt;/p&gt;
&lt;p&gt;Therefore, $25.14%$ of students didn’t pass the exam.&lt;/p&gt;
&lt;h2 id=&#34;chi-square-distribution&#34;&gt;Chi-square distribution&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Chi-square distribution $\chi^2(n)$&lt;/strong&gt;. Given $n$ independent random variables $Z_1,\ldots,Z_n$, all of them following a standard normal probability distribution, then the variable&lt;/p&gt;
&lt;p&gt;$$\chi^2(n) = Z_1^2+\cdots +Z_n^2,$$&lt;/p&gt;
&lt;p&gt;follows a &lt;em&gt;chi-square probability distribution with $n$ degrees of freedom&lt;/em&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Its range is $\mathbb{R}^+$ and its mean and variance are $\mu = n$ and $\sigma^2 = 2n.$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Below are plotted the density functions of some chi-square distribution models.&lt;/p&gt;
&lt;img src=&#34;../img/crv/chi_square_density_function.svg&#34; alt=&#34;Chi-square probability density function&#34; width=&#34;450&#34;&gt;
&lt;h3 id=&#34;chi-square-distribution-properties&#34;&gt;Chi-square distribution properties&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The range is non-negative.&lt;/li&gt;
&lt;li&gt;If $X\sim \chi^2(n)$ and $Y\sim \chi^2(m)$, then $$X+Y \sim \chi^2(n+m).$$&lt;/li&gt;
&lt;li&gt;It asymptotically approaches to a normal distribution as the degrees of freedom increase.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we will see in the next chapter, the chi-square distribution plays an important role in the estimation of the population variance and in the study of relations between qualitative variables.&lt;/p&gt;
&lt;h2 id=&#34;students-t-distribution&#34;&gt;Student’s t distribution&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Student’s t distribution $T(n)$&lt;/strong&gt;. Given a variable $Z$ following a standard normal distribution model, $Z\sim N(0,1)$, and a variable $X$ following a chi-square distribution model with $n$ degrees of freedom, $X\sim \chi^2(n)$, independent, the variable&lt;/p&gt;
&lt;p&gt;$$T = \frac{Z}{\sqrt{X/n}},$$&lt;/p&gt;
&lt;p&gt;follows a &lt;em&gt;Student’s t probability distribution model with $n$ degrees of freedom&lt;/em&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Its range is $\mathbb{R}$ and its mean and variance are $$\mu = 0$$ and $$\sigma^2 = \frac{n}{n-2}$$ if $n&amp;gt;2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Below are plotted the density functions of some student&amp;rsquo;s t distribution models.&lt;/p&gt;
&lt;img src=&#34;../img/crv/student_t_density_function.svg&#34; alt=&#34;Student&#39;s t probability density function&#34; width=&#34;450&#34;&gt;
&lt;h3 id=&#34;students-t-distribution-properties&#34;&gt;Student’s t distribution properties&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The mean, the median and the mode are the same, $\mu=Me=Mo$.&lt;/li&gt;
&lt;li&gt;It is symmetric, $g_1=0$.&lt;/li&gt;
&lt;li&gt;It asymptotically approaches to the standard normal distribution as the degrees of freedom increase. In practice for $n\geq 30$ both distributions are approximately the same. $$T(n)\stackrel{n\rightarrow \infty}{\approx}N(0,1).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we will see in the next chapter, the Student’s t distribution plays an important role in the estimation of the population mean.&lt;/p&gt;
&lt;h2 id=&#34;fisher-snedecors-f-distribution&#34;&gt;Fisher-Snedecor’s F distribution&lt;/h2&gt;
&lt;div class=&#34;alert alert-def&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Definition - Fisher-Snedecor’s F distribution $F(m,n)$&lt;/strong&gt;. Given two independent variables $X$ and $Y$ both following a chi-square probability distribution model with $m$ an $n$ degrees of freedom respectively, $X\sim \chi^2(m)$ and $Y\sim \chi^2(n)$, then the variable&lt;/p&gt;
&lt;p&gt;$$F = \frac{X/m}{Y/n},$$&lt;/p&gt;
&lt;p&gt;follows a &lt;em&gt;Fisher-Snedecor’s F probability distribution model with $m$ and $n$ degrees of freedom&lt;/em&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Its range is $\mathbb{R}^+$ and its mean and variance are $$\mu = \frac{n}{n-2}$$ and $$\sigma^2 =\frac{2n^2(m+n−2)}{m(n-2)^2(n-4)}$$ if $n&amp;gt;4$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Below are plotted the density functions of some Fisher-Snedecor&amp;rsquo;s F distribution models.&lt;/p&gt;
&lt;img src=&#34;../img/crv/fisher_f_density_function.svg&#34; alt=&#34;Fisher-Snedecor&#39;s F probability density function&#34; width=&#34;450&#34;&gt;
&lt;h3 id=&#34;fisher-snedecors-f-distribution-properties&#34;&gt;Fisher-Snedecor’s F distribution properties&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The range is non-negative.&lt;/li&gt;
&lt;li&gt;It satisfies $$F(m,n) =\frac{1}{F(n,m)}.$$ Thus, if we name $f(m,n)_p$ the value that satisfies $P(F(m,n)\leq f(m,n)_p)=p$, then $$f(m,n)_p =\frac{1}{f(n,m)_{1-p}}$$ which is helpful in order to compute probabilities from the table of the distribution function.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we will see in the next chapter, the Fisher-Snedecor’s F distribution plays an important role in the comparison of population variances and in the analysis of variance test (ANOVA).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
